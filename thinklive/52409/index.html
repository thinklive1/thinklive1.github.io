<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.24/fancybox/fancybox.css" integrity="sha256-vQkngPS8jiHHH0I6ABTZroZk8NPZ7b+MUReOFE9UsXQ=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"thinklive1.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"flat"},"fold":{"enable":true,"height":500},"bookmark":{"enable":true,"color":"#66CCFF","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":"utterances","storage":true,"lazyload":false,"nav":null,"activeClass":"utterances"},"stickytabs":true,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="关于具身智能领域的论文">
<meta property="og:type" content="article">
<meta property="og:title" content="具身智能论文阅读笔记">
<meta property="og:url" content="https://thinklive1.github.io/thinklive/52409/index.html">
<meta property="og:site_name" content="thinklive">
<meta property="og:description" content="关于具身智能领域的论文">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/Pasted%20image%2020250911154734.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/overview-icra.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-7.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-9.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-8.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/auc.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-10.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-11.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-12.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-13.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-14.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-15.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-16.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-17.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/unnamed.jpg">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-20.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-18.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-19.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-21.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-22.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-5.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-6.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-1.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-2.png">
<meta property="article:published_time" content="2025-09-09T07:53:29.848Z">
<meta property="article:modified_time" content="2025-10-17T12:09:33.015Z">
<meta property="article:author" content="thinklive">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://thinklive1.github.io/assets/ml/Pasted%20image%2020250911154734.png">


<link rel="canonical" href="https://thinklive1.github.io/thinklive/52409/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://thinklive1.github.io/thinklive/52409/","path":"thinklive/52409/","title":"具身智能论文阅读笔记"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>具身智能论文阅读笔记 | thinklive</title>
  







<script type="text/javascript" async src="/js/fairyDustCursor.js"></script>
<script type="text/javascript" async src="/js/tab-title.js"></script>
<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>
<script src="/js/tab-title.js"></script>

<!--pjax：防止跳转页面音乐暂停-->
<script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script>
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
  <script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>
<script>
const options = {
  bottom: '64px', // default: '32px'
  right: 'unset', // default: '32px'
  left: '32px', // default: 'unset'
  time: '0.5s', // default: '0.3s'
  mixColor: '#fff', // default: '#fff'
  backgroundColor: '#fff',  // default: '#fff'
  buttonColorDark: '#100f2c',  // default: '#100f2c'
  buttonColorLight: '#fff', // default: '#fff'
  saveInCookies: false, // default: true,
  label: '🌓', // default: ''
  autoMatchOsTheme: true // default: true
}

const darkmode = new Darkmode(options);
</script>
<!-- hexo injector head_end start --><script> let HEXO_MMEDIA_DATA = { js: [], css: [], aplayerData: [], metingData: [], artPlayerData: [], dplayerData: []}; </script><!-- hexo injector head_end end --><link rel="alternate" href="/atom.xml" title="thinklive" type="application/atom+xml">
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>
<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>


  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">thinklive</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">dirichlet library</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索 | search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-主页-|-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>主页 | home</a></li><li class="menu-item menu-item-标签-|-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签 | tags</a></li><li class="menu-item menu-item-分类-|-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类 | categories</a></li><li class="menu-item menu-item-归档-|-archive"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档 | archive</a></li><li class="menu-item menu-item-相册-|-photo"><a href="/photos/" rel="section"><i class="fa fa-camera fa-fw"></i>相册 | photo</a></li><li class="menu-item menu-item-留言-|-guestbook"><a href="/guestbook/" rel="section"><i class="fa fa-book fa-fw"></i>留言 | guestbook</a></li><li class="menu-item menu-item-感谢-|-thank"><a href="/thanks/" rel="section"><i class="fa custom thanks fa-fw"></i>感谢 | thank</a></li><li class="menu-item menu-item-游戏-|-game"><a href="/game/bad1.html" rel="section"><i class="fa fa-gamepad fa-fw"></i>游戏 | game</a></li><li class="menu-item menu-item-神龛-|-shrine"><a href="/cyberblog/" rel="section"><i class="fa fa-microchip fa-fw"></i>神龛 | shrine</a></li><li class="menu-item menu-item-资源地图-|-resourcemap"><a href="/webstack/" rel="section"><i class="fa fa-list fa-fw"></i>资源地图 | resourcemap</a></li><li class="menu-item menu-item-思维导图-|-mindmap"><a href="/mindmap/index.html" rel="section"><i class="fa fa-map fa-fw"></i>思维导图 | mindmap</a></li><li class="menu-item menu-item-网站地图-|-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>网站地图 | sitemap</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索 | search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">


<!--网易云音乐插件-->
<!-- require APlayer -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css">
<script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script>
<!-- require MetingJS-->
<script src="https://cdn.jsdelivr.net/npm/meting@2.0.1/dist/Meting.min.js"></script> 
<!--网易云playlist外链地址-->   
<meting-js
    server="netease"
    type="playlist" 
    id="2762741085"
    mini="false"
    fixed="false"
    list-folded="true"
    autoplay="false"
    volume="0.2"
    theme="#4c4c4c"
    order="random"
    loop="all"
    preload="auto"
    mutex="true">
    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#semantic-mapping-%E8%AF%AD%E4%B9%89%E5%9C%B0%E5%9B%BE"><span class="nav-number">1.</span> <span class="nav-text">Semantic Mapping 语义地图</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#survey"><span class="nav-number">1.1.</span> <span class="nav-text">survey</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.1.1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%8D%E8%AF%8D%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.1.2.</span> <span class="nav-text">名词介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9C%B0%E5%9B%BE%E7%BB%93%E6%9E%84"><span class="nav-number">1.1.3.</span> <span class="nav-text">地图结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#spatial-grid-map"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">Spatial grid map</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#topological-map"><span class="nav-number">1.1.3.2.</span> <span class="nav-text">Topological map</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#point-cloud-map"><span class="nav-number">1.1.3.3.</span> <span class="nav-text">Point-cloud map</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#hybrid-map"><span class="nav-number">1.1.3.4.</span> <span class="nav-text">Hybrid map</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9C%B0%E5%9B%BE%E7%BC%96%E7%A0%81-map-encoding"><span class="nav-number">1.1.4.</span> <span class="nav-text">地图编码 map encoding</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%98%BE%E5%BC%8F%E7%BC%96%E7%A0%81"><span class="nav-number">1.1.4.1.</span> <span class="nav-text">显式编码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9A%90%E5%BC%8F%E7%BC%96%E7%A0%81"><span class="nav-number">1.1.4.2.</span> <span class="nav-text">隐式编码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9C%B0%E5%9B%BE%E8%AF%84%E4%BC%B0-map-evaluation"><span class="nav-number">1.1.5.</span> <span class="nav-text">地图评估 Map Evaluation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%95%E6%9C%9B%E6%9C%AA%E6%9D%A5%E4%B8%8E%E7%BB%93%E8%AF%AD"><span class="nav-number">1.1.6.</span> <span class="nav-text">展望未来与结语</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dualmap-%E5%9C%A8%E7%BA%BF%E5%BC%80%E6%94%BE%E8%AF%8D%E6%B1%87%E8%AF%AD%E4%B9%89%E5%BB%BA%E5%9B%BE%E5%8A%A9%E5%8A%9B%E6%99%BA%E8%83%BD%E4%BD%93%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%AF%BC%E8%88%AA"><span class="nav-number">1.2.</span> <span class="nav-text">DualMap: 在线开放词汇语义建图助力智能体自然语言导航</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%83%8C%E6%99%AF"><span class="nav-number">1.2.1.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dualmap-%E7%9A%84%E6%94%B9%E8%BF%9B"><span class="nav-number">1.2.2.</span> <span class="nav-text">DualMap 的改进</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B7%E8%B1%A1%E5%9C%B0%E5%9B%BE"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">具象地图</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8A%BD%E8%B1%A1%E5%9C%B0%E5%9B%BE"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">抽象地图</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%BC%E8%88%AA%E7%AD%96%E7%95%A5"><span class="nav-number">1.2.2.3.</span> <span class="nav-text">导航策略</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%84%E4%B8%8E%E6%80%BB%E7%BB%93"><span class="nav-number">1.2.3.</span> <span class="nav-text">测试结构与总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%99%84%E5%BD%95"><span class="nav-number">1.2.4.</span> <span class="nav-text">附录</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#conceptgraphs-open-vocabulary-3d-scene-graphs-for-perception-and-planning"><span class="nav-number">1.3.</span> <span class="nav-text">ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#d-%E5%AF%B9%E8%B1%A1%E7%94%9F%E6%88%90"><span class="nav-number">1.3.1.</span> <span class="nav-text">3d 对象生成</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#d-%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90"><span class="nav-number">1.3.2.</span> <span class="nav-text">3d 场景图生成</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hierarchical-open-vocabulary-3d-scene-graphs-for-language-grounded-robot-navigation"><span class="nav-number">1.4.</span> <span class="nav-text">Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C"><span class="nav-number">1.4.1.</span> <span class="nav-text">实验</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#open-scene-graphs-for-open-world-object-goal-navigation"><span class="nav-number">1.5.</span> <span class="nav-text">Open Scene Graphs for Open-World Object-Goal Navigation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9C%BA%E6%99%AF%E5%9B%BE"><span class="nav-number">1.5.1.</span> <span class="nav-text">场景图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9C%B0%E5%9B%BE%E7%94%9F%E6%88%90%E5%92%8C%E5%AE%9A%E4%BD%8D"><span class="nav-number">1.5.2.</span> <span class="nav-text">地图生成和定位</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90-osg"><span class="nav-number">1.5.2.1.</span> <span class="nav-text">自动生成 OSG</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E8%A7%A3%E6%9E%90-image-parser"><span class="nav-number">1.5.2.2.</span> <span class="nav-text">图像解析 Image Parser</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%8A%B6%E6%80%81%E8%AF%84%E4%BC%B0"><span class="nav-number">1.5.2.3.</span> <span class="nav-text">状态评估</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#osg-%E6%9B%B4%E6%96%B0"><span class="nav-number">1.5.2.4.</span> <span class="nav-text">OSG 更新</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%A6%82%E7%8E%87%E7%9A%84-osg"><span class="nav-number">1.5.2.5.</span> <span class="nav-text">基于概率的 OSG</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A8%E7%90%86%E5%92%8C%E6%8E%A7%E5%88%B6"><span class="nav-number">1.5.3.</span> <span class="nav-text">推理和控制</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8E%A8%E7%90%86"><span class="nav-number">1.5.3.1.</span> <span class="nav-text">推理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8E%A7%E5%88%B6"><span class="nav-number">1.5.3.2.</span> <span class="nav-text">控制</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E6%80%BB%E7%BB%93"><span class="nav-number">1.5.4.</span> <span class="nav-text">实验与总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#stage-1-summary"><span class="nav-number">1.6.</span> <span class="nav-text">stage 1 summary</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#map-semnav-advancing-zero-shot-continuous-vision-and-language-navigation-through-visual-semantics-and-map-integration"><span class="nav-number">1.7.</span> <span class="nav-text">Map-SemNav: Advancing Zero-Shot Continuous Vision-and-Language Navigation through Visual Semantics and Map Integration</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BB%BA%E5%9B%BE"><span class="nav-number">1.7.1.</span> <span class="nav-text">建图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9C%BA%E6%99%AF%E5%AF%B9%E8%B1%A1%E8%A7%A3%E8%80%A6decoupling"><span class="nav-number">1.7.2.</span> <span class="nav-text">场景&#x2F;对象解耦(decoupling)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A8%E7%90%86%E4%B8%8E%E6%8E%A7%E5%88%B6"><span class="nav-number">1.7.3.</span> <span class="nav-text">推理与控制</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#scene-driven-multimodal-knowledge-graph-construction-for-embodied-ai"><span class="nav-number">1.8.</span> <span class="nav-text">Scene-Driven Multimodal Knowledge Graph Construction for Embodied AI</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#kg-%E6%9E%84%E5%BB%BA%E8%BF%87%E7%A8%8B"><span class="nav-number">1.8.1.</span> <span class="nav-text">KG 构建过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA"><span class="nav-number">1.8.2.</span> <span class="nav-text">知识增强</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9F%A5%E8%AF%86%E6%B3%A8%E5%85%A5"><span class="nav-number">1.8.2.1.</span> <span class="nav-text">知识注入</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#multimodal-data-storage-and-retrieval-for-embodied-ai-a-survey"><span class="nav-number">1.9.</span> <span class="nav-text">Multimodal Data Storage and Retrieval for Embodied AI: A Survey</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8"><span class="nav-number">1.9.1.</span> <span class="nav-text">数据存储</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%A3%80%E7%B4%A2"><span class="nav-number">1.9.2.</span> <span class="nav-text">数据检索</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#eai-%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E7%9A%84%E6%8C%91%E6%88%98"><span class="nav-number">1.9.3.</span> <span class="nav-text">eai 数据管理的挑战</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#embodied-rag-general-non-parametric-embodied-memory-for-retrieval-and-generation"><span class="nav-number">1.10.</span> <span class="nav-text">Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%B0%E5%BF%86%E6%9E%84%E5%BB%BA"><span class="nav-number">1.10.1.</span> <span class="nav-text">记忆构建</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A3%80%E7%B4%A2"><span class="nav-number">1.10.2.</span> <span class="nav-text">检索</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#vision-language-action-models"><span class="nav-number">2.</span> <span class="nav-text">Vision-Language-Action Models</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#large-vlm-based-vision-language-action-models-for-robotic-manipulation-a-survey"><span class="nav-number">2.1.</span> <span class="nav-text">Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%8D%E8%A8%80-1"><span class="nav-number">2.1.1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%83%8C%E6%99%AF%E7%9F%A5%E8%AF%86"><span class="nav-number">2.1.2.</span> <span class="nav-text">背景知识</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#monolithic-models-%E5%8D%95%E7%89%87%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.1.3.</span> <span class="nav-text">MONOLITHIC MODELS 单片模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#single-system-models"><span class="nav-number">2.1.3.1.</span> <span class="nav-text">Single-system Models</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#model-performance-enhancement"><span class="nav-number">2.1.3.1.1.</span> <span class="nav-text">Model Performance Enhancement</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#inference-efficiency-optimization"><span class="nav-number">2.1.3.1.2.</span> <span class="nav-text">Inference Efficiency Optimization</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dual-system-models"><span class="nav-number">2.1.3.2.</span> <span class="nav-text">Dual-system Models</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#cascade-based-methods"><span class="nav-number">2.1.3.2.1.</span> <span class="nav-text">Cascade-based Methods</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#parallel-based-methods"><span class="nav-number">2.1.3.2.2.</span> <span class="nav-text">Parallel-based Methods</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hierarchical-models-%E5%88%86%E5%B1%82%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.1.4.</span> <span class="nav-text">HIERARCHICAL MODELS 分层模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#planner-only"><span class="nav-number">2.1.4.1.</span> <span class="nav-text">Planner-Only</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#plannerpolicy"><span class="nav-number">2.1.4.2.</span> <span class="nav-text">Planner+Policy</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E5%89%8D%E6%B2%BF%E9%A2%86%E5%9F%9F"><span class="nav-number">2.1.5.</span> <span class="nav-text">其他前沿领域</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#vla-%E7%9A%84%E7%89%B9%E7%82%B9"><span class="nav-number">2.1.6.</span> <span class="nav-text">VLA 的特点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95"><span class="nav-number">2.1.7.</span> <span class="nav-text">数据集和基准测试</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E5%92%8C%E5%B1%95%E6%9C%9B"><span class="nav-number">2.1.8.</span> <span class="nav-text">总结和展望</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%CF%800-a-vision-language-action-flow-model-for-general-robot-control"><span class="nav-number">2.2.</span> <span class="nav-text">π0: A Vision-Language-Action Flow Model for General Robot Control</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D"><span class="nav-number">2.2.1.</span> <span class="nav-text">模型介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%AE%9E%E9%AA%8C"><span class="nav-number">2.2.2.</span> <span class="nav-text">训练与实验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%86%E8%8A%82"><span class="nav-number">2.2.3.</span> <span class="nav-text">细节</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B"><span class="nav-number">3.</span> <span class="nav-text">实例</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#openin"><span class="nav-number">3.1.</span> <span class="nav-text">OpenIN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#learning-fine-grained-bimanual-manipulation-with-low-cost-hardware"><span class="nav-number">3.2.</span> <span class="nav-text">Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%8D%E8%A8%80-2"><span class="nav-number">3.2.1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#real-work"><span class="nav-number">3.2.2.</span> <span class="nav-text">real work</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E6%80%BB%E7%BB%93-1"><span class="nav-number">3.2.3.</span> <span class="nav-text">实验与总结</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="thinklive"
      src="/images/thive.png">
  <p class="site-author-name" itemprop="name">thinklive</p>
  <div class="site-description" itemprop="description">起初，世界是一团思索，它向所有的方向迈了一步，于是万物由此而生</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">45</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">49</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/thinklive1" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;thinklive1" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/t469631989@gmail.com" title="E-Mail → t469631989@gmail.com" rel="noopener me"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/38099250?spm_id_from=333.1007.0.0" title="bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;38099250?spm_id_from&#x3D;333.1007.0.0" rel="noopener me" target="_blank"><i class="fa custom bilibili fa-fw"></i>bilibili</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://steamcommunity.com/id/thinkliving" title="steam → https:&#x2F;&#x2F;steamcommunity.com&#x2F;id&#x2F;thinkliving" rel="noopener me" target="_blank"><i class="fa custom steam fa-fw"></i>steam</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml" rel="noopener me"><i class="fa fa-rss fa-fw"></i>RSS</a>
      </span>
  </div>

<div style="Text-align:center;width:100%"><div style="margin:0 auto"><canvas id="canvas" style="width:60%" height="100" width="700">当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>!function(){function t(t,e){for(var a=0;a<l[e].length;a++)for(var n=0;n<l[e][a].length;n++)1==l[e][a][n]&&(h.beginPath(),h.arc(14*(g+2)*t+2*n*(g+1)+(g+1),2*a*(g+1)+(g+1),g,0,2*Math.PI),h.closePath(),h.fill())}function e(){var t=[],e=/(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date),a=[];a.push(e[1],e[2],10,e[3],e[4],10,e[5],e[6]);for(var r=c.length-1;r>=0;r--)a[r]!==c[r]&&t.push(r+"_"+(Number(c[r])+1)%10);for(var r=0;r<t.length;r++)n.apply(this,t[r].split("_"));c=a.concat()}function a(){for(var t=0;t<d.length;t++)d[t].stepY+=d[t].disY,d[t].x+=d[t].stepX,d[t].y+=d[t].stepY,(d[t].x>i+g||d[t].y>f+g)&&(d.splice(t,1),t--)}function n(t,e){for(var a=[1,2,3],n=["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"],r=0;r<l[e].length;r++)for(var o=0;o<l[e][r].length;o++)if(1==l[e][r][o]){var h={x:14*(g+2)*t+2*o*(g+1)+(g+1),y:2*r*(g+1)+(g+1),stepX:Math.floor(4*Math.random()-2),stepY:-2*a[Math.floor(Math.random()*a.length)],color:n[Math.floor(Math.random()*n.length)],disY:1};d.push(h)}}function r(){o.height=100;for(var e=0;e<c.length;e++)t(e,c[e]);for(var e=0;e<d.length;e++)h.beginPath(),h.arc(d[e].x,d[e].y,g,0,2*Math.PI),h.fillStyle=d[e].color,h.closePath(),h.fill()}var l=[[[0,0,1,1,1,0,0],[0,1,1,0,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,0,1,1,0],[0,0,1,1,1,0,0]],[[0,0,0,1,1,0,0],[0,1,1,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[1,1,1,1,1,1,1]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,0,0,1,1],[1,1,1,1,1,1,1]],[[1,1,1,1,1,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,0,1,1,1,0],[0,0,1,1,1,1,0],[0,1,1,0,1,1,0],[1,1,0,0,1,1,0],[1,1,1,1,1,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,1,1]],[[1,1,1,1,1,1,1],[1,1,0,0,0,0,0],[1,1,0,0,0,0,0],[1,1,1,1,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[1,1,1,1,1,1,1],[1,1,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,1,1,0,0,0,0]],[[0,0,0,0,0,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,0,0,0]]],o=document.getElementById("canvas");if(o.getContext){var h=o.getContext("2d"),f=100,i=700;o.height=f,o.width=i,h.fillStyle="#f00",h.fillRect(10,10,50,50);var c=[],d=[],g=o.height/20-1;!function(){var t=/(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date);c.push(t[1],t[2],10,t[3],t[4],10,t[5],t[6])}(),clearInterval(v);var v=setInterval(function(){e(),a(),r()},50)}}()</script></div>
<img class= 'logo' src="/images/thinklive_cyber.png"; z-index: '0'; style="max-width: 100%; width: auto; height: auto;background-color: --content-bg-color;">

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://thinklive1.github.io/thinklive/52409/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/thive.png">
      <meta itemprop="name" content="thinklive">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="thinklive">
      <meta itemprop="description" content="起初，世界是一团思索，它向所有的方向迈了一步，于是万物由此而生">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="具身智能论文阅读笔记 | thinklive">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          具身智能论文阅读笔记
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-09-09 15:53:29" itemprop="dateCreated datePublished" datetime="2025-09-09T15:53:29+08:00">2025-09-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-10-17 20:09:33" itemprop="dateModified" datetime="2025-10-17T20:09:33+08:00">2025-10-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%A7%91%E7%A0%94/" itemprop="url" rel="index"><span itemprop="name">科研</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>34k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2:02</span>
    </span>
</div>

        
        </div>
      </header>
   

    
    
    
    <div class="post-body" itemprop="articleBody"><p>关于具身智能领域的论文</p>
<span id="more"></span>
<ul>
<li><a href="#semantic-mapping-语义地图">Semantic Mapping 语义地图</a>
<ul>
<li><a href="#survey">survey</a>
<ul>
<li><a href="#前言">前言</a></li>
<li><a href="#名词介绍">名词介绍</a></li>
<li><a href="#地图结构">地图结构</a>
<ul>
<li><a href="#spatial-grid-map">Spatial grid map</a></li>
<li><a href="#topological-map">Topological map</a></li>
<li><a href="#point-cloud-map">Point-cloud map</a></li>
<li><a href="#hybrid-map">Hybrid map</a></li>
</ul></li>
<li><a href="#地图编码-map-encoding">地图编码 map encoding</a>
<ul>
<li><a href="#显式编码">显式编码</a></li>
<li><a href="#隐式编码">隐式编码</a></li>
</ul></li>
<li><a href="#地图评估-map-evaluation">地图评估 Map Evaluation</a></li>
<li><a href="#展望未来与结语">展望未来与结语</a></li>
</ul></li>
<li><a href="#dualmap-在线开放词汇语义建图助力智能体自然语言导航">DualMap: 在线开放词汇语义建图助力智能体自然语言导航</a>
<ul>
<li><a href="#背景">背景</a></li>
<li><a href="#dualmap-的改进">DualMap 的改进</a>
<ul>
<li><a href="#具象地图">具象地图</a></li>
<li><a href="#抽象地图">抽象地图</a></li>
<li><a href="#导航策略">导航策略</a></li>
</ul></li>
<li><a href="#测试结构与总结">测试结构与总结</a></li>
<li><a href="#附录">附录</a></li>
</ul></li>
<li><a href="#conceptgraphs-open-vocabulary-3d-scene-graphs-for-perception-and-planning">ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning</a>
<ul>
<li><a href="#3d-对象生成">3d 对象生成</a></li>
<li><a href="#3d-场景图生成">3d 场景图生成</a></li>
</ul></li>
<li><a href="#hierarchical-open-vocabulary-3d-scene-graphs--for-language-grounded-robot-navigation">Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation</a>
<ul>
<li><a href="#实验">实验</a></li>
</ul></li>
<li><a href="#open-scene-graphs-for-open-world-object-goal-navigation">Open Scene Graphs for Open-World Object-Goal Navigation</a>
<ul>
<li><a href="#场景图">场景图</a></li>
<li><a href="#地图生成和定位">地图生成和定位</a>
<ul>
<li><a href="#自动生成-osg">自动生成 OSG</a></li>
<li><a href="#图像解析-image-parser">图像解析 Image Parser</a></li>
<li><a href="#状态评估">状态评估</a></li>
<li><a href="#osg-更新">OSG 更新</a></li>
<li><a href="#基于概率的-osg">基于概率的 OSG</a></li>
</ul></li>
<li><a href="#推理和控制">推理和控制</a>
<ul>
<li><a href="#推理">推理</a></li>
<li><a href="#控制">控制</a></li>
</ul></li>
<li><a href="#实验与总结">实验与总结</a></li>
</ul></li>
<li><a href="#stage-1-summary">stage 1 summary</a></li>
<li><a href="#map-semnav-advancing-zero-shot-continuous-vision-and-language--navigation-through-visual-semantics-and-map-integration">Map-SemNav: Advancing Zero-Shot Continuous Vision-and-Language Navigation through Visual Semantics and Map Integration</a>
<ul>
<li><a href="#建图">建图</a></li>
<li><a href="#场景对象解耦decoupling">场景/对象解耦(decoupling)</a></li>
<li><a href="#推理与控制">推理与控制</a></li>
</ul></li>
<li><a href="#scene-driven-multimodal-knowledge-graph--construction-for-embodied-ai">Scene-Driven Multimodal Knowledge Graph Construction for Embodied AI</a>
<ul>
<li><a href="#kg-构建过程">KG 构建过程</a></li>
<li><a href="#知识增强">知识增强</a>
<ul>
<li><a href="#知识注入">知识注入</a></li>
</ul></li>
</ul></li>
<li><a href="#multimodal-data-storage-and-retrieval-for--embodied-ai-a-survey">Multimodal Data Storage and Retrieval for Embodied AI: A Survey</a>
<ul>
<li><a href="#数据存储">数据存储</a></li>
<li><a href="#数据检索">数据检索</a></li>
<li><a href="#eai-数据管理的挑战">eai 数据管理的挑战</a></li>
</ul></li>
<li><a href="#embodied-rag-general-non-parametric-embodied--memory-for-retrieval-and-generation">Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation</a>
<ul>
<li><a href="#记忆构建">记忆构建</a></li>
<li><a href="#检索">检索</a></li>
</ul></li>
</ul></li>
<li><a href="#vision-language-action-models">Vision-Language-Action Models</a>
<ul>
<li><a href="#large-vlm-based-vision-language-action-models-for-robotic-manipulation-a-survey">Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey</a>
<ul>
<li><a href="#前言-1">前言</a></li>
<li><a href="#背景知识">背景知识</a></li>
<li><a href="#monolithic-models-单片模型">MONOLITHIC MODELS 单片模型</a>
<ul>
<li><a href="#single-system-models">Single-system Models</a>
<ul>
<li><a href="#model-performance-enhancement">Model Performance Enhancement</a></li>
<li><a href="#inference-efficiency-optimization">Inference Efficiency Optimization</a></li>
</ul></li>
<li><a href="#dual-system-models">Dual-system Models</a>
<ul>
<li><a href="#cascade-based-methods">Cascade-based Methods</a></li>
<li><a href="#parallel-based-methods">Parallel-based Methods</a></li>
</ul></li>
</ul></li>
<li><a href="#hierarchical-models-分层模型">HIERARCHICAL MODELS 分层模型</a>
<ul>
<li><a href="#planner-only">Planner-Only</a></li>
<li><a href="#plannerpolicy">Planner+Policy</a></li>
</ul></li>
<li><a href="#其他前沿领域">其他前沿领域</a></li>
<li><a href="#vla-的特点">VLA 的特点</a></li>
<li><a href="#数据集和基准测试">数据集和基准测试</a></li>
<li><a href="#总结和展望">总结和展望</a></li>
</ul></li>
<li><a href="#π0-a-vision-language-action-flow-model-for--general-robot-control">π0: A Vision-Language-Action Flow Model for General Robot Control</a>
<ul>
<li><a href="#模型介绍">模型介绍</a></li>
<li><a href="#训练与实验">训练与实验</a></li>
<li><a href="#细节">细节</a></li>
</ul></li>
</ul></li>
<li><a href="#实例">实例</a>
<ul>
<li><a href="#openin">OpenIN</a></li>
<li><a href="#learning-fine-grained-bimanual-manipulation-with--low-cost-hardware">Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware</a>
<ul>
<li><a href="#前言-2">前言</a></li>
<li><a href="#real-work">real work</a></li>
<li><a href="#实验与总结-1">实验与总结</a></li>
</ul></li>
</ul></li>
</ul>
<p>具身智能: 集成了物理上的机械以及 ai(一般是常用的机器学习模型)的一种智能体, 这个词与 robotics(机器人学)的区别是, 后者较为关心硬件上的很多问题, 以及操作系统等偏底层的问题; 而具身智能较为关注抽象层面的智能, 暂时屏蔽掉底层的一些细节</p>
<h1 id="semantic-mapping-语义地图">Semantic Mapping 语义地图</h1>
<h2 id="survey">survey</h2>
<h3 id="前言">前言</h3>
<p>语义地图以结构化的方式捕获环境信息, 让代理能够利用这些信息进行推理<br />
由于具身智能面对的现实问题复杂性, 它会需要很多种 ai 技术, 单单是抓取一件物品, 就需要包括视觉辨识, 自然语言的理解与推理, 导航, 操控机械臂的能力等等<br />
在这一系列任务中, 辨识理解环境是第一步, 在认知科学中, 研究者发现人类和动物实际上也会将现实的空间抽象化为一种“认知地图”, 也就是说, 对我们的大脑来说, 一处地点在脑中的印象不只是一些有长宽高形状的障碍物, 还有名称, 物理性质, 用途之类的“语义”, 这一定程度上启发了研究者使用类似的机制让 ai 理解环境, 也就是语义地图<br />
为了方便, 将建立(语义)地图的过程简称为映射(mapping)<br />
对于导航, 寻物等任务, 当前主流的流程可分为三步:</p>
<ol type="1">
<li>将当前的环境合理地建立数据模型</li>
<li>定位自己的当前位置</li>
<li>路径规划</li>
</ol>
<p>智能机器人可以分为很多种, 我们只考虑不联网(用不了 gps), 没有复杂传感器的一类, 就叫 bot 吧, bot 想建立地图, 最直观的方法就是以自己为中心开始认知环境, 这就有点类似人类的认路, 对这种机器人来说,1. 2.其实可以视为一体, 因为在对当前环境建模的过程中可以自然地知道自己的位置, 主流的技术也是这么做的, 称为 <code>simultaneous localization and mapping (SLAM)</code><br />
总结一下, 接下来要讨论的 SLAM 是一种对环境的不断认知地同时进行地图制定及定位的一种技术<br />
现在设想一个传统印象里的机器人, 它可能有光学或者声学的某些传感器, 这些传感器能很容易地捕捉到附近的障碍物, 这和初中生物讲的蝙蝠超声波定位也没什么本质区别, 但这些障碍物信息并不包括语义信息<br />
附带语义的 slam 称为 <code>semantic slam</code>, 常见的做法是对传感器画面做特征提取, 例如可以提取成一系列词汇, 这称为 <code>Bag of Visual Words</code>, 得到这些词后, 查找词典匹配到一个具体物品; 如果想要更好的效果, 则可以进行深度学习</p>
<h3 id="名词介绍">名词介绍</h3>
<p>如果我们想要真正能用的机器人, 就不得不想办法让它能理解概念性的东西, 例如自然语言里的上下文, 词义向量, 而在具身智能的问题场景中, 则需要带有语义的地图, 例如客厅和餐厅都摆着大桌子, 要区分这两种房间就需要很多上下文信息<br />
以下开始介绍一些细节, 首先是地图的表示方法(结构), 考虑室内这个场景, 由于其最大高度是固定的, 因此可以比较方便地使用 2d 的拓扑图, 网格来表示, 除此之后也可以使用 3d 的点云或者其他混合结构<br />
我们首先做的是物体辨识和分类问题, 在室内, 例如普通住房中, 房间、家具的种类都是有限且数量级很小的, 对于机器人, 用内置芯片的算力可能就能做出来, 在机器人探索的过程中, 得到的各个空间、物体关系可以不断存入语义地图中供之后调用, 例如让机器人找一个苹果, 它能发现苹果在冰箱里面, 拓扑关系上可能就是客厅-冰箱-苹果, 这样之后找苹果也可以跟着这条路径走</p>
<div class="note info"><ul>
<li><code>开放词汇语义地图（Open-Vocabulary Semantic Maps）</code>: 系统能够识别和处理未在训练中见过的新对象或特征, 而不仅仅局限于预定义的类别; 与其相反的情况就可以视为是一个分类问题</li>
<li><code>Embodied AI tasks</code>: 大致可分为: 探索、导航、操纵, 更细粒度的划分可以使用目标的类型, 例如图像目标比定位上的目标需要更细的信息</li>
<li><code>End-to-end | Modular approaches</code>: 前者直接用传感器信息生成行动, 优点是对中小型任务效果不错, 但复杂的 3d 空间, 长期路径规划等任务中表现不佳, 此外也无法复用; 后者会将输入信息投入不同模块中处理例如编码器, 映射, 探索等, 最后生成行动, 优点是各个模块可以分开训练或者使用预训练模型</li>
<li>e2e 相关
<ul>
<li><code>intermediate map representation</code>: 输入和输出之间的中间数据, 一般用于提取关键信息</li>
<li><code>egocentric map</code>: 从个体的视角（通常是观察者或移动体的视角）表示环境的地图</li>
</ul></li>
<li>模块化相关
<ul>
<li>一般包括: <code>visual encoder</code>、<code>mapper</code>、<code>exploration</code>(决定去哪探索)、<code>planner</code>(决定机器采取什么动作), 以上提及的这些是顺序关系</li>
<li>视觉编码器: 将观察转化为带有语义的编码, 一般使用预训练的模型, 最后可以将检测到的物体放入标好名字的 box</li>
<li>映射器: 接受编码器的特征信息, 构建语义地图</li>
<li>探索: 类似红警 2 的探图能产生信息优势, 由于知道越多的地图信息应该是越好的, 因此设置这个模块鼓励智能探索未知区域, 优先探索哪边可以选取合适的方法, 例如图文相关型, VLM 输出的最佳方向</li>
<li>计划: 在地图建后, 需要指定导航路径, 这部分取决于机器的运动方式等细节</li>
</ul></li>
<li><code>Active SLAM</code>: 强调机器人自己能选择以某种方式主动收集地图信息用于映射, 比如自己走几步, 让摄像头拍到更多画面, 也就是相比 ALAM 增加了 planning 部分。</li>
</ul>
</div>
<p>具体环节:</p>
<ol type="1">
<li><code>localization</code>: 非常依赖于传感器, 由于本领域相对不关注底层细节, 一般假设每次采样都获得了最佳结果或者每次行动都实现了理想的位移
<ol type="1">
<li>由于现实中不可能有理想情况, 我们有 <code>Loop Closure(闭环检测)</code> 这样的算法来矫正误差, 该算法会利用再次访问之前访问过的地点时的数据矫正对行为位移的估计值</li>
</ol></li>
<li><code>feature extraction</code>: 关键部分, 后文详细介绍</li>
<li><code>projection</code>: 存储 3 维地图是一件非常难的事, 而且常识上高度轴上的物体关系较简单, 因此我们常用 2 维地图, 学习时将 3 维信息添加到 2 维地图上, 如果我们通过相机来获取空间信息, 那么过程就是, 根据相机的内置功能算出物体的世界坐标(X, Y, Z)将世界坐标投影到 2 维地图坐标(x, y),</li>
<li><code>Accumulation</code>: 上一步中可能遇到不同的物体堆到同一个二维点的情况, 为此有很多方法, 例如取最大, 取平均, 等, 其中可学习的方法可以有带有 LSTM 或者 GRU 的 RNN</li>
</ol>
<p>此外还有一些权衡的地方:</p>
<ul>
<li>Egocentric vs allocentric</li>
<li>Tracking visited areas</li>
<li>View point selection: 固定视角的摄像头或者全局摄像头</li>
<li>Online vs offline map building: 考虑到盲点等问题, 一般 online 是更好的</li>
<li>现实世界的复杂性: 例如我们以上提到的 localization 假设与现实肯定不符的</li>
</ul>
<p>总结一下常见的问题:</p>
<ul>
<li>计算量较大: 尤其对于本地模型来说, 更需要优秀的算法, 部分任务还会要求低延迟</li>
<li>容量需求: 对复杂的场景, 传统地图已经很庞大了, 加上语义, 复杂度更高, 储存和更新都是个问题</li>
<li>噪声: 现实世界的噪声很多, 还有各种随机因素, 学界目前没有很好的过滤方法</li>
<li>动态的环境: 如果环境不断变化, 现有的方法可能无法有效地更新</li>
<li>语义理解: 这部分较为依赖 clip 之类的视觉语言模型或者 llm</li>
<li>可用性和可靠性: 事实上, 目前的 em-ai 都很难落地</li>
<li>标准化: 目前的机器人或者 ai 社区使用不同的系统或者平台</li>
</ul>
<h3 id="地图结构">地图结构</h3>
<ol type="1">
<li>Spatial map building</li>
</ol>
<p>我们将地图做成一个(M×N×K)的网格, 其中(M, N)是空间上的维度, K 是语义上的 channel, 一般来说过程是这样的: 划分输入图片-&gt; 把图片投影到自我为中心的地图 ego_map -&gt; ego_map 注册进外部坐标系的 allo_map -&gt; 降噪</p>
<ol start="2" type="1">
<li>Topological map building</li>
</ol>
<p>类似图论, 用(v, e)两个集合就可以表示一个图, 节点和边可以各自携带语义, agent 在更新中定位或者新建自己所处 node, 并可以根据一轮中学习的信息对整个图进行修改</p>
<ol start="3" type="1">
<li>point cloud map</li>
</ol>
<p>由于成本较大, 这方面应用有限</p>
<p><img src="/assets/ml/Pasted%20image%2020250911154734.png" /></p>
<p>下面对各个结构详细介绍</p>
<h4 id="spatial-grid-map">Spatial grid map</h4>
<p>现在对室内具身智能的研究通常用 MP3D, HM3D 等数据集, 这些都是对现实空间的 3d 建模, 都小于 1000 平米, 一个 cell 表示 400-900 平方厘米的空间, 有时也会再拓展一个高度维<br />
早期研究直接使用自我中心的映射, 并直接端到端训练, 这样的缺点是无法认知到全局的空间结构, 甚至还会忘掉过去的学习结果, 于是就需要得到外部坐标系的空间地图<br />
但由于智能体只能观察到自己传感器的部分, 也就是其观察内容是自我中心的, 所以想要得到外部坐标系的空间模型, 就需要有一种办法通过一次次的观察结果一笔一笔“画”出全局的地图<br />
我们将这种办法称为 <code>registration 注册</code>, 也就是将每次的自我中心观察提取到的图像特征一点点放进全局地图, 这其实和人类画地图的方向比较像<br />
每一轮中, 将观察信息融入到一些特定的 grid cells, 如果有重叠, 则通过某种方法把两个值叠加为一个结果值, 这种叠加称为 <code>aggregation</code>(聚合)<br />
有一种基于注册的方法叫做 <code>MapNet</code>, 即通过使用已知的相机内参和对深度的测算, 将 egocentric 图像特征投影到一个 2D 俯视网格上<br />
它执行 <code>注册</code> 的方法是: 首先获取一系列旋转多次后的 egocentric 地图, 然后与之前 allocentric 地图进行 <code>密集匹配</code>, 以确定代理在地图上的当前位置</p>
<p>另一种方法是: 直接通过相机内置函数将图片像素转化为（相对于相机的）3d 坐标, 由于相机的 pose 我们是知道的, 所以可以直接将其转化为世界坐标, 并将其体素化, 投影到 2d 平面时, 我们可以把这些体素的 weight 也压上去<br />
这样的缺点就是太依赖相机功能, 对于现实的噪声敏感, 需要额外的降噪步骤, 使用这种方法的 <code>Semantic MapNet</code> 和 <code>MOPA</code> 都各自用了一些降噪算法, 前者还发现, 先对图像编码, 然后投影, 最后执行分割会减少噪声, 甚至可以不去降噪</p>
<p>总结: 使用 grid 能比较方便地表示空间, 也能方便 agent 学习, 但固定的宽高对于变化环境不太方便, 而且内存占用较多</p>
<div class="note info"><ul>
<li>密集匹配(dense matching) : 计算机视觉和图像处理中的一种技术, 用于在图像或视频序列中找到像素级别的对应关系。与稀疏匹配（只在特定特征点上进行匹配）不同, 密集匹配试图为图像中的每一个像素找到对应的像素。</li>
<li>pose: 物体在三维空间中的位置和方向</li>
<li>体素（Voxel）: 是“体积像素（volume pixel）”的缩写。它是三维空间中的最小单元, 具有位置、大小和属性（如颜色、密度等）</li>
<li>体素化(voxelized): 将三维空间中的物体或场景表示为一组 <strong>体素（voxel）</strong> 的过程</li>
</ul>
</div>
<h4 id="topological-map">Topological map</h4>
<p>拓扑图更注重节点(地标)之间的关系, 这其实也是人类认路的一种方法, 符合常识, 但缺点是抽象层级较高<br />
首先考虑怎么表示给出一个空间, 怎么画它的拓扑图, 什么是节点, 什么是边呢？符合直觉的想法是, 节点有较为重要的语义信息, 例如是一种房间的典型物体, 而边描述节点之间的连接性, 常见方法中节点通常存储图像特征或时间信息 (访问时间戳) 等信息, 而边缘可以存储一对节点之间的相对 pose<br />
实际训练中, 可以让模型预先跑几遍结合这几次的轨迹画出一个 graph, 然后的规划过程寻找 graph 中最接近的节点, 这称为 <code>Semi-Parametric Topological Memory (SPTM)</code>, 这种做法有很多缺点, 例如预探索可能有很多没走到的路, 不适合未知场景等<br />
为了解决这种问题, 我们需要让 agent 能实时地感知到自己有哪边不了解, 应该先探索那边, 也就是 online 学习, 一种做法叫 <code>Neural Topological SLAM (NTS)</code>, 它使用 <code>Graph Update</code> 模块更新 map, <code>Global Policy</code> 模块对 map 采样, <code>Local Policy</code> 模块输出离散的导航行动; 其中图更新模块作用如下:</p>
<ol type="1">
<li>定位: 尝试将智能体在前一个时间戳的图中进行定位</li>
<li>对已存在节点:
<ol type="1">
<li>如果智能体成功定位到一个现有节点, 便在该节点与上一个时间戳的节点之间添加一条边</li>
<li>存储两个节点之间的相对 pose</li>
</ol></li>
<li>如果智能体无法定位, 则在图中添加一个新节点</li>
</ol>
<p>这就产生了另一个问题, 什么情况下是在已有节点, 什么情况下要新增节点呢？这需要判断两个观察值是否相似, 可以视为一个分类（二分）问题, 用一个既有的分类器计算, 也可以用可达性估计或者一些预训练的无监督网络处理(附近拍的观察可以视为一类, 这样就不用手动标注)<br />
与网格地图相比, 拓扑的空间占用少, 但也因此可能漏掉细粒度的信息</p>
<h4 id="point-cloud-map">Point-cloud map</h4>
<p>用过建模软件的都知道, 出于优化性能考虑, 复杂的模型经常用三角形 mesh 表示, 由于训练智能体大多是在虚拟环境中, 点云与 meshes 有不错的适配性<br />
而对于语义信息, 可以直接为每个点赋予一定的寓意, 最后我们就得到了一个不那么传统的 map, 我们一般用一个神经网络来赋予每个(x, y, z)点一个语义向量, 这个网络称为 <code>neural field</code><br />
这方面的应用暂时不算多, 点云更多被用在场景理解或者分类上, 这里暂且略过少数几个例子<br />
光看简介就能看出来, 点云的计算和存储成本都很高, 并且对于稀疏的场景, 提供的信息可能也不足, 是一种有待探索的结构</p>
<h4 id="hybrid-map">Hybrid map</h4>
<p>混合方法也是常见的思路, 一些相关的应用有:</p>
<ul>
<li>拓扑更能捕捉空间之间的关系, 而 grid 更有距离上的细节, 两者结合称为 <code>topometric map</code>
<ul>
<li>例如, 先生成一个粗糙的拓扑图, 根据拓扑图完善一个细粒度的 grid; 实验发现, 拓扑图能修正一些巨大的测量问题, 也就是全局问题, 而网格则能更好地解决局部问题</li>
<li>另一篇文章中, 为了处理紧凑的环境, 作者将转角和走廊用拓扑表示, 房间则用网格表示, 由于一般来说我们只需要房间里有详细的信息, 而对交通空间不关心具体情况</li>
<li>以上两篇都比较古早, 最近的一种基于 bert 的文章离线训练 hybrid maps, 然后训练一个多模态模型进行语言指导式的空间推理</li>
<li>还有结合三者的, 网格存储 occupancy(障碍物)信息, 拓扑图存储地标及其连通性, 点云则存储详细的语义信息</li>
</ul></li>
<li>构建能够捕捉场景语义层次的地图可以实现不同层次的推理, 例如不同的节点实际上在不同层级(坐标系)中而, 边表示坐标系的转化关系; 除了分层, 还可以额外存储物体, agent 之间的时空关系, 实验证明分层的场景表示对复杂的环境效果更好, 这样的技术甚至可以用来对城市的交通系统进行建模</li>
</ul>
<p>混用这些地图结构时需谨慎考虑各自的特点, 例如拓扑和点云地图在大环境中比网格地图更具可扩展性, 但点云需要更多存储空间, 拓扑地图所需的存储空间最少</p>
<h3 id="地图编码-map-encoding">地图编码 map encoding</h3>
<p>地图编码是指将信息通过某种方式存储在语义地图中, 而这些信息可以分为隐式和显式的, 显式指的是可以被直接解释或者理解的信息, 例如像素的颜色, object 所属的类别等; 隐式则指一些提取出来的特征, 无法被直接地理解</p>
<h4 id="显式编码">显式编码</h4>
<p>那么根据常识, 至少障碍物信息, 也就是 <code>occupancy information</code> 信息是很值得显式存储的, 并且可以用一个 bool 就能表示是否占用的二元关系<br />
此外, 为了鼓励 agent 探索未去过的区域, <code>Active Neural SLAM</code> 方法会存储一个 bool 值表示是否已探索<br />
而对于更复杂或者更长期的任务, 例如针对语义目标的导航任务, 就需要额外存储语义信息, 例如 <code>SemExp</code> 会额外存储 agent 认知的语义类别标签, 这些标签通过 <code>MaskRCNN</code> 产生, 在 <code>aggregate</code> 时使用 <code>element-wise max pooling</code>(取各个子区域里的最大值), 且保留最新的预测值<br />
首先将图像分割再投影会造成 <code>label splattering</code> 现象, 也就是一些噪声标签会散布到多个 <code>grid cells</code>, 这是因为对深度的观察会受到环境噪声的影响; 但首先投影编码后的特征再分割能实现一定的降噪效果(需要预探索)<br />
此外, 图像, 文字, 目标检测概率(对目标属于某个类别的置信度评分)等值也可以作为存储标签, 近年一些图文, 图图匹配也有不错的效果; 存储音频信息(例如声音强度)的地图在视听导航任务中也有作用;<br />
以上都是网格地图的应用, 拓扑图中, 可存储每次都会替换更新的访问时间戳, 用来表示地点之间的时间相对关系</p>
<p>总结: 显式编码的优点是其可解释性, 对于具体的任务, 我们可以选择常识上有益的信息类型并编码存储, 但是这较为依赖人类对问题的理解</p>
<h4 id="隐式编码">隐式编码</h4>
<p>大部分的早期工作使用预训练的基于封闭词汇表的视觉模型提取特征, 例如 cnn, 近年来, 更多使用预训练的基于开放词汇表的大型视觉-语言模型<br />
封闭词汇表:</p>
<p>可以使用 cnn 或者流行的 <code>vision model</code> 例如 <code>ResNet</code> 除了图像以外, 也可以根据可微分的映射器和 <code>planner</code> 进行非监督学习, 通过计算微分, 模型可以不断优化性能, 地图则可以通过 <code>differentiable warp</code> 不断集成</p>
<p>开放词汇表:</p>
<p>封闭词汇表最大的漏洞是: 无法编码不认识的特征, <code>Large Vision-Language Model (LVLM)</code> 如 <code>CLIP</code> 可以缓解这个问题, 这些模型有庞大的训练资料, 对于不认识的物品, 可以通过已有知识建立新的类别<br />
<code>CLIP</code> 有强大的图文匹配能力, 利用这点, 可以在 2Dmap 中存储图文相似得分(<code>value maps</code>), 这在下游应用例如 <code>zero-shot</code> 的语言驱动的导航中表现良好, 也有论文将其拓展到 3D<br />
尽管其匹配能力相比封闭词汇表十分强大, 但仅限于图片和文本的匹配, 依旧缺乏更详细的语义和空间信息, 因此空间推理能力欠缺<br />
为了增强推理能力, 需要找到判断物体在图片中的位置, 以及提取特征的方法, 一个思路是对图片的所有像素嵌入一些特征, 根据摄像头的深度信息, 可以得知这些像素对应 3d 空间的坐标, 随后投影(聚合)到 2d 网格上, 在查询时, 先从输入提取出物品名, 再和这些像素匹配<br />
这个方法的缺点是, 并没有做到物体级别的语义, 可能忽略了空间上的一些关系, 此外很多像素可能就是空气, 毫无语义信息, 没有必要存储。24 年(<code>OneMap</code>)的一种方法使用分层的编码器缓解了这个问题<br />
NLMap(grid): 如果想直接辨别图像里的物品, 则可以使用 <code>class-agnostic region proposal network</code> 来划分出感兴趣的图像区域(region), 对于这些 region, 提取特征并将其存入 3d map(携带坐标和估计的大小信息), 这样一来, 对每次查询(会被先转化为物体名), 只要查找匹配度最高的 region 就可以了<br />
<code>ConceptGraphs(topo)</code>: 使用无类别的 2d 分割算法划分出物体, 这些物体可以直接作为拓扑图的节点并存储一些信息, 除了特征信息外, 例如对其的描述, <code>Candidate Masks</code> 的点云等也可以存储。接下来考虑边, 在这种方法中, 考察物体的点云是否有几何相似性或者重叠部分, 有两个物体的点云重合度到达一个阈值, 则认为它们是空间相关的, 从而建立边。边也可以额外存储信息, 例如大模型对链接方关系的描述<br />
开放词汇表映射编码的优点是它可以一次构建, 然后复用到不同的下游任务。它可以有效地使用开放词汇表进行查询, 并且具有较高的可解释性<br />
缺点是, 当前阶段大模型的训练开销和计算开销都极大</p>
<div class="note info"><ul>
<li><code>Zero-Shot Manner（零样本方式）</code>: 在机器学习和人工智能中, 模型能够在没有针对特定任务或类别进行微调的情况下, 直接执行任务的能力</li>
<li><code>class-agnostic region proposal network</code>: 一种用于目标检测的网络架构, 旨在生成图像中潜在目标的区域分割, 而无需事先知道目标的具体类别, 如 <code>ViLD</code></li>
<li><code>Candidate Masks</code>: 计算机视觉中的一种概念, 通常用于目标检测和图像分割任务。它们代表了图像中可能包含目标的区域或对象的区域</li>
</ul>
</div>
<h3 id="地图评估-map-evaluation">地图评估 Map Evaluation</h3>
<p>很明显, 对智能效果最直接的评价标准就是 agent 执行任务的效果, 因此目前对 map 的评估较少, 本文讨论如何从准确性, 完整性, 一致性, 健壮性以及实用性的角度评价地图对于下游任务的功效</p>
<ul>
<li>Utility 实用性: 大部分工作将语义地图作为一个中间步骤, planner 使用它来规划路径, 产生动作指令, 如果我们只用地图做这一件事, 那么直接看任务完成地怎么样就是最好的评价标准了; 但对一些和地图有关的任务, 例如导航、勘探, 则也可以用覆盖率, 导航准确性等作为指标<br />
</li>
<li>Accuracy 准确性: 地图准确度是指与现实地形进行比较时, 地图捕获语义信息的准确度, 问题在于, 很多时候现实的地理信息较难获取。如果只论语义的比对, 则可以使用一些语义分割上的指标, 与智能的语义划分进行比对; 还有一个问题是, 这种比较对常常种类有限的显式特征较为方便, 但对隐式的特征则无从下手, 对此只能再用一个分割器将隐式转化为显式, 又或者人工评估</li>
<li>Completeness 完整性: 地图是否完整地表示环境, 这包括了几何和语义层面。这点很大程度上取决于机器人在下游任务中探索环境的彻底程度, 并且与 “停止标准 <code>stopping criteria</code>” 密切相关, 一般来说, 会在任务完成或者达到时间限制时停止; 而语义上的完整则很难测算, 几乎没有成熟的方法</li>
<li>Consistency 一致性: 几何一致性指的是地图的空间结构能反映环境物理布局的准确性, 不严谨的说可以视为地图局部细节(距离, 角度, 物体相对位置)的准确性, 模拟环境下不存在传感器噪声, 因此几乎可以不考虑, 但实际情况中, 由于各种因素影响, 代理可能错估自己走过的距离, 相对位置等, 从而破坏一致性; 语义一致性则指随着机器人运动, 语义信息和物理位置能否对齐。由于当前还处于理论阶段, 这方面的研究尚且不足</li>
<li>Robustness 健壮性: 在不可预测或动态环境中的可靠性, 由于现在大量使用预训练模型, 可以用预测的置信度来评估; 此外也可以使用模型预测的方差来评估, 但由于能落地的应用有限, 这方面的研究也比较缺乏</li>
</ul>
<h3 id="展望未来与结语">展望未来与结语</h3>
<p>当前的趋势为创建灵活、通用、开放词汇和可查询的地图, 以支持多种任务; 为了提高空间推理能力, 密集, 可扩展和内存高效也是一种方向</p>
<ul>
<li>General-purpose maps: 由于机器人任务多样化, 通用的模型明显有着重要意义, 更有通用能力的开放词汇表也可能是趋势, 这需要对计算成本和内存消耗的平衡<br />
</li>
<li>Dense yet efficient maps: 为了更好的空间推理能力, 需要高密度的地图, 但又为了计算效率, 需要其尽可能节省内存和算力, 常用方法中拓扑图过于稀疏难以捕捉细粒度信息, grid 难以处理多层面信息, 点云则过于密集, 性能消耗大, 可能需要一个更好的结构</li>
<li>Dynamic maps: 当前的地图技术基本都是基于静态环境, 对动态环境如室外的交通, 则很难有高效地建模方法</li>
<li>Hybrid map structure: 上文已经提到过混合结构的优点, 但如何融合或者切换也是值得研究的方面</li>
<li>Devising evaluation metrics: 与下游任务中代理表现的评估相比, 语义地图的评估在具身 AI 研究中受到的关注较少。为了推动该领域的发展, 需要强调使用准确性、完整性、一致性和健壮性等指标对地图进行评估。</li>
</ul>
<h2 id="dualmap-在线开放词汇语义建图助力智能体自然语言导航">DualMap: 在线开放词汇语义建图助力智能体自然语言导航</h2>
<h3 id="背景">背景</h3>
<p>鉴于近年 ai 领域的发展, 机器人领域逐渐开始研究对于自然语言理解能力的集成, 对此可以分为 3 种任务:</p>
<ol type="1">
<li>开放词汇理解</li>
<li>高效地在线建图</li>
<li>动态环境导航</li>
</ol>
<p>一些经典方法如 yolo(深度学习的对象检测模型), 虽然表现较为不错, 但鉴于它基于封闭的词汇表, 对于 "不认识" 的类别是没有辨别能力的<br />
而所谓的开放词汇, 就是需要让模型能够理解没见过的类型, 并逐渐学习怎么更好地分类和认知, 常见的方法有:</p>
<ol type="1">
<li>通过图像标注模型标出标签, 再根据标签使用基于封闭集合的检测器, 不过图像模型成本较为高昂, 很难在线学习</li>
<li>类别无关分割: 使用一种不依赖具体类别的专用分割模型, 再借助视觉基础模型(VFM)提取语义特征, 开销也较大</li>
</ol>
<p>其主要的创新点正如其名, 在开放词汇的基础上使用两种地图, 空间占用较低的抽象地图用于导航任务, 具象地图在需要更多细节时用于参考</p>
<h3 id="dualmap-的改进">DualMap 的改进</h3>
<ol type="1">
<li><p>开放词汇的目标分割<br />
利用 llm 提供类别, 让 yolo 快速生成粗略的目标检测结果, 并通过 mobileSAM 生成分割掩码; 同时让 FastSAM(开放词汇表分割模型)捕捉 yolo 的封闭表外的对象(这步可能会过度分割, 之后还会进行合并操作); 对两者进行融合时 yolo 优先, 补充不重复的 FastSAM 片段;这样的设计主要是为了使用成本较低的 yolo 和 fastsam, 从而实现在线学习<br />
这里补充一下对每一帧 I 的处理, 事实上有三个模块会并行地使用 I, 即上述的 YOLO, FastSAM 以及一个点云生成器, 点云地图和语义信息无关, 只是用于提供空间布局</p></li>
<li><p>语义特征处理<br />
由于我们希望能让机器人做到能理解模糊的指令, 就需要让地图也携带语义信息, 这方面也有很方便的图文嵌入模型例如 CLIP, CLIP 提供对图像的编码功能, 在第一步的分割后, 对裁剪出的图像区域编码为 <span class="math inline">\(f_{image}\)</span> , 如果之前的分割给出应该类别标签, 对标签编码为 <span class="math inline">\(f_{text}\)</span> , 最后通过加权求和得到总的特征<br />
<span class="math display">\[\mathbf{f}=w_{\mathrm{image}}\mathbf{f}_{\mathrm{image}}+w_{\mathrm{text}}\mathbf{f}_{\mathrm{text}},\qquad\mathbf  {f}_{\{ {\mathrm{image,~text}\}}}   \in\mathbb{R}^{d}\]</span><br />
对于 FastSAM 标记为“null”的对象, 用所有已知类别嵌入的归一化平均文本特征替换 <span class="math inline">\(f_{text}\)</span>  , 以消除语义偏差</p></li>
<li><p>观察结构<br />
摄像机拍到的一帧为 I, 每个 I 提取出一个观察集 Z, Z 由 N 个片段 <span class="math inline">\(z_i\)</span> 组成, z 称为观察 , 可表示为: <span class="math inline">\(z=(P_{z},f_{z},y_{z},t_{z})\)</span><br />
其中 P 表示分割区域从深度图投影到世界坐标系中生成的 3D 点云; f 表示之前我们得到的语义特征; y 表示 yolo 检测的对象类别, 对于 fastSAM 分辨出的则设置为 null; t 为观测的时间戳</p></li>
<li><p>场景建模<br />
本模型使用点云对场景建模, 但为了减少性能消耗, 只在遇到姿态变化大的帧时通过投影(相机带有深度信息)更新点云(这个更新线程并行低频运行)</p></li>
</ol>
<p>dualmap 最大的特点正如其名, 使用两种 Map, 下文分别介绍</p>
<h4 id="具象地图">具象地图</h4>
<ol type="1">
<li><p>地图初始化<br />
地图有一系列对象组成, 每个对象, 这里称为 o, <span class="math inline">\(o=(P_{o},y_{o},f_{o},L_{o})\)</span><br />
其结构类似于观察 z, 但其中的 L 记录与对象关联的观测集, 即 <code>L=set&#123;z&#125;</code></p></li>
<li><p>地图更新<br />
当新的观测集合到达时, 系统开始新一轮匹配(第一轮所有观测 z 都会分配一个对象 o), 定义一个相似度矩阵 <span class="math inline">\({ S}\in R^{\mathbb{N}{\times}\mathcal{M}}\)</span> , 从点云重合度和语义相似度两个角度衡量<br />
<span class="math display">\[S(z_{i},o_{j})=\mathrm{cos}(f_{z_{i}},f_{o_{j}})+\mathrm{Overlap}(P_{z_{i}},P_{o_{j}})\]</span> 而更新条件用一个简单的阈值 <span class="math inline">\(\tau\)</span> 表示, 超过阈值认为是相同对象, 更新过程则是对 f 求新增一个 f 后的平均, 扩充点云, L 内部新增加一个观察 z; 否则新建对象</p></li>
<li><p>地图维护<br />
一般来说, 语义地图最“昂贵”的地方就是对 3d 物体进行合并, 为了避免这种开销, 通过轻量级的内部对象状态检查来维护地图的保真度, 可分为:</p></li>
</ol>
<ul>
<li>稳定性检查: 对一个较长时间未更新的对象
<ul>
<li>如果有足够观测次数, 至少有 2/3 的类别是同一个, 则保留</li>
<li>否则删除</li>
</ul></li>
<li>分割检测: 如果有若干连续帧(每一帧产生一个观察集)中在同一时间步内出现具有不同类别 ID 的观测, 例如对象 chair 连续 3 帧对应的时间步内有两个类别标签 chair 和 cushion
<ul>
<li>触发分割操作, 将对象的观测列表按类别 ID 分割并创建新对象</li>
</ul></li>
</ul>
<h4 id="抽象地图">抽象地图</h4>
<p>上一节我们讨论的地图依然是不包含全部语义信息, 只有对象(包括一些类别 id)信息的具象地图<br />
而相比前者, 抽象地图牺牲易变对象, 只存储锚点对象, 且新增一些语义信息与空间关系信息, 具体包括:</p>
<ol type="1">
<li>锚点对象
<ol type="1">
<li>首先使用两个代表性的锚点, 易变类别列表(list)对现有对象分类, 使用 CLIP 对对象编码得到特征 f, 如果 f 和两个列表的近似度相差达到一个门槛值, 则分到相似度更高的一类, 此时的门槛值较小, 仅设 0.05</li>
<li>除了 1.中的两个列表, 典型锚点对象的列表还有一个存储对应描述信息的列表, 对 1.中没有决定的对象, 计算其特征与描述列表的相似度, 这里设一个较大的门槛值 0.5, 并且强制二分</li>
<li>丢弃易变对象的非语义属性(因为对导航任务没用)</li>
</ol></li>
<li>空间关系(易变与锚点对象之间), 以 on 关系为主:
<ol type="1">
<li>以锚点对象点云的 Z 轴直方图以提取其支撑平面, 计算对象与锚点 2d 投影的重叠比例, 如果两者重叠且对象底部到支撑平面的垂直距离小于阈值, 则存在 on 关系</li>
<li>建立空间关系后, 易变对象的语义特征 f 将存储于对应锚点对象的特征列表 L(L 存储于其关联的易变对象语义特征), 对每个锚点对象 a, <span class="math inline">\(a\,=\,(P_{a},f_{a},y_{a},L_{a})\)</span><br />
</li>
</ol></li>
<li>场景布局
<ol type="1">
<li>将点云投影到鸟瞰平面, 划分出单元格, 对最密集的地方, 视为墙壁等结构物, 这种结构物只在新的查询请求到来时更新</li>
</ol></li>
</ol>
<h4 id="导航策略">导航策略</h4>
<ol type="1">
<li>候选检索<br />
对查询语句, 用 CLIP 编码为语义特征, 并与我们现有的锚点以及易变对象的特征集比较, 得分最高者作为候选, 候选锚点称为 a*<br />
锚点得分的计算公式:</li>
</ol>
<p><span class="math display">\[s(a)=\mathrm{max}\Biggl(\mathrm{cos}\big(f_{q},\,f_{a}\big),\,\mathrm{max\underset{i}\,cos}\big(f_{q},\,f_{v i}\big)\Big)\]</span> 其中 q 下标表示查询, a 下标表示锚点对象, vi 下标表示易变对象</p>
<ol start="2" type="1">
<li>导航策略
<ol type="1">
<li>全局路径规划: 使用基于 Voronoi 图的规划器在抽象地图上规划一条通往候选锚点的全局路径; 智能体移动时, 会逐步构建局部具象地图, 局部地图中的对象也可以用于匹配</li>
<li>局部路径规划: 对局部具象地图中的对象, 计算其特征与查询特征的余弦相似度 s, 如果 s 接近之前的全局候选相似度, 且正好在去之前的候选对象的路上, 使用 RRT*算法规划局部路径</li>
<li>动态环境导航: 如果 a*附近没有可信匹配, 那么环境可能变化, 系统会更新抽象地图 Ma'重新寻找候选
<ol type="1">
<li>Ma'会将局部具象地图中的稳定对象合并, 详解下文</li>
<li>更新地图后, 全局相似度分数依旧使用过去的版本, 即保留历史上下文信息</li>
</ol></li>
</ol></li>
<li>抽象地图更新
<ol type="1">
<li>对局部具象地图中的稳定对象, 进行一轮抽象化得到新的锚点 <span class="math inline">\(a_{new}\)</span></li>
<li>每个 <span class="math inline">\(a_{new}\)</span> 与现有的 a 比较, 计算重叠率
<ol type="1">
<li>如果重叠率超过门槛值, 说明 <span class="math inline">\(a_{new}\)</span> 和 a 有一定关系, 更新: <span class="math inline">\(f_{a}~\longleftarrow~{\frac{\left|P_{a}\right|\cdot{f}_{a} + \left|\mathcal{P}_{a_{\mathrm{new}} }\right|\cdot\ {f}a_{\mathrm{new}} } {\left|\mathcal{P}_{a}\right|\ +\left|\mathcal{P}_{a_{\mathrm{new} } }\right|} }\)</span></li>
<li>如果超过更严格的阈值, 则用 <span class="math inline">\(L_{a_{new}}\)</span> 替代 La</li>
<li>如果以上都不满足, 将 <span class="math inline">\(a_{new}\)</span> 作为新节点插入抽象地图中</li>
</ol></li>
</ol></li>
</ol>
<div class="note info"><ul>
<li><code>RRT*（Rapidly-exploring Random Tree Star）</code> 算法: 一种用于路径规划的优化算法, 尤其适用于高维空间中的移动机器人和其他自动化系统; 使用随机采样的方法在配置空间中探索路径, 逐步构建一棵树, 与基本的 RRT 算法不同, RRT* 在扩展树的过程中, 会不断优化已有路径, 尽量减少路径的成本</li>
<li><code>平均交并比（mIoU）</code>: 计算分割或检测任务中预测区域与真实区域交集与并集之比的平均值</li>
<li><code>频率加权交并比（F-mIoU）</code>: 对 mIoU 的一种加权版本, 考虑了每个类在数据集中出现的频率, 将每个类的 IoU 乘以该类在数据集中出现的频率, 然后求和, 最后除以总频率</li>
</ul>
</div>
<h3 id="测试结构与总结">测试结构与总结</h3>
<p>定义两种典型的变化: In-anchor relocation(和一个锚点有关的对象移动位置但不改变和锚点关系, 例如桌子上移动水杯)、Cross-anchor relocation(易变对象转换锚点, 例如从桌子移到架子上)<br />
对锚内变化, dualmap 可以通过更新局部具体地图找到目标;对于跨锚点变化, dualmap 可以通过更新抽象地图解决<br />
与 <code>ConceptGraphs</code> 和 <code>HOV-SG</code> 相比, dualmap 在表现提升之外, 内存使用量大大减少, 相比 <code>HOV-SG</code> 减少了 96%以上, 这是因为其避免了 3d 合并开销, 抛弃了多余的易变对象非语义数据, 并执行稳定性检查减少噪声数据等<br />
消融实验中, FastSAM、YOLO 细化、加权特征合并和对象分割检测等组件对性能有显著影响</p>
<h3 id="附录">附录</h3>
<p>实验中存在一些问题:</p>
<ul>
<li>YOLO 可能会将不一致的类标签分配给同一对象的不同部分, 基于类别的合并将无法将它们识别为一个对象</li>
<li>对多个堆叠关系或者离得很近的类的对象, 容易导致错误合并</li>
</ul>
<p>对合并问题, 在合并前使用一次额外的 rgb 检测尽可能经济地解决, 也就是对待合并对象画出 rgb 三个 channel 的直方图, 根据直方图计算余弦相似度, 超过一个阈值时才合并</p>
<h2 id="conceptgraphs-open-vocabulary-3d-scene-graphs-for-perception-and-planning">ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning</h2>
<p>来自 ICRA 2024</p>
<p>这篇论文写得比较早(2023), 所以前言部分这里写得简略一点, 在当时主流的技术往往是封闭词汇或者一些性能消耗大的开放词汇表(也就是携带语义特征的点云);此外还有个痛点是难以表达结构信息<br />
本文提出的感知图是一种以对象为中心的语义地图, 其几何信息由 3d 形式存储, 语义信息由 2d 形式存储</p>
<h3 id="d-对象生成">3d 对象生成</h3>
<ol type="1">
<li>类别无关的 2d 图像分割: 当传入观察帧后, 使用分割模型得到一个掩码集合 { <span class="math inline">\(m_{t,i}\)</span> } , 每个掩码得到图片片段, 将这个片段传入视觉特征编码器如 CLIP, DINO, 从而得到视觉特征向量;此外对片段中的点, 根据深度形象投影到 3d 坐标系中, 对投影后的点云使用聚类算法降噪, 最后放入地图中;最后我们得到了点云以及对应的单位归一化后的语义特征向量</li>
<li>对象中心的 3d 图: 令观察帧(rgbd)为 <span class="math inline">\(I={I_t}\)</span> , 其中 <span class="math inline">\(I_{t}\ =\ \langle I_{t}^{\mathrm{rgb}},I_{t}^{\mathrm{depth}},\theta_{t}^{*}\rangle\)</span> (color image, depth image, pose) 类似的, 图 M ={O, E}中的对象集合为 O, 边集合为 E<br />
</li>
<li>对象关联: 对每个新检测到的对象 <span class="math inline">\(o_t\)</span> 即 <code>&lt;p,f&gt;</code>(点云和特征向量), 计算它与在 3d 图上有空间重叠的对象 <span class="math inline">\(o_{t-1,j}\)</span> 的几何以及语义相似度
<ol type="1">
<li>几何相似度 <span class="math inline">\(\phi_{\mathrm{geo}}(i,j)\;=\;{\mathfrak{n}}_{\mathrm{nnratio}}({\bf p}_{t,i},{\bf p}_{oj})\)</span> 也就是 <span class="math inline">\(p_{t,i}\)</span> 中的最近的邻居点在 <span class="math inline">\(p_{oj}\)</span> 中的点所占比率(有一个门槛值)<br />
</li>
<li>语义相似度 <span class="math inline">\(\phi_{\mathrm{sem}}(i,j)\,=\,\mathbb{F}_{t.i}^{T}\mathbf{f_{oj}} / 2+1/2\)</span></li>
<li>以上两种相似度的简单相加作为总体相似度, 权衡计算花费和表现后, 贪心地选择相似度最高的对象匹配(如果超过一个门槛值), 而如果不到门槛值, 创建一个新对象</li>
</ol></li>
<li>对象融合: 对上一步关联的两个对象, 进行融合, 融合过程就是一个简单的求平均: <span class="math inline">\({\bf f_{o j}}\,=\,(n_{o_{j}}{\bf f_{o j}}+{\bf f}_{t,i})/(n_{o_{j}}+1)\)</span> 其中 n 是迄今为止(融合前)被关联到这个对象的观察片段数量, 点云部分则执行一个简单的并集操作, 并进行下采样降低成本</li>
<li>节点描述: 很实用主义的一步, 在我们暂时完成了一系列图像处理后, 对得到的对象, 将最好的 10 张图片送给 LVLM 处理, LVLM 生成对应的粗略描述, 再将其送给一个 LLM 生成一个最终描述</li>
</ol>
<p>简单地概括一下, 上述主要处理空间和语义上的物体(对象)表示, 空间部分是从二维图片提取的点云, 确定点云时和语义类别无关, 直到确认后才编辑语义部分, 提取语义的过程也很单纯, 将图片发给 LVM 和 LLM 得到描述, 每个对象存储一个关于自己的描述</p>
<h3 id="d-场景图生成">3d 场景图生成</h3>
<p>之前已经得到了对象层面的抽象, 这部分主要处理对象关系<br />
对每对 3d 对象, 计算器 3d 边界 Box 的 IoU 生成一个相似度矩阵, 并通过估计应该最小生成树 MST 来修剪矩阵, 从而得到数量可控的候选边<br />
(可能有人好奇为什么之前的描述不进行编码直接存储, 这是因为后面还会对 LLM 会使用描述信息)<br />
对这棵树的每条边, 将边对应的一对 obj 的描述和位置信息喂给 LLM, LLM 生成其可能的关系, 这个关系作为场景图的标签存储, 例如 on, in<br />
之后, 对自然语言类型的查询, 让 LLM 读取一个存储好相关信息的对象列表, 找到最有可能的关联对象并将这个对象交给下游应用<br />
当然还有一个很常见的思路是, 为什么不舍弃描述性信息, 直接使用 CLIP 之类的模型嵌入编码查询语句然后查找匹配对象呢？ 答案很简单: 实验者尝试过, 但效果不如 LLM, 准确地说, clip 对描述性的查找表现不错, 但对于比较模糊的查询表现不佳<br />
相比更近年的 dualmap, 这个模型还有一个比较直观的问题, 对动态环境无能为力, 但优点在于 llm 的泛化能力能解决很多导航问题, 例如一开始没找到之后去哪里找, 有障碍物的路径其障碍物能否推开等</p>
<p>场景图对象的 json 表示示例:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span>  </span><br><span class="line">    id<span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">    bbox_extent<span class="punctuation">:</span> <span class="punctuation">[</span><span class="number">2.0</span><span class="punctuation">,</span> <span class="number">0.7</span><span class="punctuation">,</span> <span class="number">0.6</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    bbox_center<span class="punctuation">:</span> <span class="punctuation">[</span><span class="number">-0.6</span><span class="punctuation">,</span> <span class="number">1.1</span><span class="punctuation">,</span> <span class="number">-1.2</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    object_tag<span class="punctuation">:</span> &#x27;wooden dresser or chest of drawers&#x27;<span class="punctuation">,</span></span><br><span class="line">    caption<span class="punctuation">:</span> &#x27;A wooden dresser or chest of drawers&#x27; </span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>简评: 无疑这个系统很实用, 但过于依赖 LLM 了, 文本描述很可能不是最好的空间表示方式</p>
<div class="note info"><ul>
<li>下采样（subsampled）: 简单地说, 尽可能保留信息地压缩原数据, 上采样与其相反</li>
</ul>
</div>
<h2 id="hierarchical-open-vocabulary-3d-scene-graphs-for-language-grounded-robot-navigation">Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation</h2>
<p>来自 RSS 2024</p>
<p>本文提出的模型简称为 HOVSG, 相比同行最大的特点在于它是分层结构的, 我们之前提到的工作往往只是用拓扑图基于一个简单的图论空间模型表示现实空间, 也就是对象层级的地图, 但对于更加复杂的现实空间例如多层建筑, 其表现力就会受限, 因此本文使用分层的模型增强表示能力</p>
<p><img src="/assets/ml/overview-icra.png" /></p>
<p>本模型专注于解决对有 rpgd 相机和里程计(里程计用于构建坐标系)的多层环境建图问题, 其最初的 3d 建图和上一章的感知图比较类似, 之后增加分层信息<br />
下面介绍其流程, 和感知图类似的部分会略过</p>
<ol type="1">
<li>进行类别无关的分割, 并将片段投影为 3d 点云, 其合并算法与感知图不同, 重叠度记作: <span class="math inline">\(R(m,n)=m a x(o\nu e r l a p(S_{m},S_{n}),o\nu e r l a p(S_{n},S_{m}))\)</span> 其中 <span class="math inline">\(overlap(S_a, S_b)\)</span> 表示 Sa 中的点在 Sb 的邻域内（在一定距离内）的比率, 但合并不是一对对进行的, 而是构建一个全连接的图, 每条边存储连接的一对节点(片段)的重叠度, 这样就能找到一些重叠度较高的子图, 将所有子图一起合并</li>
<li>接下来开始计算特征, 这里还是用我们的老熟人 CLIP, 但有一些 trick, 由于之前的分割我们其实是有各个区域的掩码的, 按常识想, 似乎应该只把分割出的部分用来求特征, 但也有些工作会把掩码部分和整个图像都求特征算加权和, 根据本文团队的经验, 其特征也是加权和的形式, 写作: <span class="math inline">\(f_{i}=w_{g}f_{g}+w_{l}f_{l}+w_{m}f_{m}\)</span> 其中 i 表示第 i 个 2d 掩码, g 表示整个图像, l 表示基于掩码的图像裁剪, m 表示在 l 基础上再去除背景的图像裁剪, 详细可参考上图左侧
<ol type="1">
<li>接下来考虑怎么将算出的语义和点云表示的空间相关联, 这里简单地说, hovsg 采取了计算消耗最大的方法, 用 3d 点云存储语义, 但是会进行一定的优化</li>
<li>在准备阶段, 会预计算出大概的点云, 这称为参考点云, 而对在之前一步中得到的掩码区域, 会计算逐点特征并将其映射到(参考点云的) 3d 坐标系中, 映射中片段的各个点会与参考点云中的最近点匹配, 也就是语义特征会关联到那个对应点, 称为参考点, 在映射结束后,对各个参考点关联到的语义算平均作为其最终语义</li>
<li>在 2. 结束后, 我们得到一个有逐点语义信息的参考点云, 接下来反过来从片段内的点出发匹配参考点作为语义, 之后在片段内用 DBSCAN 算法进行聚类(将最密集的聚类中最接近平均值的特征作为片段语义)达到降噪和规避模式崩溃的效果, 以上三步可参考上图中间, 也就是空间上每个片段只存储一个特征</li>
</ol></li>
<li>在完成语义存储后, 接下来的任务就是构建场景模型, 本文的抽象数据结构是主流的拓扑图, 但在设计上有一些 trick , 准确地说应该是分层图
<ol type="1">
<li>其中 <span class="math inline">\({\cal N}\;=\;{\cal N}_{S}\;\cup\;{\cal N}_{F}^{\;}\;\cup{\cal N}_{R}\;{ {\cup}}\;{\cal N}_{O}\)</span> 其中 s 下标是根节点, 其他则是 floor, room, object 的简称</li>
<li>类似的, 边集合定义为 <span class="math inline">\({ \mathcal{E}}={ \mathcal{E}}_{S F}\cup{\mathcal{E}}_{F R}\cup{\mathcal{E}}_{R O}\)</span> , 也就是自上到下的树状关系</li>
<li>以上的定义都比较常识化, 接下来讨论具体的实现:
<ol type="1">
<li>floor: 对目前已有片段的点云画出高度上的直方图, 然后寻找一些峰值(也就是找到点相对最多的高度), 具体来说, 只选择超过最高峰 90%或以上的峰值;用 DBSCAN 选择每个片段点云中排名最高的两个峰, 用常识想, 应该其中最密集的高度是楼层底面或者天花板; 对最后得到的所有候选高度(存储为一个向量)排序, 文章假设每两个连续的高度对是天花板和地板的高度</li>
<li>room: 基于划分好的层级, 画一个 2d 鸟瞰直方图, 由于一般墙壁处是最密集的, 用一个阈值就可以分离出大概, 然后用一个 EDF 加上分水岭算法完善分割; 同样基于常识, 房间应该有一些语义信息, 将一些观察帧用于嵌入, 用 k-means 方法选出 k 个代表;推理时, 将一系列常见房间类别输入 clip 编码, 然后将这些编码和之前的房间代表信息计算余弦相似度, 这里的代表其实是个双关, 每个房间有 k 个代表, 这些代表每人一票投给与其最类似的房间种类, 然后根据票数或者投票分数决定房间种类</li>
<li>object: 得到房间划分后, 我们其实就能知道之前的片段各自落在什么房间, 而对不落在任何房间里的对象, 将其与欧几里得距离最小的房间相关联, 类似之前的工作, 这部分也会进行重叠度高对象的合并(空间标准前文写过, 语义标准是对封闭词汇表的查询, 或者说分类任务, 有相同标签输出), 每个对象存储自己的 3d 点云, 片段信息, 特征和最大得分的标签</li>
<li>互动图: 对机器人来说, 很重要的是整个场景中物件的可交互性, 在本文的模型里, 还涉及对跨层交通的标注, 这使用一个 Voronoi graph(下文简称 v 图)实现; 在此前我们得到了各个楼层的地图, 将相机姿势作为一个个点投影到各层平面, 假设这些轨迹点的两两连线周围(有点像轨迹的描边)是可通行的, 此外, 根据高度范围 <code>[ymin + δ1, ymin + δ2] 其中ymin表示楼层内的最低点高度, 而两个偏移是经验值</code> 的点画出障碍物地图; 对姿势区域图和楼层平面区域图做并集并减去障碍物区域图, 就得到了每层的自由空间图; 对跨层交通(楼梯), 将分类为楼梯的片段上的相机姿态点连起来, 然后寻找上下两层中与楼梯最近的点, 将这些点都连起来, 就得到了跨层区域</li>
</ol></li>
</ol></li>
<li>完成以上 3 步后, 现在我们已经得到了完整的语义地图, 对导航任务, 使用 LLM 将其分解为 3 个子任务, 也就是楼层、房间和对象, 分解后是一个常见的处理; 嵌入查询语句并通过比较余弦相似度选择候选</li>
</ol>
<p><img src="/assets/ml/image-7.png" /> <img src="/assets/ml/image-9.png" /> <img src="/assets/ml/image-8.png" /></p>
<div class="note info"><ul>
<li>分水岭(Watershed)算法: 广泛应用于图像处理的分割技术, 特别用于图像的边缘检测和区域分割</li>
<li>欧几里得距离场（Euclidean Distance Field, EDF）: 在一个给定的空间中, 欧几里得距离场为每个点分配一个值, 这个值是该点到某个特定形状（如物体边界或目标点）的最短距离</li>
<li>Voronoi graph: 一种用于将空间划分为离散区域的图形表示方法, 其分割出的一个个单元是泰森多边形</li>
</ul>
</div>
<h3 id="实验">实验</h3>
<p>语义分割上, 用 mIOU 和 FmIOU 评估效果, HOVSG 对比基线模型有不错的提升, 作者将其归功于合并策略使用逐点计算的特征以及聚类算法;此外使用掩码特征的加权和来降低背景图像的影响<br />
对于房间分类,作者比较了基于使用模型预测的房间内物体分类和真实分类数据为输入的 llm 预测和本文方法(特权和非特权), hovsg 战胜了非特权与基于 gpt 3.5 的特权方法,惜败于特权 gpt4<br />
对于对象语义信息, 提出一种新的度量方法 <span class="math inline">\(\mathrm{AUC}_{k}^{t o p}\)</span>, 也就是基于类别(为了通用性横坐标设置为 k%形式)画出 top-k 的准确率曲线, 并计算与坐标轴围成的面积</p>
<p><img src="/assets/ml/auc.png" /></p>
<div class="note info"><ul>
<li>groundtruth: 一般指机器学习样本中的真实数据</li>
</ul>
</div>
<h2 id="open-scene-graphs-for-open-world-object-goal-navigation">Open Scene Graphs for Open-World Object-Goal Navigation</h2>
<p>来自 IJRR 2025 / ICRA 2024</p>
<p>本文引入了 <code>OSG Navigator</code>, 一种开放语义的面向对象,有较强泛化能力的导航模型<br />
鉴于这是一篇很新的工作, 其泛化能力也超越了以上的文章, 上文的分层模型虽然做到了层次结构的语义地图,但过于依赖人为规定好的结构, 本文通过 LLM 等模型提供更强的结构模板,增强对不同环境的适应能力, 此外结构上 OSG 使用模块化设计, 完全用 LLM 这样的基础模型组成<br />
略过相关工作部分, 开始概述这个系统:</p>
<ol type="1">
<li>之前的工作(ObjectNav,2022)输入为 rgbd 信息与无噪声的定位, 而输出是速度命令</li>
<li>在 1. 的基础上, 本系统拓展到了开放世界, zero-shot 任务, 面向现实考虑, 输入只限于 RGB, 输出则是线/角速度, 而代理部分接受自然语言的开放词汇表查询</li>
<li>本文将上述定义的任务视为一个物体和地点中心的 POMDP(<code>Partially Observable Monte Carlo Planning 部分可观察马尔可夫决策过程</code>) 对此定义以下符号 $ s_a , s_r , s_o, s_e, o, r,  , O , $ 分别是: 代理状态,空间地点状态, 物体状态, 物体/地点的关系(edge), MOVETOOBJECT(动作), MOVETOREGION(动作), 过渡, 观察, 目标(开放词汇的描述)</li>
<li>具体的模块分为: LLMs, GNMs(General Navigation Model 提供对不同机器人的泛化导航能力), VFMs(视觉基础模型, 即有完成通用性视觉任务能力的模型) for Visual Question Answering (VQA 视觉问答模型) , VF for Open-set Object Detection</li>
<li>任务流程上: 开放场景图作为基础, 被高层模块调用进行推理,产生图像子目标来指导底层导航模块, 细节上, 映射器会从图像中提取出描写其中突出物体的文本, 这些描述数据送给 LLM 用于更新场景图, 推理器则使用 LLM 从场景图中提取子目标并规划路径, 所谓的路径其实是一个图像目标序列,也就是说其路径点就是图像的经过裁剪的部分(即图像中的某个物体), 由于这些裁剪的物体是中间路径上的, 导航区能使用这些子目标具体地产生动作指令, 同时在导航过程中会动态更新场景与导航计划</li>
</ol>
<p><img src="/assets/ml/image-10.png" /></p>
<h3 id="场景图">场景图</h3>
<p>其场景图大体上可以表示为以下的类似 yaml 的形式,比起只有楼层、房间、物体三级的分层图以及没有分层关系只有物体间关系的双重图都有着更强的抽象能力<br />
可以看到,其并非是隐变量而是显式的文字描述,这就意味着也可以根据经验提供一些预设语义,本文中使用 LLM 来生成<br />
下面介绍细节</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="attr">Floor:</span></span><br><span class="line">  <span class="attr">layer_type:</span> <span class="string">Region</span></span><br><span class="line">  <span class="attr">layer_id:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">contains:</span> <span class="string">Room</span> <span class="string">,</span> <span class="string">Corridor</span></span><br><span class="line">  <span class="attr">connects_to:</span> <span class="string">Stairs</span></span><br><span class="line"></span><br><span class="line"><span class="attr">Room:</span></span><br><span class="line">  <span class="attr">layer_type:</span> <span class="string">Place</span></span><br><span class="line">  <span class="attr">layer_id:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">has:</span> <span class="string">Object</span></span><br><span class="line">  <span class="string">connects_to:Entrance,Room,Stairs</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol type="1">
<li>节点定义: 可分为 4 个抽象类, 除了 Objects 外其他 3 种都可以视为地点(location)类:
<ol type="1">
<li>Objects: 占据局部空间的对象</li>
<li>Places(地点): 较为细粒度的空间,例如对民居来说可以是房间和走廊</li>
<li>Connectors: 连接相邻空间的局部元素, 同时有物体和地点属性, 例如大门, 在语义上可以作为路径点或者探索边界</li>
<li>Region(区域): 比较粗粒度的空间, 可以包含很多局部地点, 例如楼层</li>
</ol></li>
<li>有向边定义, 也可以分为 4 类:
<ol type="1">
<li>Proximity: 空间上的近邻关系, 端点是物体或者连接体, 可用来构建上下文关系</li>
<li>Connectivity: 即可达性关系,端点可以是任意节点,</li>
<li>Inclusion: location 和 object 的单向关系, 表示地点内存在物体</li>
<li>Hierarchy: region/place-region/place 的关系, 捕获层级关系, 源节点必须是区域节点,且目标节点必须是树中下一个较低级别的位置节点(region/place)</li>
</ol></li>
<li>OSG 层次定义, 以下是自底向上的顺序:
<ol type="1">
<li>Objects Layer: 存放独立的物体, 以及它们之间的关系,
<ol type="1">
<li>每个其中的节点保存四个属性:
<ol type="1">
<li>开放词汇标签, 一般就是物体的名字 <span class="math inline">\(\mathcal{v}_{label}^o\)</span></li>
<li>对物体外观的开放词汇文本描述, <span class="math inline">\(\mathcal{v}_{desc}^o\)</span></li>
<li>独特的节点 id, <span class="math inline">\(\mathcal{v}_{id}^o\)</span></li>
<li>图片剪切, <span class="math inline">\(\mathcal{v}_{img}^o\)</span></li>
</ol></li>
<li>除了属性以外, 还会存储 <code>is near</code> 边组成的数组, 例如对有 k 个近邻的节点形如 <span class="math inline">\([(v_{l a b e l}^{o},\ \ v_{d e s c}^{\mathcal{o}})_{0},\ \ ...,\ (v_{l a b e l}^{\mathcal{o}},\ \ v_{d e s c}^{o})_{k-1}]\)</span>, 这个数组可以作为用于区分 object 的特征<br />
</li>
</ol></li>
<li>Places and Connectors Layer: 捕获细粒度的空间位置信息,并描述场景中位置之间的连通性, 在导航期间,机器人的状态被指定为该层的位置节
<ol type="1">
<li>每个 <code>Place</code> 节点 <span class="math inline">\(v^p\)</span> 有三个属性:
<ol type="1">
<li>类别信息, <span class="math inline">\(\mathcal{v}_{cls}^p\)</span> , 来自 OSG 架构,形式是字符串</li>
<li>对地点(place)的开放词汇文本标签, <span class="math inline">\(\mathcal{v}_{label}^p\)</span></li>
<li>独特的节点 id , <span class="math inline">\(\mathcal{v}_{id}^p\)</span></li>
</ol></li>
<li><code>Place</code> 节点的边集包括:
<ol type="1">
<li><code>connects to</code> 边(下文也称连接边)指向 <code>Place/Connector</code>, 表示联通关系</li>
<li><code>contains</code> 边(下文也称包含边)指向 <code>Place</code> 内的物体, 即 Place 的特征可表示为它 <code>contains</code> 的物体的特征(标签和描述)集</li>
</ol></li>
<li><code>Connectors</code> 节点不同于地点节点的是没有 <code>包含边</code> 指向物体节点(或者说它自己就是物体节点), 但是可以通过 <code>连接边</code> 指向其他地点</li>
</ol></li>
<li>Region Abstract Layer(可选): 更加通用的抽象空间划分, 例如对多层的住房场景可以是不同楼层, 从这层开始, 对高层对低层只用 <code>包含边</code>, 但是可以被低层的 <code>连接体</code> 用 <code>连接边</code> 指向,详情可见下图
<ol type="1">
<li>属性只有 <span class="math inline">\(\mathcal{v}_{label}^i\)</span> , <span class="math inline">\(\mathcal{v}_{id}^i\)</span> , 含义类似上文其他节点</li>
</ol></li>
</ol></li>
</ol>
<p><img src="/assets/ml/image-11.png" /></p>
<h3 id="地图生成和定位">地图生成和定位</h3>
<p><img src="/assets/ml/image-12.png" /> Mapper 职能如上图所示, 接下来分组件介绍其生成和更新 OSG 的方式</p>
<h4 id="自动生成-osg">自动生成 OSG</h4>
<p>为了泛化能力, 本模型的环境图结构也由模型来自动生成, 输入一个环境的简单描述, 生成器就会产生一个 OSG 架构<br />
具体地说, OSG 以 json 的形式被生成, 过程分为三个 Pipeline</p>
<ol type="1">
<li>描述生成: 让 llm 生成无结构, 尽可能长, 包含典型空间抽象关系的自然语言描述</li>
<li>规范化(Canonicalise): 让 LLM 将上一步的描述转化为规范图表示, 此外这一步会让 LLM 将节点和边定义为上面提到的类型, 形式上类似于 <code>[Abstraction1, Relation, Abstraction2]</code></li>
<li>验证: 根据 OSG 的结构要求检查规范图, 通过一个测试类检验其结构是否满足上一节定义的 OSG, 如果不满足,将检测到的错误添加到 1. 中的文字描述, 并让 LLM 修正生成的图, 循环直到满足条件,将其以 json 格式返回</li>
</ol>
<h4 id="图像解析-image-parser">图像解析 Image Parser</h4>
<p>使用 VFMs 解析视觉输入, 开放集的物体检测模型来区分标记场景元素, 以及一个 VQA 把图像观察翻译为密集的语言描述(对于 Places, Objects Connectors), 过程为:</p>
<ol type="1">
<li>解析地点: 将机器人周边的 RGB 图像输入 VQA
<ol type="1">
<li>首先提问哪种地点能最好地描述观察帧</li>
<li>得到类别后,继续提问从而获得一个更详细的标签来反映这个地点的语义信息</li>
</ol></li>
<li>物体/连接体解析: 探测器是区分不出本文自己定义出的两类物体的, 但可以得到基于自然语言的物体信息, 将这些信息输入 LLM 进行分类, 然后让 VQA 模型产生对其图像裁剪的描述,使用 <code>is near</code> 边连接两个体积盒距离低于门槛值的一对节点</li>
<li>目标探测: 使用 VQA 或者 LLM 处理上面得到的数据, 如果有和目标很相似的物体,则直接让代理导航过去</li>
</ol>
<h4 id="状态评估">状态评估</h4>
<p><img src="/assets/ml/image-13.png" /> 输入上一步得到的文本描述与已有的 OSG, 输出目前的机器人状态即占据的 Place 节点<br />
流程如上图所示, 其中(P, O, C)指 Places, Objects, Connectors 三种对象, LLM 第一次用于提取一些候选 Place, 这些候选对象根据和上一个 s 的距离被贪心地匹配, 匹配过程中由 LLM 根据两者的描述等信息决定是否匹配成功(优先考虑较大的物体)</p>
<h4 id="osg-更新">OSG 更新</h4>
<p>将来自图像解析器的观察结果递增地集成到 OSG 中, 这部分的难点主要在于门槛的设定, 即更新和新建的选择, 在本文定义的 OSG 还有个问题就是节点间关系的选取<br />
这些问题需要近似人类的 "常识", 因此使用强大的 LLM 来处理, 见下图</p>
<p><img src="/assets/ml/image-14.png" /></p>
<p>这部分伪代码比较抽象,这里简单总结一下:</p>
<ol type="1">
<li>上一步中得到了预估的状态,于是有两种可能
<ol type="1">
<li>新状态之前没有见过:
<ol type="1">
<li>先将其添加进地点集合,并增加边关系(依靠 LLM)</li>
<li>对 region 层及以上遍历 layer:
<ol type="1">
<li>贪心地让 LLM 推断是否是已有 region 还是不存在,如果是后者则新建节点</li>
</ol></li>
</ol></li>
<li>新状态之前见过
<ol type="1">
<li>获取新状态对应的叶节点
<ol type="1">
<li>对本轮观察识别到的物体和连接体,让 LLM 匹配是否也见过,如果没见过就新建节点,否则根据 LLM 输出来更新边关系</li>
</ol></li>
</ol></li>
</ol></li>
</ol>
<h4 id="基于概率的-osg">基于概率的 OSG</h4>
<p>基于 Online Probabilistic Topological Mapping (OPTM 2011), 也就是基于概率的拓扑图<br />
简单地说, 这种方法基于当前的拓扑以及观察来预测之后的拓扑结构, 每个观察都会被连接到一个节点(新建或已有节点), 即计算 <span class="math inline">\(p({\mathcal{T}}_{t}|{\mathcal{T}}_{t-1}{)}\)</span> 其中 T 表示拓扑图结构,对本文来说就是 Place &amp; Connectors 层<br />
每一轮中,实际上做的就是将新的观察链接到新建或者已有节点, 对此有两种可能比较启发性的设置:</p>
<ol type="1">
<li>新的观察 <span class="math inline">\(o_t\)</span> 倾向于关联到频繁访问的节点</li>
<li>新的观察倾向于关联到最近访问的节点</li>
</ol>
<p>换句话说就是时空局部性原理<br />
对于前者,使用狄利克雷过程来建模, 对于后者使用对附近节点的均匀分布建模<br />
本模型中可以这样设置:</p>
<p>重要性权值更新方法设置为:</p>
<p><span class="math display">\[w_{t}^{(i)}\propto\mathcal{P}(o_{t}|\mathcal{T}_{t}^{(i)})w_{t-1}^{(i)},\,\mathrm{for \space i \space th \space particle}\]</span></p>
<p>令 <span class="math inline">\(v&#39;\)</span> 为当前( <span class="math inline">\(o_t\)</span> )所属的节点, V 为语义上类似 <span class="math inline">\(o_t\)</span> 地点的拓扑图节点, 将模型定义为:</p>
<p><span class="math display">\[p(o_{t}|T_{t}^{(i)})=p(o_{t}|v^{\prime})\prod_{v_{i}\in\mathcal{V}\backslash\{v^{\prime}\}}(1-p(o_{t}|v_{i}))\]</span></p>
<p>这个定义可以视为一种贪心策略,也就是假设对每对匹配的观察 o 和节点 v, o 最有可能是由 v 处观察的, 是出自其他节点的概率则更低</p>
<h3 id="推理和控制">推理和控制</h3>
<h4 id="推理">推理</h4>
<p><img src="/assets/ml/image-15.png" /> <img src="/assets/ml/image-16.png" /></p>
<p>推理器接受当前 OSG, 状态, 目标区域(由区域提议组件生成)以及目标物体, 输出最有可能的子目标, 由 3 个组件组成:</p>
<ol type="1">
<li>区域提议: 对每一层询问 LLM 识别最有希望的节点, 遍历每一层的过程中,将候选地点标注为子目标区域</li>
<li>寻路: 从 OSG 中提取 Places 和 Connectors 的连通性子图, 使用迪杰斯特拉规划从当前位置到子目标节点路径</li>
<li>物体提议: 到达子目标后, 查询 LLM 选择其中可能接近目标的对象; 此外, 去子目标的路上也会不断寻找候选子目标, 这是因为机器人大多在地层水使用物体为导航的单位, 该组件用于不断生成 <code>MOVETOOBJECT</code> 行动</li>
</ol>
<h4 id="控制">控制</h4>
<p>这是一个底层的目标,用于不断前进到下一个目标(有点像 rts 游戏里控制单位前进), 具体说就是将 RGB 输入和目标(图像剪切)映射为速度命令, 推理器会不断发送图像剪切目标, 而且都会在机器人传感器能看到的附近位置<br />
本文使用 ViNT GNM 实现</p>
<h3 id="实验与总结">实验与总结</h3>
<p>略过一些细节, 本文 OSG 的优点在于, 详细的语义信息如区域划分、连接体等为推理和探索提供了充足的线索<br />
此外 LLM 为 OSGN 提供了强大的开放词汇能力, 对数据集上没有的物体类别也有足够的辨别能力 OSGN 目前在成熟的 LLM, VQA 等模型上取得了很好的成绩, 但主要问题在于,它使用的是 GPT3.5 这样的成熟大模型,如果想在不联网的机器人上部署接近性能的语言模型想想就不可能<br />
此外 LLM 也存在局限,例如对语料库稀少的场景如医院等准确性就会下降, 另一个缺点是由于这些都是基础模型, 除了人为指导或者优化基础模型性能,几乎没什么改进的办法</p>
<h2 id="stage-1-summary">stage 1 summary</h2>
<p>总结一下目前读过的四种语义地图:</p>
<ol type="1">
<li>概念图(ICRA 2024) : 使用类别无关的分割模型切割对象, 用点云表示空间占用, 对 merge 操作使用 clip 计算一个中间值特征, 但最后存储的是 LLM 生成的描述信息, 由于是早期工作比较粗糙</li>
<li>分层图(RSS 2024): 核心想法是构建一个 <code>floor-room-object</code> 三层的场景树, 这其中有很多 trick, 这里略过大部分细节, 实际的空间存储是优化(合并)过的点云, 语义存储是每个点云有一个特征向量以及基于封闭词汇的标签, 以占用来说是可控的, 但由于是逐点计算, 我猜想计算量应该偏大, 此外这篇文章虽然确实是开放词汇表, 但其实建图和更新过程中使用了很多封闭词汇表的分类</li>
<li>DualMap(预印): 以上两个模型都不涉及对地图的动态更新, dualmap 与其区别在于使用两种地图用于节省空间和计算量, 并动态更新地图, 更新过程避免昂贵的 3d 物体合并, 这种双图设计在节省开销方面不错,但其更新方式比较粗糙,只是单纯地取平均</li>
<li>开放场景图(IJRR 2025): 这是一个神奇的模型, 其输入是 rgb 没有 d, 也没有里程计, 其机器人的导航实现是一个个 <code>move_to_object</code> 中间数据都是显性的自然语言信息, 几乎全部的工作是云端 LLM 做的, 虽然有更新算法,但更新上也是通过 LLM 生成 json 格式的数据来更新拓扑图结构, 这其实可以视作马尔科夫链, 但是使用 LLM 来更新</li>
<li>总结: 受限与现实场景规模和成本, 当前语义地图的语义存储主要是对每个对象存储文字描述或者单个特征向量, 更新上其实很难处理, 因为很难找到合适的对先后观察的权重分配, 这点可能还需要继续搜集一些模型</li>
</ol>
<p>语义级别:</p>
<table>
<thead>
<tr class="header">
<th>ConceptMap</th>
<th>HOVSG(分层图)<br></th>
<th>DualMap</th>
<th>OSGN(开放场景图)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>物体级别的单图</td>
<td>三层的树状空间关系图</td>
<td>物体级别的双图</td>
<td>层级关系的有向图</td>
</tr>
</tbody>
</table>
<p>空间(占用)图表示:</p>
<table>
<thead>
<tr class="header">
<th>ConceptMap</th>
<th>HOVSG(分层图)<br></th>
<th>DualMap</th>
<th>OSGN(开放场景图)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>点云(降采样处理)</td>
<td>点云(有一定压缩)</td>
<td>点云(但导航时只用到锚点对象的点云)</td>
<td>无(只存储抽象对象,无距离表示)</td>
</tr>
</tbody>
</table>
<p>语义形式:</p>
<table>
<thead>
<tr class="header">
<th>ConceptMap</th>
<th>HOVSG(分层图)<br></th>
<th>DualMap</th>
<th>OSGN(开放场景图)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>描述和标签信息</td>
<td>特征向量</td>
<td>特征向量</td>
<td>描述和标签信息</td>
</tr>
</tbody>
</table>
<p>使用的基础模型:</p>
<table>
<thead>
<tr class="header">
<th>ConceptMap</th>
<th>HOVSG(分层图)<br></th>
<th>DualMap</th>
<th>OSGN(开放场景图)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>SAM, Clip, LLM</td>
<td>SAM, Clip, LLM</td>
<td>SAM, YOLO, Clip</td>
<td>GNM,, LLM</td>
</tr>
</tbody>
</table>
<p>导航(子目标选取)算法:</p>
<table>
<thead>
<tr class="header">
<th>ConceptMap</th>
<th>HOVSG(分层图)<br></th>
<th>DualMap</th>
<th>OSGN(开放场景图)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>LLM 生成</td>
<td>LLM 分解任务再用余弦相似度选取目标</td>
<td>余弦相似度选取候选</td>
<td>LLM 生成</td>
</tr>
</tbody>
</table>
<h2 id="map-semnav-advancing-zero-shot-continuous-vision-and-language-navigation-through-visual-semantics-and-map-integration">Map-SemNav: Advancing Zero-Shot Continuous Vision-and-Language Navigation through Visual Semantics and Map Integration</h2>
<p>来自 ICRA 2025</p>
<p>这是一个 VLN 模型, 用于不使用大模型的 zero-shot 任务<br />
<img src="/assets/ml/image-17.png" /></p>
<p>对零训练任务来说,最大的问题就是泛化问题,很多使用大模型的文章就是通过大模型解决该问题,而本模型使用 rl 的思路, 将问题视为在可见和隐藏变量的环境中寻找一个最优策略 π</p>
<h3 id="建图">建图</h3>
<p>模型的输入为 RGBD 观察帧, 将其用于生成一个自上而下的自我中心(网格 <span class="math inline">\({\phi}_t\)</span> )地图, 使用 ResNet18 从网格提取特征, 得到一个特征表示 $ R_t^{} $ 对输入的查询指令使用 BERT 进行编码得到特征 <span class="math inline">\(\Lambda\)</span><br />
然后使用如下的注意力机制:</p>
<p><span class="math display">\[Q=R_{t}^{\phi}W_{q},K=\Lambda W_{k},V=\Lambda W_{v}\]</span></p>
<p><span class="math display">\[\Psi_{t}^{\phi}=S o f t m a x\left(\frac{Q K^{T}}{\sqrt{d}}\right)V\]</span></p>
<p>综合得分表示为指令地图特征的综合相似度(标准化)和注意力得分的加权和</p>
<p><span class="math display">\[\displaystyle{\mathcal G}_{t}=\frac{(W_{R}R_{t}^{\phi})(W_{\Lambda}\Lambda)^{T}}{\|(W_{R}R_{t}^{\phi})\|\cdot\|(W_{\Lambda}\Lambda)\|^{T}}\]</span></p>
<p><span class="math display">\[{\tilde {\mathcal{G}}}_{t}={\frac{ {\mathcal{G}}_{t}+1}{2}}\]</span></p>
<p><span class="math display">\[\Gamma_{t}^{\phi}=\left(\lambda_{1}S o f t m a x\left(\frac{Q K^{T}}{\sqrt{d}}\right) +\lambda_{2}\tilde{\mathcal{G}}_{t}\right)V=\lambda_{1}\Psi_{t}^{\phi}+\lambda_{2}\tilde{\mathcal{G}}_{t}V\]</span></p>
<p>为了推导代理看不到的部分环境,使用双重残差网络,即:</p>
<p><span class="math display">\[\hat{\phi}_{t}=R U_{1}(\phi_{t},\Gamma_{t}^{\phi}),~~~\hat{\Upsilon}_{t}=R U_{2}(\hat{\phi}_{t},\Gamma_{t}^{\phi},\hat{\Theta}_{t})\]</span></p>
<p>其中 θ 为从 rgb 帧中进行的语义分割片段(带类别信息)后的地面投影, γ 是用于推理的语义地图</p>
<h3 id="场景对象解耦decoupling">场景/对象解耦(decoupling)</h3>
<p>这一步主要目的为去除噪声或者不重要的信息, 提取关键信息, 也是本文的主要创新点, 其最大的优点在于,让对物体的语义表示摆脱对类别等已知信息的依赖, 从而增加泛化能力</p>
<ol type="1">
<li>场景解耦: 提取主要的布局信息, 场景在模型中表示为一个物体空间的信息集合,但单个物体的信息表示为质心的 xy 坐标加上其面积占视图的比率, 对大量被分为同类的物体, 在导航时面积更大的会有更高的优先级</li>
<li>物体解耦: 使用通用性的表示方法来表示物体, 具体对每个物体来说,存储以下数据:
<ol type="1">
<li>V: 可见性, bool 值</li>
<li>K:是否为里程碑, 即指令中包含的关键对象, bool 值</li>
<li>S: 对象的语义嵌入和里程碑的语义嵌入之间的加权平均余弦相似度, 参考之前的 <span class="math inline">\({\tilde {\mathcal{G}}}_{t}\)</span> ; 已经抵达的里程碑权值为 0, 未抵达的里程碑权值递增</li>
</ol></li>
</ol>
<p>对之前提及的解耦信息,它们的存储形式为附加在网格上的矩阵,对其进行 concat 形成一个观察解耦矩阵, 并输入一个 VAE, 这里不使用 VAE 的解码功能, 只使用 VAE 的潜空间来捕捉空间结构关系</p>
<h3 id="推理与控制">推理与控制</h3>
<p>在推理和导航过程中, 类似之前的过程计算自我中心地图的注意力表示, 然后将其输入一个 Res-Unet 来得到可以导航的路径点 w(ay)p(oints)<br />
这些 wp 与此前的解耦矩阵是推理模块的输入, 这个过程中会使用 DD-PPO 来产生动作指令(底层上只有移动和转向)</p>
<h2 id="scene-driven-multimodal-knowledge-graph-construction-for-embodied-ai">Scene-Driven Multimodal Knowledge Graph Construction for Embodied AI</h2>
<p>TKDE 2024</p>
<p><img src="/assets/ml/unnamed.jpg" /></p>
<p>与之前的模型不同的是,这是一篇数据库刊物上的文章,也就是说,会使用知识增强方法<br />
从数据的角度对具身智能能学习到的知识分类, 可分为:</p>
<ol type="1">
<li>perceptual knowledge: 感官知识,也就是能由传感器检测到的知识, 由于依赖传感器输入以及物理设备的算力限制, 传感器数据很难用数据库之类的较为复杂的形式存储,一般会用 yolo, sam 之类的视觉模型进行处理转为可以被存储的更小的数据</li>
<li>apperceptive knowledge: 感知知识, 也就是关系性,描述性的信息, 这些数据通用性强, 可以用知识库来存储,供机器调用</li>
</ol>
<p>将现存知识库里得到的知识称为符号(symbotic)知识, 由可学习的模型训练得到的知识称为参数知识(如 gpt), 前者优点在于可靠性强,但成本高, 后者则相反<br />
本文提出了一种综合性的知识图谱构建方法, 这个知识图谱称为 <code>Scene-driven Multimodal Knowledge Graph (SceneMMKG)</code><br />
常识中的知识图谱一般是服务于 nlp 之类的任务,但就具身智能领域而言, 知识图谱只需要存储场景及其物体与机器人交互有关的信息就可以</p>
<h3 id="kg-构建过程">KG 构建过程</h3>
<ol type="1">
<li>prompt-based schema: 也就是解决图谱结构的问题, 一般来说可以根据不同领域手动设定, 但在机器人场景下比较难办, 这里使用万能的 LLM 来处理
<ol type="1">
<li>室内场景可以由有限个种类定义, 设场景档案集 Ws 为一系列场景(例如房间种类)的文本描述的集合, 如以下的流程图,将档案集和一个场景输入 LLM(预训练且不会更新)得到名为 concept 的抽象信息,形式是文本信息的集合 c</li>
<li>Concept Expansion: 基于现有的通用 KG, 将之前得到的 c 拓展为原有词汇与其上下位词的并集(例如 color 和 red 互为上下位词)</li>
<li>Concept Cluster: 简单地说就是去重, 比较每对概念词,若相似度大于一个门槛值则合并</li>
</ol></li>
<li>Knowledge Population: 也就是知识扩充, 将通用知识库 <span class="math inline">\(K_g\)</span> 用于构建 <span class="math inline">\({K}_{\mathrm{scene-KG}}\)</span> (根据现有的通用知识库产生的中间知识库)融入现用的知识图谱(不冲突的前提下), 下面介绍知识来源
<ol type="1">
<li>通用知识来自于已有的知识库, 开始时,从这些知识库检索, 基于已有的架构, 将检索到的数据逐步挂载, 得到 <span class="math inline">\({K}_{\mathrm{scene-KG}}\)</span></li>
<li>场景中心的知识 <span class="math inline">\(K_s\)</span>, 这种知识较难获取, 文本领域还有一些数据集, 图像数据则相对稀少</li>
</ol></li>
<li>Quality Control and Refinement: 由于一些属性有复合含义或者不同属性有类似含义, 实际上会出现少部分属性较为集中的情况, 也就是长尾现象, 为此需要进行一些处理
<ol type="1">
<li>Hierarchicalization: 如以下的算法所示, 将一个实体的属性集 A 拆开, 即对其属性 a 拆分成部分集合 P 以及通用属性 AP, P 连接到所属实体, AP 的元素 ap 则各自连接到 P 中的元素 p;这里摘一段原文 <code>a “chair” possesses attributes “frame length” and “foot length”. To manage these attributes effectively, we subdivide the “chair” into its constituent parts, “foot” and “frame”, considering “length” as a general attribute of each part. Attributes that cannot be further subdivided, such as “usage” and “conservation measures”, are directly associated with the “chair”</code></li>
<li>Aggregation: 使用预训练的语言模型对属性对去重,达到门槛值则合并, 例如 " measurement” , “size”会被这样合并</li>
</ol></li>
</ol>
<p><img src="/assets/ml/image-20.png" /> <img src="/assets/ml/image-18.png" /></p>
<h3 id="知识增强">知识增强</h3>
<p>对已有的场景知识,出于机器人任务需要(即导航或者操纵), 需要更详细的知识,即 <code>Scene-driven Knowledge Enhancement</code>, 步骤为:</p>
<ol type="1">
<li>知识检索: 这是通过比较查询 mmkg 中实体的相似度选择的, 不断取最佳候选对象,然后让 SKR(Scene Knowledge Retrieval)模块返回文本与视觉知识 <span class="math inline">\(H\)</span></li>
<li>多模态降噪: 对检索到的知识, 使用降噪模块优化, 具体则是对每个时间步的观察帧 <span class="math inline">\(O_t\)</span> 计算与检索到 H 的相似度,丢弃掉相似度大于一个门槛值的知识</li>
<li>知识编码: 详细见以下的流程图,简单地说就是用 clip 对文本和图像编码,场景知识则用 n 层 GCN 编码</li>
</ol>
<p><img src="/assets/ml/image-19.png" /></p>
<h4 id="知识注入">知识注入</h4>
<p>即将上一节中得到的知识用于具体任务中</p>
<ol type="1">
<li>VLN: 最典型的机器人任务,对各个时间戳的辨别到的物体使用 R-CNN 编码得到 Fo, 对指令使用 tf encoder 编码为 Fi, 每个时间步的场景知识被编码为 Fh, 也就是我们上一节做的事, 以上这些特征被 concat 为 m, m 作为 token 被输入 tf decoder 来产生隐藏状态 h_t(t 为时间步), 随后可以视为一个(动作)序列生成问题,使用交叉熵训练</li>
<li>3D Object Language Grounding: 指代理根据语言描述辨别现实物体的能力, 是操纵等复杂任务的前置步骤, 其思路和 VLN 也差不多, 将场景、物体、指令特征拼接后用 MLP 计算相似度,并用交叉熵作为损失函数</li>
</ol>
<h2 id="multimodal-data-storage-and-retrieval-for-embodied-ai-a-survey">Multimodal Data Storage and Retrieval for Embodied AI: A Survey</h2>
<p>这是一篇关于具身智能领域关于数据存储检索方式的综述, 前面比较水就跳过了, 主要讲的是各种领域应用的 em-ai 对数据系统的需求</p>
<h3 id="数据存储">数据存储</h3>
<p>总的来说, 具身智能领域需要的数据是多模态且能实时快速检索到的, 数据存储有五种常用范式:</p>
<ol type="1">
<li>图数据库 Graph Databases:直接通过节点和边来建模复杂的关系, 其最大的优势是可以与全局数据大小无关的复杂度查询周边节点, 也就是上下文信息, 但是同样的, 对可能变化的环境, 更新图状的知识库以及确保一致性也会非常艰难, 同样的, 在图中嵌入多模态知识也很难实现</li>
<li>多模型数据库 Multi-model Databases:在单一平台上原生支持关系型、文档型、图等多种数据模型, 对此有两种思路, 1 是依旧用不同架构的数据库, 再加一层抽象来统一调用, 2 是搞一个大一统数据库, 可以存各种数据进去; 这样听起来很好,但实际表现上开销大(包括数据转化, 检索等)是不可避免的问题,对要求实时性的 eai 来说不实用</li>
<li>数据湖 Data Lakes:采用只读架构保留原始数据, 也就是说基本上只是保留数据并提供高效检索, 而不对数据进行处理, 难以维护上下文之类的语义信息, 对于实时性活动性的 agent, 则用处有限</li>
<li>向量数据库 Vector Databases:针对高维向量相似性搜索进行优化索引; 由于目前的 eai 经常用 clip 之类的模型产生嵌入向量作为语义表示, 提供高效的近似向量检索是一种非常使用的功能,对于算力受限的情况, 可以使用带有近似最近邻检索的数据系统</li>
<li>时序数据库 Time-Series Databases:针对大量传感器数据流提供高效的写入和查询能力: 对于 eai 常见的传感器数据来说实用性较好, 问题在于,对多模态数据的支持受限</li>
</ol>
<h3 id="数据检索">数据检索</h3>
<p>接下来讨论对(多模态数据的)检索, 也可以分为 5 种范式:</p>
<ol type="1">
<li>Fusion Strategy-Based Retrieval: 将多模态特征整合到一起, 与2. 的区别是会保留对不同模态的描述信息、权值信息等, 也就是对多模态数据有一个共享的语义空间，以及其各自的私有信息, 这样的好处上信息完整性高, 缺点则是难以保持一致性, 实时性, 成本较大</li>
<li>Representation Alignment-Based Retrieval: 将多模态数据嵌入到一个统一的语义空间, 随后根据相似度检索; 这个方法的挑战是多模态之间的对齐, 但是有clip这样非常强大的基础模型, 应用上相对成熟</li>
<li>Graph-Structure-Based Retrieval: 使用图学习来捕获数据实体以及其相互关系,其优缺点和之前的图存储类似,具体可分为:
<ol type="1">
<li>图嵌入: 将给定图所有节点嵌入语义空间中</li>
<li>图索引: 对有语义信息的图, 实现索引优化</li>
<li>图匹配: 量化查询语句和目标子图的语义一致性, 从而检索子图中的对象</li>
</ol></li>
<li>Generation Model-Based Retrieval: 利用生成模型将搜索任务转化为文本生成问题, 即利用已有的跨模态数据或者其中间表示, 从而直接生成检索需要的信息，同时可以用rag等方法调用外部知识来减少幻觉; 其缺点是, 幻觉或者大型模型预训练知识与eai训练中的知识不同的倾向性可能影响准确性</li>
<li>Efficient Retrieval-Based Optimization: 使用近似最近邻算法这种高效算法或者硬件加速技术来实现高效检索</li>
</ol>
<h3 id="eai-数据管理的挑战">eai 数据管理的挑战</h3>
<p>eai的数据需求需要多模态、高速检索、尽可能准确、计算消耗低、高保真、动态环境中的稳定性以及更新成本可控<br />
满足其中的一两项都很难, 更不用说综合考虑了, 本文对其的总结见下图</p>
<p><img src="/assets/ml/image-21.png" /></p>
<h2 id="embodied-rag-general-non-parametric-embodied-memory-for-retrieval-and-generation">Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation</h2>
<p>arxiv(cmu)</p>
<h3 id="记忆构建">记忆构建</h3>
<p>本模型中的记忆构建是自底向上的, 由两个独立部分组成: 拓扑图和语义森林, 语义森林是分层结构, 会使用部分拓扑图的信息<br />
拓扑图中的每个节点包含信息:</p>
<ol type="1">
<li>pose信息, 即拍照处的姿态信息, 根据代理的历史路径或者距离关联</li>
<li>时间戳</li>
<li>(自我中心的)图像</li>
<li>图像描述(由LLM生成)</li>
</ol>
<p>语义森林的概念来自于对具身智能数据的启发式假设: 具身数据在空间和语义上有组织关系, 构建过程:</p>
<ol type="1">
<li>使用 全连接聚类(complete-linkage hierarchical clustering) 对上一步得到的(叶)节点聚类, 但是将两个节点的距离定义如下:</li>
</ol>
<p><span class="math display">\[S_{i j}=(1-\alpha)S_{i j}^{\mathrm{spatal}}+\alpha S_{i j}^{\mathrm{semanic}}\]</span> <span class="math display">\[S_{i j}^{\mathrm{\scriptsize{spatial}}}=\exp\left(-\frac{d_{\mathrm{haversine}}(i,j)}{\theta}\right)\]</span> <span class="math display">\[S_{i j}^{\mathrm{semantic}}={\frac{\bf e_{i}\cdot\bf e_j}{||\mathrm{e}_{i}||.||\mathrm{e}_{j}||}}\]</span></p>
<p>其中e表示节点描述的嵌入向量</p>
<ol start="2" type="1">
<li>聚类完成后, 对每层的各个聚类用LLM产生一个摘要, 摘要会作为一个新节点插入(在聚类的正中), 这样的过程持续到产生根节点或者没有新聚类产生 这其实和3d场景图有点像, 但本文出于抽象户外空间的目的考虑, 只用抽象化的机器自生成层级，而不是人为规定</li>
</ol>
<h3 id="检索">检索</h3>
<p>与构建相反, 检索是自顶向下的, 分为两个阶段</p>
<ol type="1">
<li>语义引导的层级遍历: 并行地遍历语义森林, 即 给定查询 q，在第 l 层候选节点集 Cl 上，用基于大模型的选择函数 fLLM 选择k个要查找的分支(节点) ：Nl = fLLM(q, Cl, k), 不断向下层递归, 直到叶节点, 过程中基于语义相关性进行剪枝,最后得到的叶节点形成一个集合</li>
<li>混合重排序（Hybrid Re-ranking）
<ol type="1">
<li>选出一些候选叶节点后, 用另一个LLM评分并降序排行并去重</li>
<li>如果有位置信息, 将其也作为前置条件，以类似构建过程中的空间分数与1.的语义分数加权和作为排序条件</li>
</ol></li>
</ol>
<p>检索到的节点作为上下文一部分和用户查询一起传递给LLM, 然后类似一些VLN模型, 让LLM根据上下文信息寻找候选航路点</p>
<p><img src="/assets/ml/image-22.png" /></p>
<h1 id="vision-language-action-models">Vision-Language-Action Models</h1>
<h2 id="large-vlm-based-vision-language-action-models-for-robotic-manipulation-a-survey">Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey</h2>
<h3 id="前言-1">前言</h3>
<p>机器操控, 作为机器人学和具身智能的关键领域, 需要精确的动力控制以及内置的视觉和语义信息理解能力的智能, 其传统做法一般局限于使用预定义的动作以及严格的控制策略, 泛用性较为有限<br />
近年来随着算力和研究方法的进步, 基于大量图像文本数据上预训练的大型视听语言模型(VLM)以及基于 VLM 增加了动作能力的 VLA, 可以称为一种新的技术趋向<br />
所谓的 VLA 其实是一件很有野心的事, 它不但有对图像、文本这两种信息量最大的载体的理解能力, 还要有做出合适行动的机械系统, 这几乎涉及了 cv, nlp, em-ai 等 cs 大部分的前沿领域, 理论也相当庞杂<br />
VLA 具体可以定义为:</p>
<ol type="1">
<li>使用 VLM 理解视觉观察和自然语言指令</li>
<li>进行推理并直接或间接地帮助动作指令的生成</li>
</ol>
<p>又可以分为:</p>
<ol type="1">
<li>Monolithic Models: 单片模型, 也就是集成度高的模型
<ol type="1">
<li>Single-system models: 将环境理解, 视觉感知, 语言理解, 机器状态认知, 动作生成都统一到一个模型</li>
<li>dual-system models: 将认知环境和动作生成分为两个任务, 分别让 VLM 和一个 action expert 去做, 这是因为相比前者, 后者时效性更强</li>
</ol></li>
<li>Hierarchical Models: 分层模型, 耦合度最低的模型, 一般有结构, 中间输出可解释的中间产物, 例如计划模块产生 <code>keypoint detections</code>, <code>affordance maps</code>, <code>trajectory proposals</code> , 交给策略模块来产生动作; 这样的解耦让不同模块可以并行训练, 使用不同的 loss 和优化方式</li>
</ol>
<div class="note info"><ul>
<li><code>Keypoint Detection</code> : 计算机视觉中的一种技术, 旨在识别图像中具有显著特征的点</li>
<li><code>Affordance Maps</code> : 一种表示对象与环境之间交互可能性的图像或图形</li>
<li><code>Backbone</code> : 一个神经网络结构的基础部分, 主要用于特征提取。Backbone 是整个模型的核心组件, 负责从输入数据（如图像）中提取重要的特征信息</li>
<li><code>Internet-scale</code> : 表示很大的数据或者整体性能消耗的规格</li>
</ul>
</div>
<h3 id="背景知识">背景知识</h3>
<p>得益于用于跨领域和多模态能力的 VLM 的出现, 现代的 VLM 模型能执行对能力需求更加多样的任务例如自动驾驶, 图形用户界面交互等<br />
这些模型往往由一个视觉编码器, 一个用于将视觉特征嵌入到文本的投影器(projector), 以及一个大语言模型组成; 这其中, 最为重要的是 <strong>大</strong> 模型为其带来的泛化能力, 如 GPT4 这种 LLM 能提供极强的过滤输入, 降噪, 处理模糊指令的能力, 从而为 VLA 定下基础, 也就是说, VLA 相当重视泛化、通用性质的能力</p>
<p>V、L、A 可以视为三种不同的任务, 早期的模型往往会将三者分开, 这样的好处是可以并行训练, 但缺点是可能降低一些理解和推理能力; 最近的模型一般会加强集成度, 例如除了传统的大规模(<code>Internet-scale</code>)训练资料外, 把机器人的动作也转化为文本标记并放入训练语料库中, 这能让 action 作为语言任务被语言模块理解, 增强整个模型的理解能力, 其他一些使用动作资料的模型也取得了较好的效果</p>
<p>接下来从单片/多片的角度讨论两种模式的发展趋势</p>
<h3 id="monolithic-models-单片模型">MONOLITHIC MODELS 单片模型</h3>
<h4 id="single-system-models">Single-system Models</h4>
<p>其经典范式为 <code>Autoregressive Decoding</code>, 将机器人的连续动作空间离散化为 token 序列, 模型能够序列化地生产动作 token, 再由一个 de-tokenizer 转化为具体动作。较为知名的模型有 <code>RT series</code> , <code>OpenVLA</code></p>
<h5 id="model-performance-enhancement">Model Performance Enhancement</h5>
<ol type="1">
<li>认知模态(Perception Modalities):
<ol type="1">
<li>3d 感知(perception): 一般能从传感器得到的数据就是 2d 图片, 为了构建 3d 的地图模型, 可以模拟环境并根据图片画出点云; 或者根据图片及其深度信息, 画出自我中心的地图</li>
<li>4d 感知: 多出来的 d 指的是轨迹, 也就是时空序列信息, 例如将动作或者轨迹信息添加到图像中, 从而增强模型的时空推理能力</li>
<li>触觉和听觉（Tactile and auditory）感知: 触觉信息可以使用视觉编码器, 听觉信息可以使用专用的编码器, 且两者都需要专用的数据集, 这两种能力均可以与语言模块对齐, 然后得到接受听觉触觉指令的能力</li>
</ol></li>
<li>推理能力: 以下简单介绍一些相关工作
<ol type="1">
<li><code>ECoT</code>: 生成一个推理链, 将高层任务规划与视觉特征相结合, 输出最终行动</li>
<li><code>CoT-VLA</code>: 引入视觉链式推理, 通过预测代表计划状态的子目标观察, 增强推理能力</li>
<li><code>LoHoVLA</code>: 采用“层次闭环控制 <code>Hierarchical Closed-Loop Control</code> ”机制, 解决规划错误、行动失败和外部干扰等问题, 适用于长周期任务</li>
<li><code>ReFineVLA</code>: 通过“选择性迁移微调 <code>Selective Transfer Fine-Tuning</code> ”策略, 仅微调上层, 增强多模态理解</li>
</ol></li>
<li>泛化能力: 也就是处理各种场景需求的能力
<ol type="1">
<li><code>UniAct</code>: 定义一个 <code>Universal Action Codebook</code>, 将所有机器人行动抽象到有限的类别中, 增强泛化能力</li>
<li><code>ReVLA</code>: 使用可逆的训练策略, 缓解遗忘问题</li>
<li><code>HybridVLA</code>: 使用一个 <code>Collaborative Action Ensemble</code> 机制, 融合 <code>diffusion and autoregressive decoding</code>, 针对任务选取不同策略</li>
<li><code>VOTE</code>: 使用一种 <code>Ensemble Voting</code> 机制, 将过去的动作预测根据与当前预测的相似性进行分组, 并对大多数集进行平均处理, 从而生成更稳健的动作输出</li>
<li>也有一些模型在物理规律层面进行学习</li>
</ol></li>
</ol>
<h5 id="inference-efficiency-optimization">Inference Efficiency Optimization</h5>
<p><code>Inference Efficiency Optimization</code> 推理效率优化 : 机器人对及时性(控制频率)往往有一定要求, 因此模型的推理效率优化也是一个重要的方向</p>
<ol type="1">
<li><code>Architectural Optimization 体系优化</code>: 一些常用的方法用, 跳过层级, 早停止等, 进阶的有一些方法会动态计算哪些层级是关键的, 从而激活关键层完成推理; 此外也有方法会逐步减少视觉 token, 也就是修建掉那些与指令生成无关的 token, 再加上注意力机制, 这甚至能同时提高指令准确性</li>
<li><code>Parameter Optimization 参数优化</code>: 也就是压缩模型的大小, 通过蒸馏的模型提高效率; 或者使用 tokenizer 将高维动作指令转化为 token 的序列从而减少模型复杂度</li>
<li><code>Inference Acceleration 推理加速</code>: 即加速动作解码, 可以使用专门优化过的 action head, 一次生成一个动作序列; 通过某种方式实现并行运算等; 也可以单纯地(但有选择地)放开限制, 接受一些“不完善”的推理结果</li>
</ol>
<div class="note info"><ul>
<li><code>Distillation-Aware Training</code>: 一种优化训练方法, 旨在通过知识蒸馏（knowledge distillation）技术提高模型的性能和效率。这一过程通常涉及将一个大型、复杂的“教师”模型的知识转移到一个较小的“学生”模型中, 以便在保持性能的同时减少计算资源和加速推理</li>
<li><code>action head</code>: 用于将模型的输出转换为具体的动作或决策的组件</li>
</ul>
</div>
<h4 id="dual-system-models">Dual-system Models</h4>
<p>双系统主要是为了解决动作生成速度与推理速度要求不同的问题, 引入了一个动作专家模块来对系统解耦, 而其与分层模型不同的是, 不会产生可解释的中间输出<br />
<img src="/assets/ml/image.png" /></p>
<h5 id="cascade-based-methods">Cascade-based Methods</h5>
<p>串联的方法中, 负责推理部分(称为高级 <code>high-level</code> )的系统一般使用 VLM 处理多模态的输入, 提取语义并产生动作 <strong>计划</strong> 输出, 这些会被编码为某种中间形态, 然后由低级(<code>low-level</code>)系统解码为具体的动作指令<br />
由于在双系统中, 认知和行为是解耦的, 行动可以使用一个不同的模型, 例如 <code>diffusion transformer</code>, <code>Behavioral Cloning transformer</code><br />
此外, 还有很多种改进方法:</p>
<ul>
<li>再加入一个视频生成模块来预测未来的帧, 并使用 <code>diffusion action expert</code></li>
<li>从人类演示中提取场景喂给 LLM 来生成可解释的行为树, 并转化为底层行为</li>
<li>将 VLM 与扩散策略模型整合, VLM 会发射一个 TOKEN 控制是否允许动作生成</li>
<li>使用预训练的卷积残差网络增加性能</li>
<li>将 action expert 集成到 VLA 主干中, 两者可以复用一些组件, 协调工作</li>
</ul>
<div class="note info"><ul>
<li><code>Cartesian Actions</code>: 机器人学和控制系统中使用的一个概念, 指的是以笛卡尔坐标系为基础的运动或操作。这种动作通常涉及到机器人的末端执行器（如机械手臂的抓手）在三维空间中移动或执行任务</li>
<li><code>convolutional residual</code>: 卷积神经网络（CNN）中使用的架构设计, 最著名的实现是 ResNet（Residual Network）。这一设计理念旨在解决深层网络训练中的退化问题, 即随着网络层数增加, 模型的性能反而下降; 引入残差连接（<code>residual connections</code>）, 模型可以学习到输入与输出之间的“残差”而不是直接学习目标输出</li>
</ul>
</div>
<h5 id="parallel-based-methods">Parallel-based Methods</h5>
<p>上图中展示了一种共享注意力的架构, 它受到 <code>mix-of-experts (MoE)</code> 启发, 在注意力层中, VLM 与动作模块进行 token 上的交互, 过程中, 输入的视觉文本信息, 噪声, 机器人特定的输入在共享的注意力层中交互, 从而提高了任务效果<br />
由于这种架构的低耦合性, 它可以实现字面意义上的 MoE, 也就是对每个子任务都让一个“专家”去做, 其中一个典例称为 <code>π0</code>, 其主干网络是 VLM, 为了处理机器人特定的输入输出, 它使用一个独立的权重集合和一个流匹配(<code>Flow Matching(FM)</code>)的动作模块<br />
π0 有一些变种, 例如增加显式推理, 基于最近推理生成动作两种模式, 可以根据需求切换; 降低信息力度; 防止动作模块的梯度流入 VLM 以保持 VLM 的知识优势; 增加动作 token 化方法从而自回归训练等; 其他的变种也一般是增加某个维度的输入或者通过复用注意力信息提高性能等常规优化</p>
<div class="note info"><ul>
<li><code>Normalizing Flows(NFs)</code>: 通过一系列概率密度函数的变量变换, 将复杂的概率分布转换为简单的概率分布, 并通过逆变换生成新的数据样本</li>
<li><code>Continuous Normalizing Flows(CNFs)</code> 是 <code>Normalizing Flows</code> 的扩展, 它使用常微分方程(ODE)来表示连续的变换过程, 用于建模概率分布</li>
<li><code>Flow Matching(FM)</code> 是一种训练 <code>Continuous Normalizing Flows</code> 的方法, 它通过学习与概率路径相关的向量场(<code>Vector Field</code>)来训练模型, 并使用 ODE 求解器来生成新样本</li>
</ul>
</div>
<h3 id="hierarchical-models-分层模型">HIERARCHICAL MODELS 分层模型</h3>
<p>对于很大规模的任务, 由于算力, 存储空间等方面的要求都较高, 更低耦合的架构如分层模型较为常用, 例如长期推理, 空间抽象, 动作分解等<br />
这些模型一般由高等级的 planner, 低等级的 policy 等组成, 计划期接受指令和观察, 转化为中间产物, policy 接受中间产物, 产生动作序列或底层指令<br />
相比单片模型, 它能实现更复杂的组合, 甚至可以只有计划器; 相比其单片双系统, 它的中间产物是显式（可解释）的</p>
<h4 id="planner-only">Planner-Only</h4>
<p>可分为:</p>
<ul>
<li><code>Program-based Methods</code>: 计划器生成中间程序, 即直接可执行的程序或者辅助程序, 辅助程序一般用于辅助对任务策略的制定</li>
<li><code>Keypoint-based Methods</code>: 标记出一些关键点, 例如机器臂的可抓取点, 导航任务的路线节点等, 这些提取出来的点可以视为对问题的一种抽象, 从而输入给通用模型如处理, 也可以用作轨迹生成</li>
<li><code>Subtask-based Methods</code>: 一般用于较大的 VLM, 接受一个抽象的隐式指令, 并输出详细的文字命令, 实际部署中可能还会需要一个控制策略。由于该种方法抽象程度较高, 可以使用一些通用模型进行任务分解或者规划</li>
</ul>
<h4 id="plannerpolicy">Planner+Policy</h4>
<p>其分类上和前一类较为类似</p>
<p>可分为:</p>
<ul>
<li><code>Keypoint-based Methods</code>: 一般用 LVLM 产生一些关键点, 或者关键路径, 但这些信息将会交给低级策略模块, 用于生成指令; 例如在导航任务中, 对 生成的关键点进行排序, 并将这个有序路径用于之后的指引; 此外也可以用 LLM 为关键点赋予 cost, 用于确定最佳路径; 也有方法会将关键点赋予互动性上的标注, 用于生成控制指令。总的来说, 一般关键点会被赋予帮助决策的一些信息, 用于最后产生指令</li>
<li><code>Subtask-based Methods</code>: 这种方法主要的工作在于如何划分子任务, 而对具体的子任务执行者则有很高的灵活性。例如对模糊的文字指令, 将其转化为一系列原子命令; 也有模型分出空间感知任务, 使用点云增强空间感知; 或者让高级模型划分任务, 低级扩散策略模型具体实现, 且这样的划分可以继续拓展, 例如再加一个中间的 skill 层辨识一些可重复使用的行动控制底层的执行者; 划分后的子任务也可以一步流水线执行从而增加效率; 这方面有非常多的类似变种</li>
</ul>
<div class="note info"><ul>
<li><code>Embodiment-Agnostic Affordance Representation</code>: Affordance 前文介绍过, 是对可交互性的标识, 而 Embodiment-Agnostic 指的是不依赖于机器人形态或者物理特性, 可以理解为通用的可互动性</li>
</ul>
</div>
<p><strong>Monolithic vs Hierarchical:</strong></p>
<ol type="1">
<li>两者的输入输出是一致的, 主要区别为统一化/模块化的内部结构, 其优缺点和这两种常见的结构是类似的</li>
<li>另一个关键区别是中间产物的可解释性, 整体模型的中建五不可解释, 但更方便机器学习一些人类可能无法理解的特征; 分层模型输出可解释的中间数据, 这对需要兼容性, 可解释性的操控领域可能更好</li>
</ol>
<h3 id="其他前沿领域">其他前沿领域</h3>
<ul>
<li>RL: 与 LLM, VLM 不同的是, VLA 的问题更加长周期, 其最大的问题在于难于产生基于规则的奖励函数, 可能人类也不知道怎么做是更好的, 这方面没有特别好的解决方法, 一般将学习过程作为密集奖励或者以视觉上的近似成功率作为奖励;
<ul>
<li>此外, 由于模拟环境现状, 在线学习效率有限, 一些方法会把在线, 离线方法混合起来, 例如离线用 q 学习, 在线用 <code>Soft Actor-Critic</code> 提高效率又或者引入人类监督</li>
<li>部分方法会只把 RL 用作数据引擎来增强泛化能力, 例如先用 HIL-SERL 训练, 然后通过 sft 将训练成果提取融入给主要模型</li>
</ul></li>
<li>Training-Free Methods: 通常利用模块化和可扩展的设计来改进现有的 VLA 架构, 而无需训练, 例如改变触发机制, 行为和视觉都稳定时不需要完全解码, 可以选择性重用之前的 visual tkoens; 其他的类似方法也是对于简单, 复用性强的任务使用低成本的方式计算, 或者通过一些变换方式压缩动作序列等负担较大的数据提高效率</li>
<li>Learning from Human Videos: 也就是将人类行为与机器行为对齐, 从而提高机器人行为的表现</li>
<li>World Model-based VLA: 世界模型(模拟环境), 允许一定程度上对未来状态的预测, 从而让 agent 能根据预测修正自己的策略, 并且世界模型也可以同步改进</li>
</ul>
<div class="note info"><ul>
<li><code>Robotic Process Reward Model (RPRM)</code>: 一种用于强化学习和机器人控制的框架, 为机器人在执行特定任务时的行为提供奖励信号, 进而不断优化其行为模式</li>
<li><code>HIL-ConRFT（Hardware-in-the-Loop Control with Reinforcement Feedback Training）</code>: 结合了硬件在环（HIL）测试和 RL 的控制方法, 旨在优化复杂系统的控制策略, HIL 测试允许在真实硬件上进行控制算法的验证和优化, 通过将模拟环境与实际硬件连接, 确保控制系统在真实条件下的有效性, 结合强化学习的反馈机制, HIL-ConRFT 可以根据系统的表现持续优化控制策略。通过从环境中获取奖励信号, 系统能够学习最佳操作策略</li>
<li><code>HIL-SERL（Hardware-in-the-Loop Sample-Efficient Reinforcement Learning）</code>: 是一种结合了硬件在环（HIL）测试和样本高效强化学习（Sample-Efficient Reinforcement Learning）的框架, 旨在优化机器人和自动化系统的控制策略</li>
<li><code>SFT（Supervised Fine-Tuning）</code>: 通常用于在预训练模型的基础上进行进一步的监督学习, 模型首先在大规模数据集上进行预训练, 以学习通用的特征和表示。然后, 在特定任务的数据集上进行微调, 使模型能够针对特定任务进行优化</li>
</ul>
</div>
<h3 id="vla-的特点">VLA 的特点</h3>
<ol type="1">
<li>多模态:
<ol type="1">
<li>Shared Embedding Space: 视觉, 文字信息会被一起嵌入一个共享的语义对齐空间, 这个共享的语义空间能减少转码过程的损失, 提供更好的推理能力, 理解能力</li>
<li>Multimodal Token-Level Integration: 通过 transformer 可以将连续的视觉文字等信息转化为离散序列, 提供了对这些不同类型的信息的综合性理解能力, 减少语义转化损失</li>
<li>Comprehensive Modal Compatibility: 得益于强大的 VLM, VLA 天生有着与具体模态无关的语义对齐能力, 新的传感器信息, 例如声音, 点云等信息都可以在不影响主干网络的情况下加入</li>
</ol></li>
<li>Instruction Following
<ol type="1">
<li>Semantic Instruction Grounding</li>
<li>Task Decomposition and Collaboration</li>
<li>Explicit Reasoning via Chain-of-Thought: 链式的思考能力能让 VLA 能一定程度上预测未来的图像, 减少了短视现象</li>
</ol></li>
<li>Multi-Dimensional Generalization
<ol type="1">
<li>Cross-Task Generalization: 相比传统模型任务特化的训练过程, 很多 VLA 模型有着 zero-shot 或者 few-shot 级别的泛化能力, 有时可以直接用到没有特别训练的领域</li>
<li>Cross-Domain Data Generalization: 相比传统模型, 多模态的 VLA 可以接受各种各样的数据, 从而得到相应的泛化能力</li>
<li>Cross-Embodiment and Sim-to-Real Generalization: 由于现代 VLA 的解耦性质, 其经过抽象的规划起和底层动作解码器可以互相解耦, 也就是不同的机器人形态也可以被泛化</li>
</ol></li>
</ol>
<h3 id="数据集和基准测试">数据集和基准测试</h3>
<ol type="1">
<li>真实世界机器人数据集: 尽管理论上讲真实世界是检验以及训练的最好数据, 但由于条件限制, 往往不够全面或者丰富</li>
<li>模拟数据集: 其优点是可以针对想要特化的能力调整模拟环境, 而缺点则是相比现实可能有较大差距, 导致可迁移不足</li>
<li>人类行为数据集: 对一些特定任务, 例如物体识别, 任务分解等较为实用</li>
<li>具身数据集: 强调主动感知, 推理和执行, 提供严格的协议来评估通用 VLA 中的高级语义规划</li>
</ol>
<h3 id="总结和展望">总结和展望</h3>
<ol type="1">
<li>数据集和基准测试: 当前的真实数据稀少, 模拟环境存在缺陷, 可能需要更好的结合方法; 基准测试需要增加长期性, 且更丰富的指标</li>
<li>记忆机制和长期规划: 大多数当前的 VLAs 依赖于逐帧推理, 产生了短视问题, 需要增加记忆以及未来预测能力</li>
<li>3D&amp;4D 感知: 当前的主要输入形式依旧是 2d 图片, 如果能增加输入维度(空间与时间维度)或许能增强模型能力</li>
<li>移动操纵: 鉴于现实任务特点, 导航能力和交互能力应该更加集成</li>
<li>多代理合作: 现实有很多协作任务, 且增加代理数量可以更灵活地划分子任务</li>
<li>开放世界终身学习: 大部分 VLA 模型基于静态数据集, 由于硬件以及当前软件性质难以长期积累知识</li>
<li>模型效率: 在机器人平台的性能往往受限, 因此需要平衡存储, 计算成本与模型表现</li>
</ol>
<h2 id="π0-a-vision-language-action-flow-model-for-general-robot-control">π0: A Vision-Language-Action Flow Model for General Robot Control</h2>
<p>VLM 大模型的发展, 不仅在通用能力上取得了进步, 同时由于海量数据带来的能力, 在一些专业任务上也有应用空间<br />
来到 VLA 领域, 想要融合 VLM 并得到一定的行动能力, 有几个挑战:</p>
<ol type="1">
<li>这样的研究需要很大规模的数据和训练</li>
<li>需要能有效从海量数据中学习的模型架构</li>
<li>需要正确的训练技巧</li>
</ol>
<p>π0 正是为了解决这三个问题的原型模型, 相比其他系统, 其技术特点:</p>
<ol type="1">
<li>跨实体地进行训练</li>
<li>为了提供对复杂动作的生成能力使用动作块体系和流匹配方法</li>
</ol>
<p>最近涌现出一批高质量数据集(关于机器人控制), 但往往总共的时长规模在 10h 左右甚至更少, 而为了尽可能地获取综合能力, 本文使用量 10kh 的演示数据(由研究者自己的数据和一些开源数据补充组成), 除了数量以外, 在涉及机器人的种类、任务类别数、标注数外规模也较大<br />
与其他领域的大模型一样, 极大规模的输入也产生了极强的效果</p>
<div class="note info"><ul>
<li>Broad Generalization: 一个模型或系统在面对新数据或未见过的任务时, 能够有效地适应和推理的能力。这种概念通常在机器学习和人工智能领域讨论, 特别是在评估模型的泛化能力时</li>
</ul>
</div>
<h3 id="模型介绍">模型介绍</h3>
<p>其主干可以说是一个基于 tf 的视觉模型, 将图像输入编码嵌入到语言空间, 但正如很多动作模型一样, 出于任务考虑输入输出数据会增加机器人感知与动作<br />
模型使用条件流匹配(之前写的 <a href="https://thinklive1.github.io/thinklive/48061/#flow-matching">介绍博文</a>)来对连续的动作分布建模<br />
模型不依赖特定的 VLM, 为了方便实验中使用 <code>PaliGemma</code></p>
<p>受一个叫 <code>transfusion</code> 的模型(用一个 tf 处理不同类型的输出, 对连续输出使用流匹配的损失函数, 对离散输出使用交叉熵的损失函数)启发, 基于 transfusion, 在工程上(高情商说法), 发现机器人特有的 token 上(即动作与状态), 应用特定的权重集合能提高表现<br />
作为一个生成任务, 技术上具体可以描述为对分布 <span class="math inline">\(p(A_t|o_t)\)</span> 建模, 其中 At 是从 t 下标开始, 长度为 H 的动作序列（动作块）, 文中的 H 为 50, 而观察 o 由 RGB 图像、语言指令和机器人状态(关节角度)组成分别用 I, l, q 表示<br />
虽然输入类别有很多, 但 I, q 都会由编码器以及一个线性投影层投影到 l 的嵌入空间<br />
将损失函数定义为:</p>
<p><span class="math display">\[L^{\tau}(\theta)=\mathbb{E}_{p(\mathbf{A}_{t}|o_{t}),q(\mathbf{A}_{t}^{\tau}|\mathbf{A}_{t})}||\mathbf{v}\theta(\mathbf{A}_{t}^{\tau},\mathbf{o}_{t})-\mathbf{u}(\mathbf{A}_{t}^{\tau}|\mathbf{A}_{t})||^{2}\]</span></p>
<p>其中上标是流匹配的时间步, 下标则是机器人的时间步, <span class="math inline">\(\tau\ \in\ [0,1]\)</span> , 训练时从标准正态分布中采样一个 <span class="math inline">\(\epsilon\)</span> , 令噪声行动 <span class="math inline">\(A_t^\tau = \tau A_t + (1-\tau ) \epsilon\)</span> , 训练目标是让 <span class="math inline">\(\mathbf{v}\theta(\mathbf{A}_{t}^{\tau},\mathbf{o}_{t})\)</span> 与去噪向量场 <span class="math inline">\(\mathbf{u}(\mathbf{A}_{t}^{\tau}|\mathbf{A}_{t}) = \epsilon - A_t\)</span> 匹配, 初始动作输入是一个从标准正态分布中取样的噪声动作</p>
<p>这里先不讨论太多细节, 整体上这是比较标准的条件流匹配方法, 只不过学习的不是直接的动作输出, 而是如何对输入降噪, 最后我们得到的其实是一个和定义出来的降噪向量场尽可能接近的一个基于模型参数的向量场 v<br />
得到向量场后, 推断过程就是做一个积分: <span class="math inline">\({\bf A}_{t}^{\tau+\delta}={\bf A}_{t}^{\tau}+\delta{\bf v}_{\theta}({\bf A}_{t}^{\tau},{\bf o}_{t})\)</span><br />
其中 $ $ 是积分的步长, 这里设 0.1, 由于 $ $ 范围是 <code>[0,1]</code>, 这就意味着一共积分 10 次<br />
由于作者是企业, π0 其实算是一篇技术介绍或者说报告, 原理写得很简略, 后文会结合其他资料阐述 类似 LLM, 本文也将模型分为两个阶段, 预训练阶段让模型尽可能获得泛化能力(包括对物理任务的泛化), 后训练阶段则让模型流畅地执行下游任务</p>
<h3 id="训练与实验">训练与实验</h3>
<p>数据上, 正如前文所说, π0 使用了一组庞大的数据集, 包括一些开源数据与自己的数据, 且与以往数据不同的是, 自己的数据部分复杂度较高, 可以视为一些行为的组合<br />
由于数据组成较为复杂, 因此对不同的数据集来源进行加权, 动作和状态向量(a, q)按最复杂的机器人设置, 对更简单的机器人, 则填零或者掩盖(mask out)掉缺失的维度<br />
此外一个常见的思路是将复杂的任务分解为简单的任务组合, 由于 VLM 赋予了语言能力, π0 可以实现这种程度的语义推断<br />
进一步的实验中, 为了评估语言能力, 对指令进行划分, 只接受整体性指令的模型表现最差, 而接受人类指令和高级 VLM 指令的模型互有胜负, 比没有任务分解的表现好一个阶层<br />
论文还比较了 π0 的无预训练版本和预训练版本以及其他模型(包括这篇博文写过的 ACT), 较为符合常识的是, 对任务类型相似的任务, 预训练能带来更大提升, 但对其他模型, 往往直接从头开始面向任务训练的模型表现最好; 有些不不符合常识的是, 部分任务训练时间更长的版本表会更差(但降幅不高)<br />
最后的实验针对最为复杂的任务, 例如兼顾移动与操控, 对这种任务, 往往预训练并微调后的模型有相当好表现, 有时能成倍于从头训练版本和 zero-shot 版本</p>
<h3 id="细节">细节</h3>
<p>前置知识: <a href="https://thinklive1.github.io/thinklive/48061/#flow-matching">flow matching</a></p>
<p>参考 :</p>
<iframe src="//player.bilibili.com/player.html?isOutside=true&amp;aid=114844942470263&amp;bvid=BV142uFzWEdY&amp;cid=31186487108&amp;p=1&amp;autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true">
</iframe>
<p>这部分解释一些技术细节:</p>
<p>先放一张架构图<br />
<img src="/assets/ml/image-5.png" /></p>
<p>π0 将输入分为 3 个 block, 分别是 图像与文字指令、机器人状态、动作, 第一个 block 由 VLM 处理, 后两个交给动作专家<br />
从 VLM 部分说起, 先简单地对图像处理为 token, 将图像 token 和文字 token 拼接, 交给注意力来计算 kvq, 这是个很简单的处理, 需要注意的是为了提高效率这里的 kv 之后会复用<br />
然后是较为关键的动作专家, 机器人状态是一种很单纯的输入不需要额外处理, 而对(噪声化的)动作, 因为动作是有序的, 需要编码时间步信息(使用正弦编码), 这里不是常见的相加, 而是使用拼接(concat), 最后会得到两倍宽度的一个序列, 然后在经过线性层和激活函数得到最后的 action tokens<br />
<img src="/assets/ml/image-6.png" /></p>
<p>现在我们有了三种输入, 符合常识地, 他们之间存在顺序关系(图文-&gt; 状态-&gt; 动作), 后面的可以“注意”前面的数据(如果长度不够就 padding 0), 这种注意力关系会用 mask 实现, 且使用之前的 kv cache<br />
所有这些信息最后喂给动作专家(一个较小的 VLM), 但沿用的 kv-cache 来自最初的大型 VLM, 结果投影至动作空间, 也就是流匹配的向量 v<br />
那么 π0 使用的流匹配具体是怎样的呢, 作者根据先前研究的经验使用简单的线性高斯, 即 <span class="math display">\[ q(\mathbf{A}_{t}^{\tau}|\mathbf{A}_{t})={\mathcal{N}}(\tau\mathbf{A}_{t},(1-\tau)\mathbf{I}) ; \mathrm{A}_{t}^{\tau}=\tau\mathrm{A}_{t}+(1- \tau )\epsilon \]</span><br />
而 <span class="math inline">\(\mathbf{u}(\mathsf{A}_{t}^{\mathsf{\tau}}|\mathsf{A}_{t})\ =\ \mathsf{\epsilon}-\ A_{t}\)</span> 此外, <span class="math inline">\(\tau\)</span> 来自于 β 采样</p>
<p>简单回顾一下流匹配, 它的概率路径应该有的性质:</p>
<ol type="1">
<li>t = 0 时, p(x|x1)服从一个标准正态分布</li>
<li>t = 1 时, p(x|x1)服从一个均值 x1, 方差较小的正态分布</li>
</ol>
<p>上面提到的 t 其实对应本文的 <span class="math inline">\(\tau\)</span>, 确实是满足条件的, 而这个 u 也很直观, 既然对原来的 At 加上噪声(动作噪声同系数, 动作正噪声负), 那么去噪过程就是噪声减去动作, 也就是噪声如何还原动作</p>
<h1 id="实例">实例</h1>
<h2 id="openin"><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/Ro3Vrjx0GiN8eWe6veT-kg">OpenIN</a></h2>
<p>本文提出了一种「<strong>开放词汇、实例导向</strong>」的导航系统, 该系统「<strong>支持多模态和多类型对象导航指令</strong>」, 能够实现对 <strong>位置可变日常实例</strong> 的有效导航<br />
OpenIN 的框架和大部分系统比较类似, 都可以分为场景图构建以及图更新(包括导航)两个主要模块</p>
<ol type="1">
<li><p>CRSG<br />
具体而言, 本系统的地图称为 <code>载体关系场景图（CRSG）</code>, 它基于一个开放词汇实例图 M, 结构上分为 <strong>建筑与房间层、载体层、承载层和其他对象层</strong></p></li>
<li><p>位移对象导航策略<br />
本系统特殊之处在于, 输入是多模态的, 即可以是文本、图像的任意组合:</p></li>
</ol>
<ul>
<li>对图像输入: 使用大语言模型获取文本描述</li>
<li>对文本输入以及图像的文本描述: 使用 SBERT 对其编码, 与 CRSG 对象的特征比较余弦相似度, 最高者作为模板对象 Ot</li>
</ul>
<p>确定目标后, 以 MDP 模型定义探索过程:</p>
<p>定义状态空间: <span class="math inline">\(S_{t}=(L_{t},C R_{t},C T_{t},F_{t})\)</span> 其中 L 为机器人位姿, CR 是未探索的载体层对象集合, CT 为未探索载体层对象上的候选目标对象集合, F 是一个表示是否找到目标的布尔值, 对初始状态 CR0＝ 载体层对象全集, CT0 是上一轮选择的候选对象 Ot<br />
定义动作空间: <span class="math inline">\(A=S t o p,E x p l o r e(c r),G o t o(c t)|c r\in C R_{t},c t\in C T_{t}\)</span> 其中 stop 表示任务完成或者所有载体对象已探索, explore 表示探索载体层对象, goto 表示导航到 ct 的位置<br />
定义策略(以下省略下标 t, 用伪代码表示):<br />
1. if F ==1 CR == <span class="math inline">\(\emptyset\)</span> then a = stop<br />
2. if F == 0 &amp;&amp; <span class="math inline">\(CT_t !=\emptyset\)</span> , CT.sort(by priority); SS = similarity(CT, O) ;<br />
3. Depth = distance(L, CT); D' = mean(distance(camera, CT));<br />
4. <span class="math inline">\(P_{-}R(O_{t j})=\omega_{r}\cdot\frac{\omega_{1}s s_{t j}\cdot f(\bar{d}_{t j})}{1\ +\omega_{2}d_{t j}}\)</span> (priority of <span class="math inline">\(O_{tj}\)</span> in CT)<br />
5. if F == 0 &amp;&amp; $CT ==$ &amp;&amp; $ CR!=$ then (CR.description, O.image, O.image_description) -&gt; LLM -&gt; $cr_k CR_t $ ( <span class="math inline">\(cr_k\)</span> is best target )<br />
6. <span class="math inline">\(a_t\)</span> = Explore( <span class="math inline">\(cr_k\)</span> ); excute( <span class="math inline">\(a_t\)</span> )<br />
7. if <span class="math inline">\(a_t\)</span> == Explore(cr) || <span class="math inline">\(a_t\)</span> == Goto(ct) then while moving: <span class="math inline">\(CR_{observed}\)</span> = observed objects which don't include candidates in the radial r ; <span class="math inline">\(CR_{new}\)</span> = new candidates in unexplored CT objects $</p>
<p>总结: 每轮中在候选对象里优先级排序, 对最佳候选对象的位置, 机器人导航到此处并探索是否存在目标对象; 如果没有候选对象但有为探索载体对象, 让 llm 选一个候选者进行探索; 在探索过程中, 机器人会不断更新 map 且寻找新的候选对象或者载体对象</p>
<h2 id="learning-fine-grained-bimanual-manipulation-with-low-cost-hardware">Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware</h2>
<p>精细的操控任务例如扎电线、将电池插入槽中, 往往需要较大的计算成本或者高端的硬件设备, 而想要落实应用, 成本控制是非常重要的, 因此需要找到在有限的预算下实现精细操作的智能<br />
本文基于端到端的模仿学习, 训练数据也完全来自于现实的演示, 在高精度任务中, 模仿学习的缺点是对策略错误较为敏感, 且人类演示数据相差较大, 不便于学习; 为此, 本文提出了一种 <code>Action Chunking with Transformers (ACT)</code> 算法, 可以通过低成本地通过动作序列学习, 并得到不错的准确率</p>
<h3 id="前言-2">前言</h3>
<p>举一个生活中的例子, 例如打开一个有盖的杯子, 这就需要一只手(假设为右手)握住杯子, 倾斜让左手靠近杯盖, 然后辨别出杯盖边缘, 让左手轻轻掀起; 这种任务对 ai 来说相当精细<br />
由于需要在有限的成本内完成精细任务, 我们选择端到端的模型, 使用低成本的摄像头 rgb 图像作为输入, 让模型只学习操作策略而无需对整个环境建模(当然对这种复杂任务建模也非常难, 例如在电子游戏界, 几乎没有游戏能做好吃饭等人物的日常动画)<br />
以下简述本文使用的一些技术:</p>
<ul>
<li>远程操控(tleoperation): 使用两只大致相同的低成本机械臂, 共用一个空间地图, 使用一下些 3d 打印组件来增强反向驱动, 总预算控制在 $20k 内</li>
<li>模仿学习算法: 为了避免模仿学习常见的累积误差问题, 参考心理学的 <code>动作块</code>, 让学习策略预测接下来 k 步的一连串动作, 防止某个错误的一步影响后面的推理; 且能减少一些不相干因素的影响, 例如演示中的部分暂停
<ul>
<li>为了提高策略的平滑度, 设置了 <code>temporal ensembling</code> 机制, 更频繁地查询策略, 且将重叠的动作块取平均值</li>
<li>使用 Transformers 处理动作块(序列数据)</li>
</ul></li>
</ul>
<div class="note info"><ul>
<li><code>Backdriving</code> : 在机械和控制系统中使用的术语, 通常指的是在某些系统中, 外部力或运动驱动系统的运动, 而不是通过其内部控制机制来实现, 也就是正向驱动是电机驱动机械装置, 反向驱动则可以是相对机械的外力——比如人力也可以驱动机械臂</li>
<li><code>compounding error（累积误差）</code>: 指在(模仿)学习过程中, 由于对错误示范的逐步复制和执行, 错误逐渐积累, 从而影响系统的整体表现</li>
<li><code>action chunking</code>: 心理学术语, 将一系列动作或行为组合成一个更大、更有意义的单元（或“块”）, 以提高任务执行的效率和流畅性</li>
</ul>
</div>
<h3 id="real-work">real work</h3>
<ol type="1">
<li><p><code>Behavioral cloning</code><br />
行为克隆是最简单的模仿学习方法, 但很明显也有非常多的问题, 例如之前提及的累积误差, 应该解决思路是使用额外的在线学习或者额外标注来更正, 但对低成本硬件的远程操纵常见, 难以适用<br />
如果常识在数据集加入噪声, 很可能会直接让训练失败, 又或者离线地生成校正数据, 但这种方法这在部分场景有效<br />
出于对以上过往问题的考虑, 本文引入了 <code>动作块</code> 机制</p></li>
<li><p>实体机器介绍<br />
如前文介绍, 本文的技术选择准则可以概括为“物美价廉”, 两个机械臂控制在数千美元, 本文使用的 <code>ViperX arm</code> 有 750g 的负重能力,1.5m 的宽度, 5-8mm 的准确度, 其关键电机可替换, 但手指对复杂任务不够适用, 为此 3d 打印了一些手指, 并用 Grip tape(握把胶带)缠一圈来提高抓取能力(it just works!)<br />
接下来要选择远程操作系统, 市面上一些常见的系统通过 vr 或者摄像头捕捉手臂姿势来操纵机械臂, 这称为 <code>task-space mapping</code>, 本文使用 <code>WidowX</code> 一种让用户通过反向驱动 "leader"(一个更小的机器部件)来驱动机械臂("follower")的系统, 这称为 <code>joint-space mapping</code><br />
为什么选择这种系统:</p>
<ol type="1">
<li>系统中的机械臂有 6 个自由度且没有冗余, 如果使用反向运动学, 很容易陷入起一点</li>
<li>共用空间减少计算消耗和延迟</li>
<li>leader 的重量防止用户动作过快且相比 VR 控制器减少振动</li>
</ol></li>
</ol>
<p>除此之外, 系统做了一些改造减少 leader 的耗力, 详见官网<br />
这个机器系统称为 ALOHA</p>
<p><img src="/assets/ml/image-1.png" /></p>
<ol start="3" type="1">
<li>ACT 理论部分</li>
</ol>
<p>ACT 系统可以视为一种有条件(conditional)的 VAE(之前写的 <a href="https://thinklive1.github.io/thinklive/48061/#vae-%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8">介绍博文</a>), 其编码器将动作序列和关节位置作为观察结果(输入数据)编码为 z(测试时会丢弃编码器), tf 部分的编码器接受所有相机输入、关节位置以及 z, 其解码器生成一个动作序列<br />
实操中, 让操作员操作 leader 机器人(leader 由于要人类操纵, 规格与 follower 不同, 两者的动作参数之间可以隐式转换), ACT 不断预测人类操作员在给定观察下的未来 k 步操作<br />
这样有个额外的好处, 由于学习的样本是人类提供的行为, 而这种行为不一定是符合马尔科夫假设的, 单步学习很可能会被暂停之类的行为误导, 而如果误导行为集中在一个动作块中, 对训练的损害则不会那么巨大<br />
但相应的, 由于每 k 步切换, 可能过程不会太平滑, 因此在每步都会查询策略, 类似滑动窗口, 这样依赖每次预测的动作块都会有部分重叠, 因此为了充分利用这些信息, 对未来时刻的所有预测动作, 计算一个加权和, 这个过程称为 <code>temporal ensembling</code>, 实验发现这样的聚合效果要好于传统的对于时间上相邻动作动作的平滑化, 这部分机制会在下文详细讨论</p>
<p>接下来阐释对输入的处理:</p>
<ol type="1">
<li>我们用 CVAE 处理人类输入, 它由一个编码器和一个解码器组成, 解码器会预测 z 的均值与方差, 用作对角高斯分布, 也就是根据输入数据拟合一个高斯分布</li>
<li>出于训练速度考虑, 对输入数据, 不使用图像观察结果, 只用机器人的本体感知数据(例如姿势, 角度等)</li>
<li>解码器则会使用 z 以及当前观察结果(图像+关节位置)预测未来的动作序列</li>
<li>测试时, 设 z 为先验分布(给定)的均值, 如果想要确定性地解码, 可以直接设 0(注意是测试时这么做)</li>
<li>训练的过程中不断地最大化模型输出与演示数据里的相同动作块的对数概率; 同时也使用标准 VAE 的两个标准(重建损失与正则化项), 对正则化项, 乘以一个超参数 β(用于控制 z 包含的信息量, 更高的正则化会导致更少的信息转入 z)</li>
</ol>
<p>具体实现:</p>
<p><img src="/assets/ml/image-2.png" /></p>
<ol type="1">
<li>使用 tf 作为编码和解码器, 具体来说, CVAE 的编码器类似 BERT, 其输入是当前关节位置以及长度为 k 的演示动作序列, 同样类似 BERT, 每段序列会有个 <code>CLS</code> 前缀, 具体见上图左侧部分</li>
<li>CVAE 解码器实际上更为复杂, 由 <code>ResNet image encoders, a transformer encoder, a transformer decoder</code> 组成, 接受的输入也比编码器更多, 其动作空间是机器人的所有关键位置也就是一个 14 维向量(一只手对应 7 维), 大致过程如下
<ol type="1">
<li>resnet 的编码器将图像输入转化为特征地图, 再被展平(flatten)为序列(另外使用正弦位置编码保存空间关系)</li>
<li>处理完图像后, 特征数据以及关节位置, z 作为输入通过一个线性层投影到一个向量组, 其向量数为 k</li>
<li>得益于前一步中得到的固定长度的向量组, tf 解码器通过 cross-attention 产生相同长度的输出组, 然后在经过 MLP(多层感知机)的投影为 14 长度的动作向量组</li>
</ol></li>
</ol>
<p>神奇的是, 用 L1 损失作为重建损失的表现更好, 此外用 <code>delta joint position</code>(关节变化量)训练效果不如目标位置</p>
<div class="note info"><ul>
<li>逆向运动学（Inverse Kinematics, IK）: 运动学（Kinematics）是在给定所有关节角度的情况下计算链接结构（如一节人体的关节）的末端的空间位置的过程; 逆向运动学则相反, 知道末端状态, 需要求解关节应该是什么角度</li>
<li>奇异点（singularities）: 机器人在某些特定配置下失去自由度或控制能力的状态。在这些状态下, 机器人可能无法有效地执行运动或控制其末端执行器的位置和姿态</li>
<li>变分自编码器（Variational Autoencoder, VAE）: 一种生成模型, 结合了神经网络和贝叶斯推断的思想, 广泛应用于无监督学习、数据生成和特征学习</li>
<li>风格变量（Style Variable）: 生成模型中, 用于控制输出样本风格或特征的潜在变量, 能够影响生成内容的“风格”而不改变其“内容”</li>
</ul>
</div>
<h3 id="实验与总结-1">实验与总结</h3>
<p>由于此部分大多是现实情况的细节, 省略这些部分, 只说结果<br />
ACT 主要创新点在于能够预测连续的, 精确的动作序列, 而一些先前的模仿学习方法只能像分类问题一样预测离散动作, 此外端到端的训练方式对特化的任务可能有更好的效果<br />
消融实验中, CVAE, 动作块等机制都较为有效<br />
相比先前方法, ACT 大部分任务都取得了更好的表现, 但是会在对比度低图像难以捕捉特征的场景表现不佳</p>
<div class="note info">
</div>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"><i class="fa fa-tag"></i> 人工智能</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
          </div>

        
  <div class="social-like a2a_kit a2a_kit_size_32 a2a_default_style">
    <a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a>
      <a class="a2a_button_facebook"></a>
      <a class="a2a_button_twitter"></a>
      <a class="a2a_button_wechat"></a>
      <a class="a2a_button_qzone"></a>
  </div>

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/thinklive/23089/" rel="prev" title="pdfs">
                  <i class="fa fa-angle-left"></i> pdfs
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/thinklive/44308/" rel="next" title="计算机专业英语术语表">
                  计算机专业英语术语表 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2023 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">thinklive</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">615k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">37:15</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 技术支持
  </div><script defer src="/lib/three.js"></script><script defer src="/lib/lines.js"></script><script defer src="/lib/waves.js"></script>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.24/fancybox/fancybox.umd.js" integrity="sha256-oyhjPiYRWGXaAt+ny/mTMWOnN1GBoZDUQnzzgC7FRI4=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>


  <script src=""></script>
  <script src="/%5Bobject%20Object%5D"></script>
  <script src="/%5Bobject%20Object%5D"></script>
  <script src="/%5Bobject%20Object%5D"></script>
  <script src="/%5Bobject%20Object%5D"></script>


<script>
var options = {
  bottom: '64px', // default: '32px'
  right: 'unset', // default: '32px'
  left: '32px', // default: 'unset'
  time: '0.5s', // default: '0.3s'
  mixColor: '#fff', // default: '#fff'
  backgroundColor: '#fff',  // default: '#fff'
  buttonColorDark: '#100f2c',  // default: '#100f2c'
  buttonColorLight: '#fff', // default: '#fff'
  saveInCookies: true, // default: true,
  label: '🌓', // default: ''
  autoMatchOsTheme: true // default: true
}
const darkmode = new Darkmode(options);
darkmode.showWidget();
</script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>


  <script class="next-config" data-name="wavedrom" type="application/json">{"enable":true,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/wavedrom/3.3.0/wavedrom.min.js","integrity":"sha256-IRMDzTC+wK5stMucZ/XSXkeS5VNtxZ+/Bm8Mcqfoxdo="}}</script>
  <script class="next-config" data-name="wavedrom_skin" type="application/json">{"enable":true,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/wavedrom/3.3.0/skins/default.js","integrity":"sha256-fduc/Zszk5ezWws2uInY/ALWVmIrmV6VTgXbsYSReFI="}}</script>
  <script src="/js/third-party/tags/wavedrom.js"></script>

  <script src="/js/third-party/fancybox.js"></script>

  <script src="/js/third-party/pace.js"></script>

  <script src="/js/third-party/addtoany.js"></script>

  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":false,"archive":false,"delay":true,"timeout":3000,"priority":true,"url":"https://thinklive1.github.io/thinklive/52409/"}</script>
  <script src="/js/third-party/quicklink.js"></script>
<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

<script>
var options = {
  bottom: '64px',
  right: '32px',
  left: 'unset',
  time: '0.5s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: true,
  label: '🌓',
  autoMatchOsTheme: true
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
</script>
<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"thinklive1/blog_comments","issue_term":"pathname","theme":"photon-dark"}</script>
<script src="/js/third-party/comments/utterances.js"></script>
<script>
    const snowflakes = ["❄", "❄", "❆", "❅", "✥","❄", "❄", "❆", "❅", "✥","✻"];
    // 创建雪花
    function createSnowflake() {
        const snowflake = document.createElement("span");
        snowflake.classList.add("snowflake");
        const randomIndex = Math.floor(Math.random() * snowflakes.length);
        snowflake.textContent = snowflakes[randomIndex];
        
        // 起始位置
        /* 80%概率 生成在页面两侧 30% 的位置
        const probability = Math.random();
        let startPosition = Math.random() * 100;

        if (probability < 0.8) {
            startPosition = Math.random() < 0.5 ? Math.random() * 30 : (Math.random() * 30) + 70;
        }
        snowflake.style.left = `${startPosition}vw`;
        */
        snowflake.style.left = `${Math.random() * 100}vw`;
        snowflake.style.top = `-30px`;
        // 雪花大小与透明度
        const size = Math.random() * 18 + 10;
        snowflake.style.fontSize = `${size}px`;
        const opacity = Math.random() * 0.6 + (size > 18 ? 0.4 : 0);
        snowflake.style.setProperty("--opacity", opacity);
        // 动画持续时间
        const fallDuration = Math.random() * 10 + 10;
        // 旋转持续时间
        const rotateDuration = Math.random() * 3 + 1;

        snowflake.style.animationDuration = `${fallDuration}s, ${fallDuration}s`; // 向 CSS 添加淡出动画的持续时间
        // 横向幅度
        const translateX = (Math.random() * 500 - 200);
        snowflake.style.setProperty("--translateX", `${translateX}px`);
        // 纵向幅度
        snowflake.style.setProperty("--translateY", `${window.innerHeight}px`);

        document.body.appendChild(snowflake);
        // 移除雪花
        setTimeout(() => {
            snowflake.remove();
        }, fallDuration * 1000);
    }
    
    function snowfallAnimation() {
        // 载入时若边栏是隐藏状态则不加载雪花
        const sidebarnav = document.querySelector('.sidebar');
        const sidebarnavdisplay = window.getComputedStyle(sidebarnav).getPropertyValue('display'); 
        if (sidebarnavdisplay !== 'none') {
            createSnowflake();
        }
        setTimeout(snowfallAnimation, 500); // 生成速度，毫秒
    }
    snowfallAnimation();
function toggleMode() {
    console.log("change color!");
    const root1 = document.documentElement;

    // 检查当前 color-scheme
    const isLightMode = getComputedStyle(root1).getPropertyValue('--content-bg-color').trim() === '#fff';

    if (isLightMode) {
        // 切换到暗模式
        const images = document.querySelectorAll('img'); // 选择所有<img>标签
        images.forEach(img => {
            img.style.filter = 'brightness(50%)'; // 设置亮度为50%
        });

        root1.style.setProperty('--content-bg-color', '#333');
        root1.style.setProperty('--link-color', '#aaa');
        root1.style.setProperty('--text-color', '#fff');
        root1.style.setProperty('--highlight-background', '#444');
        root1.style.setProperty('--highlight-foreground', '#bbb');
        root1.style.setProperty('--btn-default-bg', '#777');
        root1.style.setProperty('--menu-item-bg-color', '#777');
        root1.style.setProperty('--table-row-odd-bg-color', '#444');
        root1.style.setProperty('--note-warning-bg-color', '#555');
        root1.style.setProperty('--note-bg-color', '#555');
        root1.style.setProperty('--note-info-bg-color', '#555');
        root1.style.setProperty('--highlight-gutter-foreground', '#98d9ffff');
        root1.style.setProperty('', '#777');
        root1.style.transition = 'all 0.5s ease';

    }

    else {
        const images = document.querySelectorAll('img'); // 选择所有<img>标签
        images.forEach(img => {
            img.style.filter = 'brightness(100%)'; // 设置亮度为50%
        });
        root1.style.setProperty('--content-bg-color', '#fff');
        root1.style.setProperty('--text-color', '#555');
        root1.style.setProperty('--highlight-background', '#eaeef3');
        root1.style.setProperty('--highlight-foreground', '#00193a');
        root1.style.setProperty('--btn-default-bg', '#fff');
        root1.style.setProperty('--menu-item-bg-color', '#f5f5f5');
        root1.style.setProperty('--note-warning-bg-color', '#fdf8ea');
        root1.style.setProperty('--note-bg-color', '#f9f9f9');
        root1.style.setProperty('--note-info-bg-color', '#eef7fa');
        root1.style.setProperty('--table-row-odd-bg-color', '#f9f9f9');
        root1.style.setProperty('--highlight-gutter-foreground', '#172e4c');
        root1.style.transition = 'all 0.5s ease';
    }
}

function DarkTrigger() {
    console.log('dark!!')
    let isDarkMode = getComputedStyle(document.documentElement).getPropertyValue('--content-bg-color').trim() === '#000';
    console.log(isDarkMode)
    if (isDarkMode) {
        // 切换到暗模式
        const warningNotes = document.querySelectorAll('.post-body .note.warning');
        // 修改背景颜色
        warningNotes.forEach(note => {
        note.style.background = '#666';
        });

        const infoNotes = document.querySelectorAll('.post-body .note.info');
        // 修改背景颜色
        infoNotes.forEach(note => {
        note.style.background = '#666';
        });
    }
}


</script>

 <!--js: 线条特效-->
  <script type="text/javascript" color="255,255,255" opacity='1' zIndex="-1" count="50" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>

<button style="background: #868686;
  width: 3rem;
  height: 3rem;
  position: fixed;
  border-radius: 50%;
  border: none;
  right: unset;
  bottom: 2rem;
  left: 2rem;
  cursor: pointer;
  transition: all 0.5s ease;
  display: flex;
  justify-content: center;
  align-items: center;" class="darkmode-toggle" role="checkbox" onclick="toggleMode()">🌓</button>

  <video autoplay loop muted playsinline style="position:fixed;top:50%;opacity: 0.8;left:50%;min-width:100%;min-height:100%;transform:translateX(-50%)translateY(-50%);z-index:-2;">
  <source src="/images/red.mp4" type="video/mp4">
<!-- hexo injector body_end start --><script src="/assets/mmedia/mmedia-loader.js"></script><!-- hexo injector body_end end --></body>
</html>
