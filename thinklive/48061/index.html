<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.24/fancybox/fancybox.css" integrity="sha256-vQkngPS8jiHHH0I6ABTZroZk8NPZ7b+MUReOFE9UsXQ=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"thinklive1.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"flat"},"fold":{"enable":true,"height":500},"bookmark":{"enable":true,"color":"#66CCFF","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":"utterances","storage":true,"lazyload":false,"nav":null,"activeClass":"utterances"},"stickytabs":true,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="李宏毅机器学习 动手学深度学习">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习笔记 all in one">
<meta property="og:url" content="https://thinklive1.github.io/thinklive/48061/index.html">
<meta property="og:site_name" content="thinklive">
<meta property="og:description" content="李宏毅机器学习 动手学深度学习">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/Pasted%20image%2020250401210138.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/Pasted%20image%2020250402152918.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/Pasted%20image%2020250402152932.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/pk7amw95.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/o7aq5jae.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/output_8_0.svg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Csigma_x%5E2%3D%5Cfrac%7B1%7D%7Bn-1%7D%5Csum_%7Bi%3D1%7D%5En%5Cleft%28x_i-%5Cbar%7Bx%7D%5Cright%29%5E2&amp;consumer=ZHI_MENG">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Csigma%5Cleft%28x%2Cy%5Cright%29%3D%5Cfrac%7B1%7D%7Bn-1%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Cleft%28x_i-%5Cbar%7Bx%7D%5Cright%29%5Cleft%28y_i-%5Cbar%7By%7D%5Cright%29&amp;consumer=ZHI_MENG">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/Pasted%20image%2020250902150544.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/Pasted%20image%2020250831171404.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/seq2seq_v9.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/seq2seq_v9_1.png">
<meta property="article:published_time" content="2025-09-09T07:53:29.840Z">
<meta property="article:modified_time" content="2025-09-09T07:53:29.840Z">
<meta property="article:author" content="thinklive">
<meta property="article:tag" content="课程笔记">
<meta property="article:tag" content="python">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="国立台湾大学">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://thinklive1.github.io/assets/ml/Pasted%20image%2020250401210138.png">


<link rel="canonical" href="https://thinklive1.github.io/thinklive/48061/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://thinklive1.github.io/thinklive/48061/","path":"thinklive/48061/","title":"机器学习笔记 all in one"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>机器学习笔记 all in one | thinklive</title>
  







<script type="text/javascript" async src="/js/fairyDustCursor.js"></script>
<script type="text/javascript" async src="/js/tab-title.js"></script>
<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
  <script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>
<script>
const options = {
  bottom: '64px', // default: '32px'
  right: 'unset', // default: '32px'
  left: '32px', // default: 'unset'
  time: '0.5s', // default: '0.3s'
  mixColor: '#fff', // default: '#fff'
  backgroundColor: '#fff',  // default: '#fff'
  buttonColorDark: '#100f2c',  // default: '#100f2c'
  buttonColorLight: '#fff', // default: '#fff'
  saveInCookies: false, // default: true,
  label: '🌓', // default: ''
  autoMatchOsTheme: true // default: true
}

const darkmode = new Darkmode(options);
</script>
<!-- hexo injector head_end start --><script> let HEXO_MMEDIA_DATA = { js: [], css: [], aplayerData: [], metingData: [], artPlayerData: [], dplayerData: []}; </script><!-- hexo injector head_end end --><link rel="alternate" href="/atom.xml" title="thinklive" type="application/atom+xml">
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>
<script src="/js/tab-title.js"></script>
<script type="text/javascript" async src="/js/text.js"></script>

<!--pjax：防止跳转页面音乐暂停-->
 <script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script>
<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>
<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>


  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">thinklive</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">dirichlet library</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索 | search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-主页-|-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>主页 | home</a></li><li class="menu-item menu-item-标签-|-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签 | tags</a></li><li class="menu-item menu-item-分类-|-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类 | categories</a></li><li class="menu-item menu-item-归档-|-archive"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档 | archive</a></li><li class="menu-item menu-item-相册-|-photo"><a href="/photos/" rel="section"><i class="fa fa-camera fa-fw"></i>相册 | photo</a></li><li class="menu-item menu-item-留言-|-guestbook"><a href="/guestbook/" rel="section"><i class="fa fa-book fa-fw"></i>留言 | guestbook</a></li><li class="menu-item menu-item-感谢-|-thank"><a href="/thanks/" rel="section"><i class="fa custom thanks fa-fw"></i>感谢 | thank</a></li><li class="menu-item menu-item-游戏-|-game"><a href="/game/bad1.html" rel="section"><i class="fa fa-gamepad fa-fw"></i>游戏 | game</a></li><li class="menu-item menu-item-神龛-|-shrine"><a href="/cyberblog/" rel="section"><i class="fa fa-microchip fa-fw"></i>神龛 | shrine</a></li><li class="menu-item menu-item-资源地图-|-resourcemap"><a href="/webstack/" rel="section"><i class="fa fa-list fa-fw"></i>资源地图 | resourcemap</a></li><li class="menu-item menu-item-思维导图-|-mindmap"><a href="/mindmap/index.html" rel="section"><i class="fa fa-map fa-fw"></i>思维导图 | mindmap</a></li><li class="menu-item menu-item-网站地图-|-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>网站地图 | sitemap</a></li><li class="menu-item menu-item-蒸汽-|-steam"><a href="/steamgames/index.html" rel="section"><i class="fa fa custum steam fa-fw"></i>蒸汽 | steam</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索 | search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">


<!--网易云音乐插件-->
<!-- require APlayer -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css">
<script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script>
<!-- require MetingJS-->
<script src="https://cdn.jsdelivr.net/npm/meting@2.0.1/dist/Meting.min.js"></script> 
<!--网易云playlist外链地址-->   
<meting-js
    server="netease"
    type="playlist" 
    id="2762741085"
    mini="false"
    fixed="false"
    list-folded="true"
    autoplay="false"
    volume="0.2"
    theme="#4c4c4c"
    order="random"
    loop="all"
    preload="auto"
    lrc-type="2"
    mutex="true">
    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E6%9C%9F%E5%B7%A5%E4%BD%9C"><span class="nav-number">1.</span> <span class="nav-text">前期工作</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E5%AD%A6"><span class="nav-number">1.1.</span> <span class="nav-text">数学</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%9F%E8%AE%A1"><span class="nav-number">1.1.1.</span> <span class="nav-text">统计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E8%AE%BA"><span class="nav-number">1.1.2.</span> <span class="nav-text">概率论</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E5%B8%83"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">分布</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%A1%E6%81%AF%E8%AE%BA"><span class="nav-number">1.1.3.</span> <span class="nav-text">信息论</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%86%B5"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">熵</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="nav-number">1.1.4.</span> <span class="nav-text">线性代数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%8C%83%E6%95%B0"><span class="nav-number">1.1.4.1.</span> <span class="nav-text">范数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B7%A5%E5%85%B7"><span class="nav-number">1.2.</span> <span class="nav-text">工具</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#cuda"><span class="nav-number">1.2.1.</span> <span class="nav-text">cuda</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%B9%E5%99%A8"><span class="nav-number">1.2.2.</span> <span class="nav-text">容器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#python"><span class="nav-number">1.2.3.</span> <span class="nav-text">python</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%90%86%E8%AE%BA"><span class="nav-number">1.3.</span> <span class="nav-text">理论</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F"><span class="nav-number">1.3.1.</span> <span class="nav-text">张量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="nav-number">1.3.2.</span> <span class="nav-text">神经网络的定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98"><span class="nav-number">1.3.3.</span> <span class="nav-text">常见问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95-optimizer"><span class="nav-number">1.3.4.</span> <span class="nav-text">优化算法 optimizer</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96-normalization"><span class="nav-number">1.3.4.1.</span> <span class="nav-text">归一化 Normalization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">1.3.4.2.</span> <span class="nav-text">梯度下降</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">1.3.4.3.</span> <span class="nav-text">反向传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#momentum"><span class="nav-number">1.3.4.4.</span> <span class="nav-text">Momentum</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#adagrad"><span class="nav-number">1.3.4.5.</span> <span class="nav-text">AdaGrad</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">1.3.5.</span> <span class="nav-text">正则化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7"><span class="nav-number">1.3.6.</span> <span class="nav-text">训练技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E6%9D%83%E5%80%BC"><span class="nav-number">1.3.6.1.</span> <span class="nav-text">初始权值</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#batch-normalization"><span class="nav-number">1.3.6.2.</span> <span class="nav-text">Batch Normalization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8A%91%E5%88%B6%E8%BF%87%E6%8B%9F%E5%90%88"><span class="nav-number">1.3.6.3.</span> <span class="nav-text">抑制过拟合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0"><span class="nav-number">1.3.6.4.</span> <span class="nav-text">超参数</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.</span> <span class="nav-text">神经网络模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#python-%E7%9B%B8%E5%85%B3"><span class="nav-number">2.1.</span> <span class="nav-text">python 相关</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E7%A4%BA%E4%BE%8B"><span class="nav-number">2.1.1.</span> <span class="nav-text">基础训练方法示例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.2.</span> <span class="nav-text">线性模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">2.2.1.</span> <span class="nav-text">线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#python%E5%AE%9E%E7%8E%B0"><span class="nav-number">2.2.1.1.</span> <span class="nav-text">python实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E7%B1%BB"><span class="nav-number">2.2.2.</span> <span class="nav-text">分类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-number">2.2.2.1.</span> <span class="nav-text">感知机</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92discriminative-model"><span class="nav-number">2.2.2.2.</span> <span class="nav-text">逻辑回归(discriminative model)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%A6%82%E7%8E%87%E7%9A%84%E5%88%86%E7%B1%BB%E6%96%B9%E6%B3%95generative-model"><span class="nav-number">2.2.2.3.</span> <span class="nav-text">基于概率的分类方法(generative model)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#softmax"><span class="nav-number">2.2.2.4.</span> <span class="nav-text">softmax</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7"><span class="nav-number">2.2.3.</span> <span class="nav-text">优化技巧</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#normalization"><span class="nav-number">2.2.4.</span> <span class="nav-text">Normalization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Ccnn"><span class="nav-number">2.3.</span> <span class="nav-text">卷积神经网络CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#spatial-transformer-layer"><span class="nav-number">2.3.1.</span> <span class="nav-text">spatial transformer layer</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#self-attention"><span class="nav-number">2.4.</span> <span class="nav-text">self-attention</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%98%E5%BD%A2%E9%87%91%E5%88%9A"><span class="nav-number">2.4.1.</span> <span class="nav-text">变形金刚</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bert"><span class="nav-number">2.4.2.</span> <span class="nav-text">bert</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rnn"><span class="nav-number">2.5.</span> <span class="nav-text">RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#long-short-term-memory-lstm"><span class="nav-number">2.5.1.</span> <span class="nav-text">Long Short-term Memory (LSTM)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%AE%E9%A2%98"><span class="nav-number">2.5.2.</span> <span class="nav-text">问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="nav-number">2.6.</span> <span class="nav-text">深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E5%BF%B5"><span class="nav-number">2.6.1.</span> <span class="nav-text">概念</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="thinklive"
      src="/images/thive.png">
  <p class="site-author-name" itemprop="name">thinklive</p>
  <div class="site-description" itemprop="description">起初，世界是一团思索，它向所有的方向迈了一步，于是万物由此而生</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">44</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">49</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/thinklive1" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;thinklive1" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/t469631989@gmail.com" title="E-Mail → t469631989@gmail.com" rel="noopener me"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/38099250?spm_id_from=333.1007.0.0" title="bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;38099250?spm_id_from&#x3D;333.1007.0.0" rel="noopener me" target="_blank"><i class="fa custom bilibili fa-fw"></i>bilibili</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://steamcommunity.com/id/thinkliving" title="steam → https:&#x2F;&#x2F;steamcommunity.com&#x2F;id&#x2F;thinkliving" rel="noopener me" target="_blank"><i class="fa custom steam fa-fw"></i>steam</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml" rel="noopener me"><i class="fa fa-rss fa-fw"></i>RSS</a>
      </span>
  </div>

<div style="Text-align:center;width:100%"><div style="margin:0 auto"><canvas id="canvas" style="width:60%" height="100" width="700">当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>!function(){function t(t,e){for(var a=0;a<l[e].length;a++)for(var n=0;n<l[e][a].length;n++)1==l[e][a][n]&&(h.beginPath(),h.arc(14*(g+2)*t+2*n*(g+1)+(g+1),2*a*(g+1)+(g+1),g,0,2*Math.PI),h.closePath(),h.fill())}function e(){var t=[],e=/(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date),a=[];a.push(e[1],e[2],10,e[3],e[4],10,e[5],e[6]);for(var r=c.length-1;r>=0;r--)a[r]!==c[r]&&t.push(r+"_"+(Number(c[r])+1)%10);for(var r=0;r<t.length;r++)n.apply(this,t[r].split("_"));c=a.concat()}function a(){for(var t=0;t<d.length;t++)d[t].stepY+=d[t].disY,d[t].x+=d[t].stepX,d[t].y+=d[t].stepY,(d[t].x>i+g||d[t].y>f+g)&&(d.splice(t,1),t--)}function n(t,e){for(var a=[1,2,3],n=["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"],r=0;r<l[e].length;r++)for(var o=0;o<l[e][r].length;o++)if(1==l[e][r][o]){var h={x:14*(g+2)*t+2*o*(g+1)+(g+1),y:2*r*(g+1)+(g+1),stepX:Math.floor(4*Math.random()-2),stepY:-2*a[Math.floor(Math.random()*a.length)],color:n[Math.floor(Math.random()*n.length)],disY:1};d.push(h)}}function r(){o.height=100;for(var e=0;e<c.length;e++)t(e,c[e]);for(var e=0;e<d.length;e++)h.beginPath(),h.arc(d[e].x,d[e].y,g,0,2*Math.PI),h.fillStyle=d[e].color,h.closePath(),h.fill()}var l=[[[0,0,1,1,1,0,0],[0,1,1,0,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,0,1,1,0],[0,0,1,1,1,0,0]],[[0,0,0,1,1,0,0],[0,1,1,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[1,1,1,1,1,1,1]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,0,0,1,1],[1,1,1,1,1,1,1]],[[1,1,1,1,1,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,0,1,1,1,0],[0,0,1,1,1,1,0],[0,1,1,0,1,1,0],[1,1,0,0,1,1,0],[1,1,1,1,1,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,1,1]],[[1,1,1,1,1,1,1],[1,1,0,0,0,0,0],[1,1,0,0,0,0,0],[1,1,1,1,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[1,1,1,1,1,1,1],[1,1,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,1,1,0,0,0,0]],[[0,0,0,0,0,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,0,0,0]]],o=document.getElementById("canvas");if(o.getContext){var h=o.getContext("2d"),f=100,i=700;o.height=f,o.width=i,h.fillStyle="#f00",h.fillRect(10,10,50,50);var c=[],d=[],g=o.height/20-1;!function(){var t=/(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date);c.push(t[1],t[2],10,t[3],t[4],10,t[5],t[6])}(),clearInterval(v);var v=setInterval(function(){e(),a(),r()},50)}}()</script></div>
<img class= 'logo' src="/images/thinklive_cyber.png"; z-index: '0'; style="max-width: 100%; width: auto; height: auto;background-color: --content-bg-color;">

        </div>
      </div>
    </div>

    
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://thinklive1.github.io/thinklive/48061/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/thive.png">
      <meta itemprop="name" content="thinklive">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="thinklive">
      <meta itemprop="description" content="起初，世界是一团思索，它向所有的方向迈了一步，于是万物由此而生">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="机器学习笔记 all in one | thinklive">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习笔记 all in one
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-09-09 15:53:29" itemprop="dateCreated datePublished" datetime="2025-09-09T15:53:29+08:00">2025-09-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">课程笔记</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>16k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>57 分钟</span>
    </span>
</div>

        
        </div>
      </header>
   

    
    
    
    <div class="post-body" itemprop="articleBody"><p><a target="_blank" rel="noopener" href="https://speech.ee.ntu.edu.tw/~hylee/ml/2022-spring.php">李宏毅机器学习</a> <a target="_blank" rel="noopener" href="https://zh.d2l.ai/">动手学深度学习</a></p>
<span id="more"></span>
<h1 id="前期工作">前期工作</h1>
<h2 id="数学">数学</h2>
<h3 id="统计">统计</h3>
<p>机器学习的本质是寻找一个理想的函数,这个理想可以定义为在训练集和全集上表现几乎一致,使用统计的原理可以尽可能理解这其中的一些关系:<br />
定义: <code>|𝐿 ℎ, 𝒟𝑡𝑟𝑎𝑖𝑛 − 𝐿 ℎ, 𝒟𝑎𝑙𝑙 | &gt; 𝜀</code>时<code>Dtrain</code>是差的,那么:</p>
<p><span class="math display">\[P(\mathcal{D}_{t r a i n}\;i s\;b a d)\leq|\mathcal H|\cdot2e x p(-2N\varepsilon^{2})\]</span></p>
<p>其中,N是训练样本数,H为备选函数的集合,也就是如果想要这个理想条件,H和N会存在这种约束条件<br />
需要注意的是,这个理想条件并不包括具体的loss值,如果H过小,那么loss就可能会很大</p>
<h3 id="概率论">概率论</h3>
<p>数学期望:</p>
<p><span class="math display">\[E[X]=\sum_{x}x P(X=x).\]</span></p>
<p>当函数f(x)的输入是从分布P中抽取的随机变量时,f(x)的期望值为:</p>
<p><span class="math display">\[E_{x\sim P}[f(x)]=\sum_{x}f(x)P(x).\]</span></p>
<p>方差：</p>
<p><span class="math display">\[\operatorname{Var}[X]=E\left[(X-E[X])^{2}\right]=E[X^{2}]-E[X]^{2}.\]</span></p>
<h4 id="分布"><a target="_blank" rel="noopener" href="https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/distributions.html">分布</a></h4>
<h3 id="信息论"><a target="_blank" rel="noopener" href="https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html">信息论</a></h3>
<h4 id="熵">熵</h4>
<p>在不同的领域中，熵被表示为混乱程度，不确定性，惊奇程度，不可预测性，信息量等，但在机器学习中我们只需要对信息论中的熵有基本了解<br />
信息论中熵的概念首次被香农提出，目的是寻找一种高效/无损地编码信息的方法，即<strong>无损编码事件信息的最小平均编码长度</strong> 即信息熵H(X) = -Σ p(x) log2 p(x) (p为概率)</p>
<p>接下来说明这个公式，假设我们用二进制的哈夫曼编码，一个信息出现概率是1/2,即其他所有情况加起来也是1/2，那么我们会发现其编码长度必然是-log(1/2),也就是1，恰好和我们的香农熵定义一致了，这是为什么呢？<br />
严谨的数学证明超出了cs专业范围，这里只说一下直观理解，熵有两个性质：</p>
<ul>
<li>概率越小信息量越大(如果一个小概率事件发生了，就排除了非常多其他可能性)</li>
<li>假设两个随机变量x,y相互独立，那么分别观测两个变量得到的信息量应该和同时观测两个变量的信息量是相同的，<code>h(x+y)=h(x)+h(y)</code></li>
</ul>
<p>如此一来对数函数的负数完美符合条件，基数则无所谓，直观地理解，基数对应用几进制编码，而要最短化编码，越小概率就应该用更长的位数，把短位数腾出来给大概率事件用，当然实际中编码的位数是离散的，而且相比对数取负只能多不能少，因此香农熵是一个理论最优值，熵编码就指无损情况下的编码方式，最常用的就是哈夫曼编码，所有熵编码方式的编码长度大于等于香农熵</p>
<p>现实中常用二进制编码信息，例如对8种不同的信息，最直观的编码是三位二进制，每三位表示一个独特信息。<br />
我们可以用概率表示每种信息出现的可能，例如8种信息，每个都等可能出现，那么以概率为权的哈夫曼编码就会用所有的3位二进制编码这8种信息，熵就是3，而其他情况熵可以当做哈夫曼树的总编码长度算<br />
那么如何理解熵能反映混乱度呢？如果熵比较大(即平均编码长度较长)，意味着这一信息有较多的可能状态，相应的每个状态的可能性比较低；因此每当来了一个新的信息，我们很难对其作出准确预测，即有着比较大的混乱程度/不确定性/不可预测性</p>
<p><strong>交叉熵</strong>：<br />
交叉熵用于评估估计概率得到的熵与真实熵的差距，交叉的含义很直观，就是使用P计算期望，使用Q计算编码长度<br />
为什么这么选而不是反过来呢？这取决于我们的目的，一般来说，我们希望估计的编码长度和理论最优的熵差距较小，要比对取优的主要是模型的编码长度即logQ，可以这么理解，熵公式中的对数函数视为视为对一个特定概率事件的编码长度，由于现实的概率分布实际上是确定的，那么需要评估的也就是编码方式的效率<br />
由于熵是给定概率分布下的最优值，<strong>交叉熵只可能大于等于熵</strong>，两者差越小或者说交叉熵越小表示模型估计越准<br />
例如在最极端的one-hot编码中，交叉熵等价于<strong>对应正确解标签的输出的自然对数</strong></p>
<h3 id="线性代数">线性代数</h3>
<h4 id="范数">范数</h4>
<p>范数是具有“长度”概念的函数，用于衡量一个矢量的大小（测量矢量的测度）<br />
由于不是数学系的，这里就极为不严谨地记录一下范数的理解：</p>
<ul>
<li>0范数，向量中非零元素的个数</li>
<li>1范数，为绝对值之和</li>
<li>2范数，就是通常意义上的模</li>
</ul>
<p>正则化的目的可以理解为限制权重向量的复杂度，实际做法为在损失函数中加入与权重向量复杂度有关的惩罚项，而范数在某种意义上可以反映这点，因此可作为选取正则项的依据<br />
顺便一提a star算法也会用类似的测度估计距离</p>
<h2 id="工具">工具</h2>
<h3 id="cuda">cuda</h3>
<p>Compute Unified Device Architecture (CUDA):简单地说，就是允许软件调用gpu来计算的一个接口<br />
CUDA Runtime API vs. CUDA Driver API</p>
<ul>
<li>驱动版本需要≥运行时api版本</li>
<li>driver user-space modules需要和driver kernel modules版本一致</li>
<li>当我们谈论cuda时，往往是说runtime api</li>
</ul>
<p>以下是nvida的介绍原文：</p>
<div class="note default"><p>It is composed of two APIs:</p>
<ul>
<li>A low-level API called the CUDA driver API,</li>
<li>A higher-level API called the CUDA runtime API that is implemented on top of the CUDA driver API.</li>
</ul>
<p>The CUDA runtime eases device code management by providing implicit initialization, context management, and module management. The C host code generated by nvcc is based on the CUDA runtime (see Section 4.2.5), so applications that link to this code must use the CUDA runtime API.</p>
<p>In contrast, the CUDA driver API requires more code, is harder to program and debug, but offers a better level of control and is language-independent since it only deals with cubin objects (see Section 4.2.5). In particular, it is more difficult to configure and launch kernels using the CUDA driver API, since the execution configuration and kernel parameters must be specified with explicit function calls instead of the execution configuration syntax described in Section 4.2.3. Also, device emulation (see Section 4.5.2.9) does not work with the CUDA driver API.</p>
</div>
<p>简单地说,driver更底层，更抽象但性能和自由度更好，runtime则相反</p>
<h3 id="容器">容器</h3>
<p><img src="/assets/ml/Pasted%20image%2020250401210138.png" /><br />
infrastructure(基础设施)<br />
简单地说，虚拟机的隔离级别比容器更高，虚拟机会模拟出一个系统及其系统api，而docker依旧调用宿主机的api，因此docker更为轻量级<br />
docker是处理复杂环境问题的良策，比虚拟机更为轻量<br />
其他常用的容器:Slurm and Kubernetes</p>
<p><a target="_blank" rel="noopener" href="https://hub.docker.com/r/pytorch/pytorch/tags">Docker Hub repository of PyTorch</a></p>
<h3 id="python">python</h3>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  </span><br><span class="line"><span class="comment"># 生成数据  x = np.arange(0, 6, 0.1) # 以 0.1 为单位,生成 0 到 6 的数据 </span></span><br><span class="line"></span><br><span class="line">y1 = np.sin(x)</span><br><span class="line">y2 = np.cos(x)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制图形  </span></span><br><span class="line">plt.plot(x, y1, label=<span class="string">&quot;sin&quot;</span>) plt.plot(x, y2, linestyle = <span class="string">&quot;--&quot;</span>, label=<span class="string">&quot;cos&quot;</span>) </span><br><span class="line"><span class="comment"># 用虚线绘制 </span></span><br><span class="line">plt.xlabel(<span class="string">&quot;x&quot;</span>) </span><br><span class="line"><span class="comment"># x 轴标签 </span></span><br><span class="line">plt.ylabel(<span class="string">&quot;y&quot;</span>) </span><br><span class="line"><span class="comment"># y 轴标签 </span></span><br><span class="line">plt.title(<span class="string">&#x27;sin &amp; cos&#x27;</span>) </span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="理论">理论</h2>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/tensors.html">官方文档</a><br />
<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/index.html">Official Pytorch Tutorials</a><br />
<a target="_blank" rel="noopener" href="https://github.com/wkentaro/pytorch-for-numpy-users">pytorch-for-numpy-users</a></p>
<h3 id="张量">张量</h3>
<p><strong>张量tensor</strong>:用于表示n维数据的一种概念，例如一维张量是向量，二维是矩阵……</p>
<p><code>dim in PyTorch == axis in NumPy</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&quot;</span>, torch.cuda.is_available())</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        device = torch.device(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;There are <span class="subst">&#123;torch.cuda.device_count()&#125;</span> GPU(s) available.&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Device name:&quot;</span>, torch.cuda.get_device_name(<span class="number">0</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;No GPU available, using the CPU instead.&quot;</span>)</span><br><span class="line">        device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">test()</span><br></pre></td></tr></table></figure>
<pre><code> True
There are 1 GPU(s) available.
Device name: NVIDIA GeForce RTX 2070</code></pre>
<p>以下是一些朴素的张量操作:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>, -<span class="number">1</span>], [-<span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">x = torch.from_numpy(np.array([[<span class="number">1</span>, -<span class="number">1</span>], [-<span class="number">1</span>, <span class="number">1</span>]]))</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[ 1, -1],
        [-1,  1]])
tensor([[ 1, -1],
        [-1,  1]])</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros([<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">x = torch.ones([<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>]) </span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[0., 0.],
        [0., 0.]])
tensor([[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]])</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line"></span><br><span class="line">x = x.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([2, 3])  
torch.Size([3, 2])</code></pre>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">A = torch.arange(<span class="number">20</span>, dtype=torch.float32).reshape(<span class="number">5</span>, <span class="number">4</span>) </span><br><span class="line">B = A.clone() <span class="comment"># 通过分配新内存,将A的一个副本分配给B  </span></span><br><span class="line"></span><br><span class="line">A*B<span class="comment">#按元素乘法的Hadamard积，即矩阵相同位置的元素相乘，得到结果矩阵该位置的元素，数学符号⊙</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">结果如下：</span></span><br><span class="line"><span class="string">tensor([[ 0., 1., 4., 9.], </span></span><br><span class="line"><span class="string">        [ 16., 25., 36., 49.], </span></span><br><span class="line"><span class="string">        [ 64., 81., 100., 121.],</span></span><br><span class="line"><span class="string">        [144., 169., 196., 225.],</span></span><br><span class="line"><span class="string">        [256., 289., 324., 361.]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">A_sum_axis0 = A.<span class="built_in">sum</span>(axis=<span class="number">0</span>)  <span class="comment">#沿0轴对A求和</span></span><br><span class="line">A_sum_axis0, A_sum_axis0.shape</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">numpy的对某一轴求和，就是把输入轴所有值加起来，投影到剩下的轴上，这个轴不是消失了，而是现在被投影到一起，例如二维矩阵里，沿0轴求和，就是沿着所有行求和，所有行的对应元素加起来，投影到一行，结果行数为1可以降低一个维度为向量，作为sum的输出(更高维情况下除了输入轴都不变，最后维度-1)</span></span><br><span class="line"><span class="string">在这个例子中，结果是：</span></span><br><span class="line"><span class="string">(tensor([40., 45., 50., 55.]), torch.Size([4]))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">如果让sum的参数keepdims=True，就不会执行维度减一的操作，结果会是个行数1，列数4的矩阵</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>沿某个轴计算A元素的累积总和,比如<code>axis=0(按行计算)</code>,可以调用<code>cumsum</code>函数。此函数不会沿任何轴降低输入张量的维度。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">A.cumsum(axis=<span class="number">0</span>)  </span><br><span class="line">tensor([[ <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],  </span><br><span class="line">        [ <span class="number">4.</span>, <span class="number">6.</span>, <span class="number">8.</span>, <span class="number">10.</span>],  </span><br><span class="line">        [<span class="number">12.</span>, <span class="number">15.</span>, <span class="number">18.</span>, <span class="number">21.</span>],  </span><br><span class="line">        [<span class="number">24.</span>, <span class="number">28.</span>, <span class="number">32.</span>, <span class="number">36.</span>],  </span><br><span class="line">        [<span class="number">40.</span>, <span class="number">45.</span>, <span class="number">50.</span>, <span class="number">55.</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="神经网络的定义">神经网络的定义</h3>
<p>对每一层神经网络，输入的x乘以权重向量w(eight)，加上一个标量b(ias)后就是输出y<br />
例如训练一个将32维向量转化为64维向量输出的模型，权值矩阵规模是64×32,输入向量是32×1，输出是64×1<br />
算出线性的权值和之后，增加一层激活函数<strong>Activation Function</strong>,激活函数常是非线性的，用于增强网络学习能力，如果没有激活函数，网络就是单纯的有很多层数的线性回归<br />
最基础的神经网络是<strong>全连接前馈神经网络(fully-connected feed-forward network)</strong>:前馈表示从前到后训练，全连接表示相邻的两层中，所有的神经元都是相连的;网络第一层称为输入层（input layer），最后一层称为输出层（output layer），中间的层数被称为隐藏层(hidden layer)</p>
<div class="note info"><p><em>前向和反向，forward &amp;&amp; backward:理解这两个词应该看英文，forward这个前指的是时间上从前往后，也就是训练时的正常时间顺序，backward与其相反，就是从结果推开头</em></p>
</div>
<p>深度学习：这个定义非常简单粗暴，意思是隐藏层很多，一般可能有三位数起步，并非越多层就越好，这需要合适的数据集，合适的提取特征方式，即<strong>特征工程feature engineering</strong>，才能定义一个较好的网络</p>
<p>常见激活函数：</p>
<ul>
<li>Sigmoid函数也叫Logistic函数，用于隐层神经元输出，取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好，图像类似一个S形曲线:
<ul>
<li> $ f(x)=\frac{1}{1+e^{-x}} $  </li>
</ul></li>
<li>ReLU函数又称为修正线性单元（Rectified Linear Unit），是一种分段线性函数，弥补了sigmoid函数的梯度消失问题(即该函数两端非常平滑，导数趋近0，遇到数值偏两端的数据，loss很难传播):
<ul>
<li> $f(x)={\left\{\begin{array}{l l}{x}&{,x\gt =0}\\ {0}&{,x\lt 0}\end{array}\right.} $ </li>
</ul></li>
</ul>
<p><img src="/assets/ml/Pasted%20image%2020250402152918.png" /> <img src="/assets/ml/Pasted%20image%2020250402152932.png" /></p>
<p><strong>损失函数loss function</strong>：评估训练成果的一个标准，越小越好<br />
loss = criterion(model_output, expected_value) #nn:neural network</p>
<ul>
<li><code>criterion = nn.MSELoss()</code>:Mean Squared Error</li>
<li><code>criterion = nn.CrossEntropyLoss()</code>:Cross Entropy 交叉熵</li>
</ul>
<h3 id="常见问题">常见问题</h3>
<ol type="1">
<li>模型偏差：模型在训练资料上的损失函数很大时，可能是因为在这个问题中选择的模型太过简单，以至于无论用这个给模型选择什么样的参数θ，损失函数f(θ)都不会变得很小</li>
<li>优化问题，梯度下降看上去很美好，但常常会卡在一个局部最优（local minima）点，这个局部最优可能和全局最优（global minima）差得很远，因此需要选取更好的优化算法如 Adam，RMSProp，AdaGrad 等</li>
<li>过拟合，训练集Loss很小，测试集却很大，需要注意的是首先得满足前一个条件，不然也可能是1.2.问题
<ol type="1">
<li>最有效的一种解决方案是增加训练资料，但很多时候是无法做到的</li>
<li>第二种方法就是数据增广（Data Augmentation），常用于图像处理。既然不能增加数据，那就更好地利用现有数据；例如：对图像左右镜像，改变比例等等，需要注意不能过度改变数据特征，例如上下颠倒图片</li>
<li>增加对模型的限制，常见如早停止，正则化，丢弃部分不合理数据等等</li>
</ol></li>
</ol>
<h3 id="优化算法-optimizer">优化算法 optimizer</h3>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html">torch.optim</a><br />
e.g.</p>
<ol type="1">
<li>optimizer.zero_grad()：重设梯度(即训练完一段后梯度置0,截断反向传播再继续训练)</li>
<li>Call loss.backward() 反向传播以减少损失</li>
<li>Call optimizer.step() 调整模型参数</li>
</ol>
<h4 id="归一化-normalization">归一化 Normalization</h4>
<p>训练中的数据很多情况下大小完全不统一，基于直觉的想法是：同一维度的数据我们只关心其相对大小关系，不同维度的数据我们认为它们的地位平等，尺度应到一致，所以需要归一化<br />
简单地说就是把一个维度的数据大小都调整到<code>[0-1]</code>这个区间，例如softmax函数就用于将一个向量转化成概率分布(令其总和为1)</p>
<div class="note info"><p>归一化<strong>normalization</strong>容易和正则化<strong>regularization</strong>搞混，来看看词典解释：</p>
<ul>
<li>normalization: to start to consider something as normal, or to make something start to be considered as normal</li>
<li>regularization: the act of changing a situation or system so that it follows laws or rules, or is based on reason</li>
</ul>
<p>也就是，归一化偏向于“正常”，正则化偏向于“规则”，差别非常微妙，但硬要说的话，0-1的数据看起来是比其他的更“正常”一点</p>
<p>神经网络中进行的处理有推理(inference)和学习两个阶段。神经网络中未被正规 化(归一化)的输出结果有时被称为“得分”。也就是说,当神经网络的推理只需要给出一个答案的情况下,因为此时只对得分最大值感兴趣,所以不需要 Softmax 层。 不过,神经网络的学习阶段则需要 Softmax 层</p>
</div>
<h4 id="梯度下降">梯度下降</h4>
<p><strong>梯度下降法</strong>：<br />
有点类似于牛顿法(牛顿法理论是二阶收敛，梯度则为一阶，牛顿法速度更快计算量更大)，所谓的梯度就是一个多元函数中，对一个点求各个元的偏导数取值组成的一个表示方向的向量(即偏导数乘对应元的单位向量)<br />
这个梯度一般指向当前点增加最快的方向，把他乘以-1就会得到下降最快的方向(一般用于最小化损失函数)，梯度只表示方向，因此还需要选择合适的步长α，乘以方向向量后就得到移动的路径，步长太长了会跨过极小值然后来回震荡，太短了效率会很差<br />
梯度下降算法需要设置一个<strong>学习率(learning rate)</strong>，每次迭代中，未知参数的梯度下降的步长取决于学习率的设置，这种由人为设定的参数被称为<strong>超参（hyperparameters）</strong><br />
如果我们的数据集很大，计算梯度会相当复杂，则可以分为n个batch,每个batch有B个资料，即 <strong>mini-batch 梯度下降（MBGD，Mini-Batch GradientDescent</strong><br />
在numpy里用形如<code>np.random.choice(scope, num)</code>的方法就可以在scope内随机选取num个索引作为mini batch</p>
<h4 id="反向传播">反向传播</h4>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/charlotte77/p/5629865.html">通俗讲解反向传播</a><br />
反向传播是梯度下降在神经网络上的具体实现方式，与前向训练过程相反，用结果的损失值修正训练中的权值，在数学上就是求损失对前一层权重的偏导数<br />
但是损失函数的参数是本层的输出值，因此需要链式法则求偏导，先求出损失函数对输出的偏导，再乘以本层输出对前一层权重的偏导(当然我们知道本层神经元收到上层的输出后还要算个激活函数才能得到最后的输出，因此中间还有一步本层输出对上层输出求偏导，只有上层输出直接和上层权重有关)</p>
<p><img src="/assets/ml/pk7amw95.png" /> 如图，权重结合上层out得到net,net经过激活函数得到本层out，本层out用来计算损失值，因此反向传播会反过来算<br />
对着这些复杂的名称求偏导数看起来有点奇怪，但这和常见的yx没什么不同<br />
得到偏导函数后，接下来正如在梯度下降中学到的，我们希望通过不断调整上层权重来最小化结果层的损失函数，每次用一个“学习速率 <span class="math inline">\(\eta\)</span> ”乘以偏导数来更新权重</p>
<p><img src="/assets/ml/o7aq5jae.png" /></p>
<p>涉及隐藏层时稍微有所不同，在由于隐藏层的下一层会连到不同的结果，也就会产生多个损失值，在out部分需要对不同的损失值求偏导，即总误差对outh1的偏导是E(o1)和E(o2)对outh1偏导的和</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">dataset = MyDataset(file)</span><br><span class="line">tr_set = DataLoader(dataset, <span class="number">16</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">model = MyModel().to(device)</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> tr_set:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        x, y = x.to(device), y.to(device)</span><br><span class="line">        pred = model(x)</span><br><span class="line">        loss = criterion(pred, y)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step() <span class="comment">#更新模型的参数</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<div class="note info"><p><strong>梯度消失</strong>：</p>
<p>如果我们用最常见的sigmoid函数，且在某层上输入值在函数的两端，此时导数非常小，也就是梯度非常小，这层上的反向传播几乎无法更新，不仅如此，更之前的层级也很难继续传播，这就是所谓的梯度消失问题。<br />
除了更新缓慢问题外，这还会导致浅层和深层的更新程度区别巨大，让模型变得“不均匀”，此外，由于sig函数的梯度区间较小，模型深了几乎必然有这种问题</p>
<p>ReLU用于解决梯度消失问题，其梯度要么是0要么是1，只在负端消失，这样有个可能的好处，如果负端的数据其实是噪声或者是一些我们不关注的特征，那么扔掉反而会让模型效果更好<br />
这种激活函数的缺点是，梯度非负数，对于一层所有的w，梯度的符号都是一样的，只能一起增大或者减小，这可能减少模型的准确度<br />
通常，激活函数的输入值有一项偏置项(bias)，假设bias变得太小，以至于输入激活函数的值总是负的，那么反向传播过程经过该处的梯度恒为0,对应的权重和偏置参数此次无法得到更新。如果对于所有的样本输入，该激活函数的输入都是负的，那么该神经元再也无法学习，称为神经元”死亡“问题</p>
<p>LeakyReLU的提出就是为了解决神经元”死亡“问题，其输入小于0的部分，值为负，且有微小的梯度，除了避免死亡还有一个可能的好处是，该微小的梯度可能让训练有一些微小的振动，在特定情况能跳出局部最优解</p>
<p>python实现:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>): </span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x)) <span class="comment">#NumPy的广播特性:如果在标量和 NumPy 数组之间进行运算,则标量会和 NumPy 数组的各个元素进行运算</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">x</span>): </span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>, x)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</div>
<p>参考项目:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers">Huggingface Transformers</a> (transformer models: BERT, GPT, ...)</li>
<li><a target="_blank" rel="noopener" href="https://github.com/pytorch/fairseq">Fairse</a> (sequence modeling for NLP &amp; speech)</li>
<li><a target="_blank" rel="noopener" href="https://github.com/espnet/espnet">ESPnet</a> (speech recognition, translation, synthesis, ...)</li>
</ul>
<h4 id="momentum">Momentum</h4>
<p>梯度下降简单易懂，但当然也存在问题，考虑如下的函数：</p>
<p><span class="math display">\[f(x,y)=\frac{1}{20}x^{2}+y^{2}\]</span></p>
<p>其x轴梯度远小于y轴梯度，函数像一个山岭，但底部是个起伏不大的抛物线，更新路径是一个个之字形，y轴更新地快不断震荡，x轴则慢慢向真正的底部前进<br />
也就是说，对非均向(anisotropic)的函数，梯度下降效率有限</p>
<p>Momentum(动量)是一种改进的优化方式，这里不管其物理含义，具体更新方法如下所示：</p>
<p><span class="math display">\[v\leftarrow\alpha v-\eta{\frac{\partial L}{\partial W}}\]</span> <span class="math display">\[W\leftarrow W+v\]</span></p>
<p>其他变量我们都知道了，这个v对应物理上的速度，表示物体在梯度方向受力，α模拟类似摩擦力导致的减速，一般比1略小，如0.9，也就是说，原来的梯度下降可视为无阻力的运动，动量法让其速度越来越慢，这可以有效减少路径的折线情况</p>
<h4 id="adagrad">AdaGrad</h4>
<p>学习率衰减(learning rate decay),即随着学习的进行,使学习率逐渐减小是一种常见的思路；AdaGrad进一步发展了这个想法,针对不同参数,赋予相互独立的学习率，即Adaptive Grad</p>
<p><span class="math display">\[h\leftarrow h+{\frac{\partial L}{\partial W}}\leftarrow{\frac{\partial L}{\partial W}}\]</span> <span class="math display">\[W\leftarrow W-\eta\frac{1}{\sqrt{h}}\frac{\partial L}{\partial W}\]</span></p>
<p>h保存了以前的所有梯度值的平方和,这样能够按参数的元素进行学习率衰减,使变动大的参数的学习率逐渐减小，直至无限接近0；RMSProp 方法会舍弃一些比较早的梯度避免这个问题<br />
为了避免棘手的除0问题，h的平方根可以加一个微小值如1e-7</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1412.6980">adam</a>是2015年提出的新方法,直观但不准确地说就是融合了 Momentum 和 AdaGrad 的方法</p>
<h3 id="正则化">正则化</h3>
<p>正则化指为解决适定性问题或过拟合而加入额外信息的过程，在机器学习中，常见的就是为损失函数添加一些调整项<br />
根据前文所述，学习的过程就是根据损失函数与权重的关系不断调整权重以最小化损失，而正则化的目的是不要让权重关系太复杂以致于没有普适性。我们将原始的损失函数画一个图，正则项再画一个图，需要找的就是两个函数同样权重基础上的最小和<br />
用比较简单的双权重，均方误差损失函数来说，w1,w2就是xy轴，最后的loss作为z轴，原始损失函数L可能千奇百怪，但最后要找的是其与正则项的最小和.这个值通常在两个函数的交点取，而(二维层面上)L1的图像是菱形，l2是个圆，符合直观地推导，前者交点很容易在坐标轴上取，后者容易在离坐标轴近的地方取，即l1容易让权重稀疏，L2容易让它们的值的绝对值较小且分布均匀<br />
数学上讲，抛开不确定的损失函数，l1正则项的导数是w×正负信号量，迭代时如果w大于0会减少，大于0会增加，最后很容易变成0；而l2的导数是w的一次函数且一次项系数小于1，迭代让w不断减小，这个减小量与w本身有关，因此一般来说不容易减到0</p>
<h3 id="训练技巧">训练技巧</h3>
<h4 id="初始权值">初始权值</h4>
<p>什么是最好的初始权重？这个问题很难回答，不如反过来举一些反例<br />
相同的权重肯定是最坏的选择，由于随机梯度下降法对相同或者相似的权值会有非常相似的传导效果，最终模型的权值也会趋同，降低其表达力</p>
<p>一般来说，我们用高斯分布来初始权重，例如常用的Xavier初始值根据上层(以及下层)的节点数量确定初始权重的分布，例如在与前一层有n个节点连接时,初始值使用标准差为 <span class="math inline">\(\frac{1}{\sqrt{n}}\)</span> 的分布<br />
Xavier初始值是以激活函数是线性函数为前提而推导出来的。因为sigmoid函数和tanh函数左右对称,且中央附近可以视作线性函数,所以适合使用Xavier初始值。但当激活函数使用ReLU时,一般推荐使用ReLU专用的初始值,也就是Kaiming He等人推荐的初始值,也称为“He初始值”<br />
当前一层的节点数为n时,He初始值使用标准差为 <span class="math inline">\(\sqrt{\frac{2}{n}}\)</span> 的高斯分布。当Xavier初始值是 <span class="math inline">\({\sqrt{\frac{1}{n}}}\)</span> 时,(直观上)可以解释为,因为ReLU的负值区域的值为0,为了使它更有广度,所以需要2倍的系数</p>
<h4 id="batch-normalization">Batch Normalization</h4>
<p>Batch Norm的思路是调整各层的激活值分布使其拥有适当的广度,具体而言,就是进行使数据分布的均值为0、方差为1的正规化</p>
<p><span class="math display">\[\mu_{B}\,\leftarrow\,\frac{1}{m}\sum_{i=1}^{m}x_{i}\]</span> <span class="math display">\[\sigma_{B}^{2}\iff\sum_{i=1}^{m}(x_{i}-\mu_{B})^{2}\]</span> <span class="math display">\[\hat{x}_{i}\enspace\leftarrow\ \frac{x_{i}\leftarrow\mu_{B}}{\sqrt{\sigma_{B}^{2}\,+\,\varepsilon}}\,\]</span></p>
<p>ε 是一个微小值，用来防止除0</p>
<p>Batch Norm 层会对正规化后的数据进行缩放和平移的变换:</p>
<p><span class="math display">\[y i\longleftarrow\gamma\hat{x}_{i}+\beta\]</span></p>
<p>γ、β是参数，初始为1、0，随后根据学习来调整</p>
<h4 id="抑制过拟合">抑制过拟合</h4>
<p><strong>权值衰减</strong>：为损失函数加上权重的平方范数(L2范数)，即让正则项为 <span class="math inline">\(\textstyle{ {\frac{1}{2}}\lambda W^{2} }\)</span> ，其中λ是控制正则化强度的超参数，其梯度也会加上一个λW</p>
<p><strong>dropout</strong>：在学习的过程中随机删除神经元，停止向前传递信号</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Dropout</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dropout_ratio=<span class="number">0.5</span></span>):</span><br><span class="line">        self.dropout_ratio = dropout_ratio</span><br><span class="line">        self.mask = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, train_flg=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="keyword">if</span> train_flg:</span><br><span class="line">            self.mask = np.random.rand(*x.shape) &gt; self.dropout_ratio<span class="comment">#*x.shape 将 x 的形状解包</span></span><br><span class="line">            <span class="keyword">return</span> x * self.mask</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> x * (<span class="number">1.0</span> - self.dropout_ratio)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dout</span>):</span><br><span class="line">        <span class="keyword">return</span> dout * self.mask</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>集成学习</strong>：让多个模型单独进行学习,推理时再取多个模型的输出的平均值。Dropout可以理解为：通过在学习过程中随机删除神经元,从而每一次都让不同的模型进行学习；推理时,通过对神经元的输出乘以删除比例,可以取得模型的平均值。</p>
<h4 id="超参数">超参数</h4>
<p>超参数(hyper-parameter)：如各层的神经元数量、batch 大小、参 数更新时的学习率或权值衰减等<br />
需要注意的是，不能使用测试数据评估超参数的性能，否则会让超参数的值会对测试数据发生过拟合，一般用验证数据来评估性能 <a target="_blank" rel="noopener" href="https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf">有报告显示</a>，在进行神经网络的超参数的最优化时,与网格搜索等有规律的搜索相比,随机采样的搜索方式效果更好。这是因为在 多个超参数中,各个超参数对最终的识别精度的影响程度不同 大致的步骤是：</p>
<ol type="1">
<li>设定超参数的范围</li>
<li>从设定的超参数范围中随机采样</li>
<li>使用1.中采样到的超参数的值进行学习,通过验证数据评估识别精度(但是要将epoch设置得很小)</li>
<li>重复1. 2. 不断缩小参数到一个合理的值</li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">weight_decay = <span class="number">10</span> ** np.random.uniform(-<span class="number">8</span>, -<span class="number">4</span>)<span class="comment">#uniform生成指定范围内的随机数,默认0维度，可以指定维度</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="神经网络模型">神经网络模型</h1>
<h2 id="python-相关">python 相关</h2>
<ul>
<li><code>DataLoader(train_date,shuffle=True)</code>中shuffle表示打乱数据集，符合直觉的想法是：这对避免过拟合有帮助<br />
</li>
<li>epoch是一个单位。一个epoch表示学习中所有训练数据均被使用过一次时的更新次数。比如,对于 10000 笔训练数据,用大小为100笔数据的mini-batch进行学习时,重复随机梯度下降法100次,所有的训练数据就都被“看过”了。此时,100次就是一个epoch</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">My_model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self，inputdim</span>):</span><br><span class="line">        <span class="built_in">super</span>(My_model,self).__init__()</span><br><span class="line">        self.layers=nn.sequential(<span class="comment">#将多个层按顺序组合在一起</span></span><br><span class="line">            nn.Linear(input_dim,<span class="number">64</span>), </span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>，<span class="number">32</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">32</span>，<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.layers(x)</span><br><span class="line">        x = x.sgueeze(<span class="number">1</span>)<span class="comment">#(B,l) -&gt; B</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment">#Training loop</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3000</span>):</span><br><span class="line">    model.train() <span class="comment"># Set your model to train mode.</span></span><br><span class="line">    <span class="comment"># tgdm is a package to visualize your training progress</span></span><br><span class="line">    train_pbar = tqdm(train_loader, position=<span class="number">0</span>, leave=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> train_pbar:</span><br><span class="line">        x, y = x.to(<span class="string">&#x27;cuda&#x27;</span>), y.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">        pred = model(x)</span><br><span class="line">        loss = criterion(pred, y)</span><br><span class="line">        loss.backward()<span class="comment"># Compute gradient(backpropagation).</span></span><br><span class="line">        optimizer.step()<span class="comment"># Update parameters.</span></span><br><span class="line">        optimizer.zero_grad() <span class="comment"># Set gradient to zero.</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/">PyTorch Documentation</a></p>
<p>广播机制(broadcasting mechanism):</p>
<ol type="1">
<li>通过适当复制元素来扩展一个或两个数组,以便操作的不同张量具有相同的形状;<br />
</li>
<li>对生成的数组执行按元素操作</li>
</ol>
<p>例如，a和b分别是3 × 1和1 × 2矩阵，广播会成为一个更大的3 × 2矩阵:矩阵a将复制列,矩阵b将复制行,然后再按元素相加 广播机制有一些实用的技巧:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">A = torch.arange(<span class="number">20</span>, dtype=torch.float32).reshape(<span class="number">5</span>, <span class="number">4</span>) </span><br><span class="line">sum_A = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">A / sum_A <span class="comment">#sum_A没有降维，仍然是个矩阵，可以通过广播将A除以sum_A。</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">结果如下：</span></span><br><span class="line"><span class="string">tensor([[0.0000, 0.1667, 0.3333, 0.5000],  </span></span><br><span class="line"><span class="string">        [0.1818, 0.2273, 0.2727, 0.3182],</span></span><br><span class="line"><span class="string">        [0.2105, 0.2368, 0.2632, 0.2895],  </span></span><br><span class="line"><span class="string">        [0.2222, 0.2407, 0.2593, 0.2778],  </span></span><br><span class="line"><span class="string">        [0.2286, 0.2429, 0.2571, 0.2714]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">这是一种很方便的归一化操作</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h3 id="基础训练方法示例">基础训练方法示例</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_array</span>(<span class="params">data_arrays, batch_size, is_train=<span class="literal">True</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;构造一个PyTorch数据迭代器&quot;&quot;&quot;</span></span><br><span class="line">    dataset = data.TensorDataset(*data_arrays)</span><br><span class="line">    <span class="keyword">return</span> data.DataLoader(dataset, batch_size, shuffle=is_train)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">data_iter = load_array((features, labels), batch_size)</span><br></pre></td></tr></table></figure>
<p>使用<code>iter</code>构造Python迭代器，并使用<code>next</code>从迭代器中获取第一项</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">next</span>(<span class="built_in">iter</span>(data_iter))</span><br></pre></td></tr></table></figure>
<pre><code>[tensor([[-0.0714, -1.8597],
         [-0.4744,  0.4050],
         [ 0.2402,  0.5660],
         [ 1.6367, -0.9899],
         [ 0.6723,  0.1904],
         [ 0.5322, -0.4337],
         [-0.5749,  0.6719],
         [-0.0317,  1.3456],
         [ 1.0865, -1.3968],
         [-0.0130, -0.9245]]),
 tensor([[10.3747],
         [ 1.8749],
         [ 2.7762],
         [10.8325],
         [ 4.9005],
         [ 6.7224],
         [ 0.7743],
         [-0.4169],
         [11.1058],
         [ 7.3157]])]</code></pre>
<p><code>Sequential</code>类将多个层串联在一起,并自动让其前向传播<br />
在PyTorch中，全连接层在<code>Linear</code>类中定义。<br />
值得注意的是，我们将两个参数传递到<code>nn.Linear</code>中，第一个指定输入特征形状，即2，第二个指定输出特征形状，输出特征形状为单个标量，因此为1。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nn是神经网络的缩写</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>可以直接访问参数以设定它们的初始值，如通过<code>net[0]</code>选择网络中的第一个图层，然后使用<code>weight.data</code>和<code>bias.data</code>方法访问参数。<br />
我们还可以使用替换方法<code>normal_</code>和<code>fill_</code>来重写参数值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">0</span>].weight.data.normal_(<span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">net[<span class="number">0</span>].bias.data.fill_(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>损失函数与优化算法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)<span class="comment">#使用小批量随机梯度下降，第一个参数是优化对象，lr则是学习率</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        l = loss(net(X) ,y)</span><br><span class="line">        trainer.zero_grad()</span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step()</span><br><span class="line">    l = loss(net(features), labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l:f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>epoch 1, loss 0.000247
epoch 2, loss 0.000110
epoch 3, loss 0.000110</code></pre>
<p>比较生成数据集的真实参数和通过有限数据训练获得的模型参数<br />
要访问参数，我们首先从<code>net</code>访问所需的层，然后读取该层的权重和偏置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w = net[<span class="number">0</span>].weight.data</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w的估计误差：&#x27;</span>, true_w - w.reshape(true_w.shape))</span><br><span class="line">b = net[<span class="number">0</span>].bias.data</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b的估计误差：&#x27;</span>, true_b - b)</span><br></pre></td></tr></table></figure>
<pre><code>w的估计误差： tensor([0.0005, 0.0006])
b的估计误差： tensor([0.0006])</code></pre>
<h2 id="线性模型">线性模型</h2>
<p>回归问题常常用若干输入产生一个连续值作为输出，线性回归（Linear Regression）和逻辑回归（Logistics Regression）是常见的线性模型</p>
<h3 id="线性回归">线性回归</h3>
<p>线性回归，即y = wx + b ,是最简单的回归模型，但纯一次项的拟合能力较为受限，这种情况下就需要多项式回归<br />
我们将w视为权值向量，x视为从一次x'到n次x'组成的向量，那么多项式模型依旧可以用原先的线性公式表示<br />
增加多项式的次数可以更好拟合训练集，但对测试集的效果就未必了，很容易出现过拟合问题,如果出现，依旧需要正则化，增加数据数量或者维度等优化 线性模型优化是个纯粹的数学问题，其解析解在线代课上就会讲到，即：</p>
<p><span class="math display">\[\mathbf{w}^{*}=(\mathbf{X}^{\mathsf{T}}\mathbf{X})^{-1}\mathbf{X}^{\mathsf{T}}\mathbf{y}.\]</span></p>
<h4 id="python实现">python实现</h4>
<p>求梯度更像一个数学问题，这里就用pytorch的自动求导功能，实际上也可以自己通过计算图实现<br />
简单生成一个有噪声项 <span class="math inline">\(\epsilon\)</span> 的数据集，噪声项有标准差0.01,均值为0的正态分布生成</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w, b, num_examples</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成y=Xw+b+噪声&quot;&quot;&quot;</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, <span class="built_in">len</span>(w)))根据正态分布随机计算初始权值向量</span><br><span class="line">    y = torch.matmul(X, w) + b</span><br><span class="line">    y += torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape)</span><br><span class="line">    <span class="keyword">return</span> X, y.reshape(-<span class="number">1</span>,<span class="number">1</span>)<span class="comment">#-1表示不固定形状，根据总元素数和其他维度数量计算该维度</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])<span class="comment">#w形状与features单行的形状相同</span></span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = synthetic_data(true_w, true_b, <span class="number">1000</span>)<span class="comment">#labels被reshape为单列的向量</span></span><br><span class="line">labels.shape</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([1000])</code></pre>
<p>通过生成第二个特征<code>features[:, 1]</code>和<code>labels</code>的散点图， 可以直观观察到两者之间的线性关系。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d2l.set_figsize()<span class="comment">#控制图像大小</span></span><br><span class="line">d2l.plt.scatter(features[:, <span class="number">1</span>].detach().numpy(), labels.detach().numpy(), <span class="number">1</span>);<span class="comment">#detach() 用于从计算图中分离张量</span></span><br></pre></td></tr></table></figure>
<figure>
<img src="/assets/ml/output_8_0.svg" alt="" /><figcaption>svg</figcaption>
</figure>
<p>为了提高效率，设置一个划分小批量的工具函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    <span class="comment"># 这些样本是随机读取的，没有特定的顺序</span></span><br><span class="line">    random.shuffle(indices)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        batch_indices = torch.tensor(</span><br><span class="line">            indices[i: <span class="built_in">min</span>(i + batch_size, num_examples)])</span><br><span class="line">        <span class="keyword">yield</span> features[batch_indices], labels[batch_indices]</span><br></pre></td></tr></table></figure>
<p>定义模型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X, w, b</span>): </span><br><span class="line">    <span class="string">&quot;&quot;&quot;线性回归模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X, w) + b</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>): </span><br><span class="line">    <span class="string">&quot;&quot;&quot;均方损失&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.reshape(y_hat.shape)) ** <span class="number">2</span> / <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;小批量随机梯度下降&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param -= lr * param.grad / batch_size</span><br><span class="line">            param.grad.zero_()</span><br><span class="line"></span><br><span class="line">w = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(<span class="number">2</span>,<span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        l = loss(net(X, w, b), y)  <span class="comment"># X和y的小批量损失</span></span><br><span class="line">        <span class="comment"># 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起， 并以此计算关于[w,b]的梯度</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()</span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用参数的梯度更新参数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        train_l = loss(net(features, w, b), labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>epoch 1, loss 0.030660
epoch 2, loss 0.000105
epoch 3, loss 0.000046</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;w的估计误差: <span class="subst">&#123;true_w - w.reshape(true_w.shape)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;b的估计误差: <span class="subst">&#123;true_b - b&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>w的估计误差: tensor([ 0.0001, -0.0006], grad_fn=&lt;SubBackward0&gt;)
b的估计误差: tensor([0.0004], grad_fn=&lt;RsubBackward1&gt;)</code></pre>
<h3 id="分类">分类</h3>
<p>分类问题最简单的解决方法莫过于对回归产生的结果进行筛选，一个区间对应一个类别，但这么做会很难处理区间的划分情况，因此需要其他的处理方法<br />
分类问题的损失函数与回归不同，可以单纯用分类错误率计算，常用模型有感知机、支持向量机等</p>
<h4 id="感知机">感知机</h4>
<p>感知机接收多个输入信号,根据权重输出一个信号，例如0和1<br />
如果我们用最简单的感知机(依旧输出一个连续值，通过激活函数产生分类)，那么分类任务的重点就是找的一个合适的门槛值threshold<br />
需要注意的是，感知机本质上是个线性的界限，通过权重向量和偏置值划分不同的输入，设想这样的情况：</p>
<ol type="1">
<li>有多种输入需要分成两类</li>
<li>其中一类有两个输入连成直线L1,另一类中有两个输入可以连成直线L2</li>
<li>如果L1和L2相交，那么我们不可能在中间画一条线把两个直线分开(证明就不管了)</li>
</ol>
<p>事实上，例如异或门就无法通过感知机实现，因为我们要分开(1,0)(0,1)以及(0,0)(1,1)，这两类的连线相交，准确地说，这是说单层感知机，因为曲线就可以划分这两类，也就是“<strong>单层感知机无法分离非线性空间</strong>”<br />
例如，对异或门这个问题。加一层神经就能将分界线拓展为抛物线，也就是次数+1，这样就能进行非线性划分(多层感知机) 此外，也可以通过加一层feature转化层，将原来的x映射为可以被线性划分的x'</p>
<div class="note info"><p>python中定义阶跃函数(输入超过阈值,就切换输出):</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">step_function</span>(<span class="params">x</span>): </span><br><span class="line">    y=x&gt;<span class="number">0</span> <span class="keyword">return</span> <span class="comment">#产生bool数组</span></span><br><span class="line">    y.astype(np.<span class="built_in">int</span>) <span class="comment"># 将bool数组转换为1与0的int数组</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">step_function</span>(<span class="params">x</span>): </span><br><span class="line">    <span class="keyword">return</span> np.array(x &gt; <span class="number">0</span>, dtype=np.<span class="built_in">int</span>) </span><br><span class="line"></span><br><span class="line">x = np.arange(-<span class="number">5.0</span>, <span class="number">5.0</span>, <span class="number">0.1</span>) </span><br><span class="line">y = step_function(x) </span><br><span class="line">plt.plot(x, y) </span><br><span class="line">plt.ylim(-<span class="number">0.1</span>, <span class="number">1.1</span>) <span class="comment"># 指定 y 轴的范围 </span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</div>
<h4 id="逻辑回归discriminative-model">逻辑回归(discriminative model)</h4>
<p>逻辑回归的目标是预测一个二元变量（例如，0或1、是或否）。通过逻辑函数（sigmoid 函数）将线性组合的输入转换为概率值<br />
相比线性回归，其最大的特点是输出在0-1之间，可以理解为概率，一般用于处理分类（二分）问题</p>
<p>经过简单（并不）的梯度运算，可以得到常用loss函数的梯度为：</p>
<ol type="1">
<li>交叉熵: <span class="math inline">\(\sum_{n}-{\bigg(}{ {\hat{y}^{n}-f_{w,b}(x^{n})} { } } {\bigg)}x_{i}^{n}\)</span></li>
<li>均方根: <span class="math inline">\(2(f_{w,b}(x)-\hat{y})f_{w,b}(x)\left(1-f_{w,b}(x)\right)x_{i}\)</span></li>
</ol>
<p>这样一来就有个问题，均方根的梯度在损失较大和较小时都很小，只有交叉熵符合远更新快近更新慢的条件</p>
<h4 id="基于概率的分类方法generative-model">基于概率的分类方法(generative model)</h4>
<p>如果有两个类c1,c2用于分类，抽到一个样本x,这就像高中数学地抽小球问题，随机抽个样本是某类小球的概率取决于在抽取的黑盒子里不同类别的分布。<br />
此时我们的目的是根据参数预测分类，也就是说不同类别的分布只能猜想，于是假设在最简单的两个参数情况下，概率密度函数满足基于这两个参数的高斯(正态)分布，训练时，我们希望分别通过样本得到两类各自的分布情况，也就是这个高斯分布的均值和协方差矩阵</p>
<div class="note info"><p>极大似然估计可以理解为利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值，例如抽球(放回式)问题中抽一百次，七十次是白球,三十次为黑球，若抽到白球的概率是p，这个结果的概率是p<sup>70(1-p)</sup>30，符合直觉的猜想是p=0.7，这是因为我们下意识用了极大似然估计。<br />
要令出现此情况的概率最大，只需要求导算一次极值就会得到p=0.7，由此产生了一个估计</p>
</div>
<p>使用极大似然估计，类似抽球问题，得到目前结果的概率其实结果所有取样点概率的积，省略怎么计算，最后我们能得到两组(μ, ∑)<br />
此时模型确定了，我们可以得到P(x|Ci), i = 1, 2，这是种先验概率（Prior Probability），通过贝叶斯公式就能算出后验概率（Posterior Probability）：P(Ci|x)</p>
<p><span class="math display">\[ P(A | B) = \frac{P(B | A) \cdot P(A)}{P(B)} \]</span></p>
<p>这么一来，分类就可以根据最大后验概率对应的种类来选<br />
优化：<br />
为了避免过拟合可以统一∑，模型概率变为： <span class="math display">\[\operatorname{L}(u^{1},\vert u^{2},\Sigma)=\prod_{i=1}^{79}f_{\mu^{1},\Sigma}(x^{i})\times\prod_{j=1}^{61}f_{\mu^{2},\Sigma}(x^{79+j})\]</span></p>
<p>协方差矩阵可以直接按样本数量加权和<br />
统一协方差矩阵后，其实如果进行化简会发现此时依旧是一个线性模型，即wx+b形式的模型</p>
<div class="note info"><p>在统计学中，方差是用来度量单个随机变量的离散程度，而协方差则一般用来刻画两个随机变量的相似程度<br />
<img src="https://www.zhihu.com/equation?tex=%5Csigma_x%5E2%3D%5Cfrac%7B1%7D%7Bn-1%7D%5Csum_%7Bi%3D1%7D%5En%5Cleft%28x_i-%5Cbar%7Bx%7D%5Cright%29%5E2&amp;consumer=ZHI_MENG" /> <img src="https://www.zhihu.com/equation?tex=%5Csigma%5Cleft%28x%2Cy%5Cright%29%3D%5Cfrac%7B1%7D%7Bn-1%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Cleft%28x_i-%5Cbar%7Bx%7D%5Cright%29%5Cleft%28y_i-%5Cbar%7By%7D%5Cright%29&amp;consumer=ZHI_MENG" /></p>
</div>
<p>以上两种model，其实逻辑回归的准确率在宝可梦数据集上略好于概率分布，这可能是因为概率模型会预先假设数据符合一种概率分布，因此概率模型相对更适合模型数量少或者有一定noise的情况</p>
<h4 id="softmax">softmax</h4>
<p>前两个模型考虑了二分的情况，那么多个类别呢？这时仍然需要模型输出概率，但是概率总和需要小于等于1，然后选一个最高的作为输出；问题变成了如何得到这样的概率，可以用<strong>softmax</strong><br />
对每个输出值 <span class="math inline">\(o_j\)</span> 预测概率值 <span class="math inline">\(\hat y_j\)</span> 可以这么得到：<br />
<span class="math display">\[ {\hat{y} }_{j}={\frac{\exp(o_{j})}{\sum_{k}\exp(o_{k})} } \]</span><br />
<span class="math inline">\(\hat y_j\)</span> 可以视为对给定任意输入x的每个类的条件概率，即P(y = 某类 | x)<br />
整个数据集的条件概率为：<br />
<span class="math display">\[P({\bf Y}\mid{\bf X})=\prod_{i-1}^{n}P({\bf y}^{(i)}\mid{\bf x}^{(i)}).\]</span><br />
根据最大似然估计,我们最大化P(Y|X),相当于最小化所有子概率的负对数和<br />
<span class="math display">\[ -\log P(\mathbf{Y}\mid\mathbf{X})=\sum_{i=1}^{n}-\log P(\mathbf{y}^{(i)}\mid\mathbf{x}^{(i)})=\sum_{i=1}^{n}l(\mathbf{y}^{(i)},{\hat{\mathbf{y} } }^{(i)}) \]</span><br />
其中的损失函数是交叉熵<br />
<span class="math display">\[l({\bf y},\hat{\bf y})=-\sum_{j=1}^{q}y_{j}\log\hat{y}_{j}.\]</span> 

$$\begin{align}
l({\bf y},\hat{\bf y})=  -\sum_{j=1}^{q}y_{j}\log{\frac{\exp(o_{j})}{\sum_{k=1}^{q}\exp(o_{k})}}   \\ 
=\sum_{j=1}^{q}y_{j}\log\sum_{k=1}^{q}\exp(o_{k})-\sum_{j=1}^{q}y_{j}o_{j} \\
=\log\sum_{k=1}^{q}\exp(o_{k})-\sum_{j=1}^{q}y_{j}o_{j}.

\end{align}$$

$$ \partial_{o_{j}l}({\bf y},\hat{\bf y})=\frac{\exp(o_{j})}{\sum_{k=1}^{q}\exp(o_{k})}-y_{j}=\mathrm{sofmax}({\bf o})_{j}-y_{j} $$

</p>
<p>softmax是一个非线性函数,但softmax回归的输出仍然由输入特征的仿射变换(保持点、直线和面之间相对关系的变换)决定，因此,softmax回归是一个线性模型<br />
看上去很巧，导数就是softmax模型分配的概率与实际发生的情况(由独热标签向量表示)之间的差异 <a href="./#分布">数学原理</a></p>
<h3 id="优化技巧">优化技巧</h3>
<p>训练效果取决于很多因素，常见的排查思路有：</p>
<ul>
<li>Model Bias: 训练数据有一定倾向性，实际的function set过于小以致于没有理想的函数，此时可能需要增加参数或者增加数据量</li>
<li>局部最小值和鞍点: 可以用泰勒公式估算附近的函数值，这两种点的梯度(一阶导)都是0，区别在于二阶导数，如果恒非正/非负，就是极值，否则就是鞍点
<ul>
<li>令附近点与求导的点之间的向量为v,泰勒公式的二阶项可以写成 <span class="math inline">\(v^THv\)</span> 的形式(H是对各个w的二阶导数项组成的句子)，用线代的指示，该式恒非正/非负等价于H的特征值恒非正/非负
<ul>
<li>这样一来，对鞍点，设H的特征向量为u， <span class="math inline">\(u^THu = \lambda||u||^2\)</span> , 沿着为负的特征向量方向就能继续下降</li>
</ul></li>
<li>另一种思路是所谓的动量，动量本质上是之前的梯度的加权和，类比物理上的动量，能一定程度上保持训练整体的倾向,将其与当前梯度相加,能一定程度上帮助跳出局部最低点</li>
</ul></li>
<li>有时静态的更新很难达到最低点(更新快会遇到不同参数收敛速度不同导致的震荡，慢会龟速爬行)，因此需要动态的更新机制: <span class="math inline">\(\theta_{i}^{t+1}\leftarrow\theta_{i}^{t}-\frac{\eta}{\sigma_{i}^{t} }\,g_{i}^{t}\)</span> 其中 <span class="math inline">\(\sigma_{i}^{t}=\sqrt{\frac{1}{t+1} \sum_{i=0}^{t}(g_{i}^{t})^{2} }\)</span> <span class="math inline">\(g_{i}^{t}=\frac{\partial L}{\partial\theta_{i}}|_{\theta=\theta^{t}}\)</span> (adagrad)
<ul>
<li>但依然有问题: 同一个参数在不同的取值范围内收敛速度也不同，因此需要进一步的动态机制(RMS Prop): <span class="math inline">\(\theta_{i}^{t+1}\leftarrow\theta_{i}^{t}-\frac{\eta}{\sigma_{i}^{t}}\,g_{i}^{t}\quad\sigma_{i}^{t}=\sqrt{\alpha\!\left(\sigma_{i}^{t-1}\right)^{2}+(1-\alpha)\!\left(g_i^{t}\right)^{2}}\)</span> 从而让最近的梯度影响更大</li>
<li>著名的adam就是同时用了 <code>momentum</code>和 <code>RMS Prop</code>,动量与 <span class="math inline">\(\sigma\)</span> 的不同是: 动量考虑方向,而后者只考虑大小</li>
</ul></li>
<li>上述方法还是会有震荡,只不过震荡会最后收敛,因此考虑能否动态调整学习率
<ul>
<li>最常见的做法是学习率衰减<code>learning rate decay</code></li>
<li><code>warm up</code>,也就是让学习率先快后慢,很难解释,但先快后慢的warmup在很多知名模型里表现很好</li>
</ul></li>
</ul>
<p>一般来说,adam比SGDM(使用动量的sgd)速度快,但最后收敛的效果差一点,这两者可以说是两个极端,常用的优化策略会在两者间折中或者微调,但可解释性嘛,都很难说 以下简单的记录一些相关方法:</p>
<p>adam部分:</p>
<ul>
<li>SWATS: 朴素的方法,先用adam快速逼近目标,再用sgdm慢慢收敛</li>
<li>AMSGrad: 某些局部的梯度可能会超过几个数量级的高,因但由于adam算的是平方均根,这个局部梯度影响会非常有限,(例如a比b小100倍,平方根只差10倍,100个a的梯度却大于一个b的梯度) 因此可以记住一个历史最大值,从而相对扩大大梯度的影响力, 这样的话连续遇到小梯度学习率不会减少, <span class="math inline">\({ {\theta_{t}=\theta_{t-1}-\frac{p}{\sqrt{\hat{v}_{t}+\varepsilon}}m_{t}\space\space } } { {\hat{v}_{t}=\operatorname*{max}(\hat{v}_{t-1},v_{t})} }\)</span> 但这其实没有解决adagrad学习率不断衰减的问题,只是延缓了</li>
<li>AdaBound: 给learning rate设置上下限(clip函数),简单粗暴,可解释性也很迷</li>
<li>cyclicalLR: 让lr周期性波动,简单的线性波动或者cos函数都可以,lr大是探索性的,小是寻找收敛点,类似的有<code>SGDR</code>,<code>One-cycle LR</code>等</li>
</ul>
<p>RAdam: 用于解决训练初期梯度方差大,先用sgdm积累足够的样本,再转到类似adam的方法,同时让非初期的梯度有更高影响力<br />
假设梯度是取样于一种分布,因此参数只和取样次数t有关,学习率满足以下条件,其中rt是恒增的</p>
<p><span class="math display">\[\begin{array}{c}{ {\rho_{t}=\rho_{\infty}-\frac{2t\beta_{2}^{\ t} }{1-\beta_{2}^{\ 2} } } }\\ { {\rho_{\infty}=\displaystyle\frac{2}{1-\beta_{2}^{\prime} }-1} }\\ r_{t}={\sqrt{\frac{(\rho_{t}-4)(\rho_{t}-2)\rho_{\infty} } {(\rho_{\infty}-4)(\rho_{\infty}-2)\rho_{t} } } } \end{array}\]</span></p>
<p><span class="math display">\[\theta_{t}=\theta_{t-1}-\eta\hat{m}_{t} \space when \space 𝜌𝑡 ≤ 4\]</span></p>
<p><span class="math display">\[\theta_{t}=\theta_{t-1}-{\frac{\eta r_{t} }{ {\sqrt{\hat{v} }_{t}+\varepsilon} } }\,{\hat{m} }_{t} \space when \space 𝜌𝑡 &gt; 4\]</span></p>
<p>这是一个比较保守的策略,防止太过激进的学习</p>
<p>动量相关:</p>
<ul>
<li>Nesterov accelerated gradient (NAG): 与普通动量法区别是,用动量来预测下一个参数位置,通过预测位置的梯度更新参数 <span class="math inline">\(m_{t}={\lambda}m_{t-1}+\eta\nabla L(\theta_{t-1}-\lambda m_{t-1})\)</span></li>
</ul>
<p>其他:</p>
<ul>
<li>Lookahead: 一种很抽象的方法,不管用什么优化方法,每轮中走k步到一个理论上的终点,在起点和终点间找一个点作为实际终点<br />
</li>
<li>Shuffling,Dropout,Gradient noise: 这些都是增加随机性的方法</li>
<li>Warm-up,Curriculum learning(先学容易的数据),Fine-tuning(使用预训练的模型)</li>
</ul>
<p>经验上,cv用sgdm多一点;nlp,gan用adam多一点</p>
<h3 id="normalization">Normalization</h3>
<p>为了处理不同维度上输入规模不一的问题,需要归一化<br />
Feature Normalization: 我们把统一维度的x输入视为正态分布的,令 <span class="math inline">\(\widetilde{x}_{i}^{r}\leftarrow{\frac{x_{i}^{r}-m_{i} }{\sigma_{i}} }\)</span> 其中m为平均数,𝜎是标准差<br />
当然也可以对与w的加权和z向量做归一化: <span class="math inline">\({\tilde{z}}^{i}=\frac{z^{i}-\mu}{\sigma}\)</span><br />
然后令 <span class="math inline">\(z^{i}=\gamma\Theta\widetilde{z}^{i}+\beta\)</span> , 其中γ初始为1,β初始为0,这两个是学习参数,用于在之后调整分布<br />
这样的归一化计算量较大,实际中一般只对batch做归一化</p>
<p><img src="/assets/ml/Pasted%20image%2020250902150544.png" /></p>
<h2 id="卷积神经网络cnn">卷积神经网络CNN</h2>
<p>名词解释:</p>
<ul>
<li>Receptive Field（感受野）: 字面意思,就是神经元的视野,cv中我们希望神经元各自只捕捉一个局部特征,一般感受野会是方形矩阵
<ul>
<li>最常见的rf是覆盖所有维度(channel)的,也就是所有色彩空间,常见的rf是3×3,且会有复数的神经元</li>
<li>共享参数: 即两个神经元用相同的参数,常见的共享方法是: 每个rf有n组神经元,不同的rf相同组序号的神经元共享参数;也可以理解为用不同的filter矩阵(也就是共享参数的神经元)做卷积</li>
</ul></li>
<li>stride(步幅): 感受野之间的步幅,例如一个rf对上的最左上元素是(i,j),下一个对上的左上元素就是(stride+i,j)</li>
<li>padding: 由于内核矩阵以及步幅未必会让最后一步正好够运算,有时需要填充若干行/列,最常用的是直接填0</li>
<li>pooling(池化): 在不改变关键信息的前提下尽可能简化输入规模,例如对规模是m×m的矩阵,对每个n×n的子矩阵取一个最大值,最后得到边长m/n的方阵</li>
<li>flatten: 将最后的结果拉长为一维向量,用于之后的模型学习</li>
</ul>
<p>基于以上限制,cnn的<code>model bias</code>其实相对较大,但在影像辨识中不是坏事</p>
<h3 id="spatial-transformer-layer">spatial transformer layer</h3>
<p>鉴于CNN的特点,它对图片缩放,旋转,镜像后的数据,不会依旧保有识别能力.当然想处理这个问题很简单,把经过变化的图片也塞进训练集就可以了,但是这样会严重影响训练效率,而更好的做法是,增加一个图片变化层,用于转化图片到训练集的对应数据<br />
对图片的每个像素,一个2×2的权重矩阵加一个2维的偏移向量就可以得出其对应像素,而对于非整数的对应坐标,出于可微性考虑,将其与周边点的距离积为权值乘以周边点的值,最后相加,如下所示:<br />
<span class="math display">\[\begin{array}{l}{ {a_{22}^{l}=(1-0.4)\times(1-0.4)\times{}(1-0.4)\times a_{22}^{l-1} } }\\ { {+(1-0.6)\times(1-0.4)\times(1-0.6)\times a_{12}^{l-1} } }\\ { {+(1-0.6)\times(1-0.6)\times(1-0.6)\times a_{23}^{l-1} } } \\ { +(1-\,0.4)\times(1-0.6)\times a_{23}^{l-1} } \end{array}\]</span> 由于这样的取值是可微的,也就可以用梯度下降进行学习,将其嵌入神经网络中则可提高对变化后图片的识别效果</p>
<h2 id="self-attention">self-attention</h2>
<p>sequence labeling: 即为输入序列中的每个元素分配一个标签,例如nlp中标注词性.这种问题的难点在于: 由于输入的向量长度不定,难以确定应该用什么规格的网络,于是需要注意力这种机制来让一个输入向量a能获取一些序列中的上下文信息形成向量b再进入网络<br />
那么怎么让a携带上下文信息呢,常见的思路有做加权和(additive)或者加权积(dot-product),后者更常用,也就是输入向量各种乘以一个(互不相同的)权重,然后乘在一起<br />
s-a层所做的运算如下图:</p>
<p><img src="/assets/ml/Pasted%20image%2020250831171404.png" /></p>
<p>实际上,该层学习的就是三个权值矩阵</p>
<p>相关技术:</p>
<ul>
<li>Multi-head Self-attention: 简单地说就是用两个独立的注意力神经,其产物算加权和作为最终输出</li>
<li>Positional Encoding: 为了弥补注意力没有位置信息的问题,最早的处理是对输入ai加上一个ei偏置,后续发展中有不同的添加位置信息的方法</li>
<li>Truncated Self-attention: 对语音这种规模很大的输入,出于效率考虑,可以减少注意力计算的范围,只看很小的上下文</li>
<li>CNN: cnn可以视为注意力的一种特例</li>
<li>RNN: 相比rnn,注意力在并行性,以及对上下文信息的利用能力上都相对更好</li>
</ul>
<h3 id="变形金刚">变形金刚</h3>
<p>transformer,下文简称tf,是一种seq2seq模型,这种模型有相当广泛的应用,语音辨识/合成,句法分析,目标检测,以及现在热门的对话生成都可以使用</p>
<p><img src="/assets/ml/seq2seq_v9.png" /></p>
<p>tf的架构相当复杂,这里简单地描述一下:其主要用en/decoder组成,encoder中有很多<code>block</code>用于生成中间向量,每层<code>block</code>先做一次注意力(结果向量加上输入向量,这叫做<code>residual connection</code>,随后做一次<code>layer normalization</code>),再用全连接(fc)网络计算,这个fc网络也会用<code>residual</code><br />
<code>decoder</code>除了接受<code>encoder</code>数据外,还要接受一个表示开始产生输出的符号(bos),这个符号可以用one-hot表示,类似rnn,dec不断把自己的输出当做下一轮的输入,如果输出一个终止符来结束,就叫<code>Autoregressive</code>;而<code>Non-autoregressive</code>则一次生成所有输出,一个输入对应一个输出,为此需要一个分类器来产生长或者一个足够长的默认长度,让机器自己选一个槽位输出终止符,这样的好处是并行化且容易控制长度,缺点则是表现差<br />
关于dec的架构,观察其与enc不同,首先是注意力层多了masked前缀,这个掩码就是让所有输入维度的注意力只能注意自己极其以前的输入(由于decoder输出有顺序,这样很符合直觉)<br />
关于两者的交互,看下面的图比较直观,不太准确的说就是同时用双方的数据不断地做masked 注意力(输出会不断作为下一轮输入加进来),这叫做<code>Cross Attention</code><br />
原始论文中dec不断从enc的最后一层拿数据,但也有论文会对应着拿<br />
<img src="/assets/ml/seq2seq_v9_1.png" /></p>
<p>相关技术:</p>
<ul>
<li>Teacher Forcing: 简单地说就是训练时直接将答案作为dec输入</li>
<li>Copy Mechanism: 从输入中复制文字给输出用的能力</li>
<li>Guided Attention: 对一些规则严格的场景,可以直接对训练中的模型加以限制,例如语音合成</li>
<li>Beam Search: 一种常用于序列预测任务的搜索算法,能在一定程度上预测相对更优的序列/路径</li>
<li>exposure bias: dec辨识错误的能力差,导致一步错步步错,解决方法是增加一些Noise</li>
<li>Scheduled Sampling: 在训练过程中,逐步减小使用真实目标序列的概率</li>
</ul>
<h3 id="bert">bert</h3>
<p>bert,目前很火的预训练seq2seq模型,是一种无监督学习模型,也就是没有label数据,其特点是训练中使用掩码数据(Masking Input),也就是遮住输入的部分字词,而加上遮住数据的完整输入就是我们希望bert能输出的结果,而损失函数也可以比较方便地用交叉熵(可以理解为以所有字符为类别的分类问题)<br />
bert是基于tf的,其架构和tf类似,区别就是用掩码机制来无监督学习,由于不需要标注的数据集,bert很容易得到规模非常庞大的数据,因此有着很好的表现 除此以外有一些其他训练方法,如Next Sentence Prediction: 预测两个句子是否相接;Sentence order prediction:判断句子的顺序关系<br />
在以下这些常见的一些基准测试中,bert都有不俗的表现:</p>
<ul>
<li>Corpus of Linguistic Acceptability (CoLA)</li>
<li>Stanford Sentiment Treebank (SST-2)</li>
<li>Microsoft Research Paraphrase Corpus (MRPC)</li>
<li>Quora Question Pairs (QQP)</li>
<li>Semantic Textual Similarity Benchmark (STS-B)</li>
<li>Multi-Genre Natural Language Inference (MNLI)</li>
<li>Question-answering NLI (QNLI)</li>
<li>Recognizing Textual Entailment (RTE)</li>
<li>Winograd NLI (WNLI)</li>
</ul>
<p>尽管 BERT 的预训练是无监督的，但在特定下游任务（如文本分类、语法分析等）中(对这些下游任务来说,可以简单地给bert接上分类或者线性模型)，BERT 可以进行微调,这个过程是监督学习。微调阶段使用标注好的数据集，通过已知的标签来优化模型参数.所以如果想准确一点,可以叫半监督学习(semi)<br />
bert的优异性能常常被归因于注意力对上下文的捕获能力以及大量的训练资料,但神奇的是用于做蛋白质分类效果也很好,英语Bert用在中文上效果也很好,这或许可以理解为这些有规律的编码作为语言其实在词义向量以及结构上有相似之处,根据李老师自己的实验,这种神奇的能力只会出现在足够大的训练集上</p>
<h2 id="rnn">RNN</h2>
<p>相关场景:</p>
<p>slot filling: 类似一个分类问题,将给定输入向量(一句话)中的词语分类到特定的槽位去</p>
<p>rnn用于解决输入向量间有顺序关系的问题,普通的前馈网络所有输入的词语都是地位相同的,因此很难捕捉文字的前后语义关系,于是产生了rnn这种方法,也就是把前面的计算结果作为之后的输入,常见的类型有:</p>
<ul>
<li>简单rnn
<ul>
<li>elman network: 先前的隐藏层计算结果存起来,后面被下一个神经元的隐藏层调用</li>
<li>jordan network: 将前一个神经元的输出存起来,被下一个神经元调用</li>
</ul></li>
<li>其他
<ul>
<li>Bidirectional RNN: 训练正向和反向的两个rnn,最后的输出算加权和</li>
</ul></li>
</ul>
<h3 id="long-short-term-memory-lstm">Long Short-term Memory (LSTM)</h3>
<p>简单地说就是用两个阀门控制是否存入或放出历史信息,一个阀门控制是否遗忘已有的信息,阀门的开闭让网络学习<br />
其训练过程相对来说比较繁琐,还好李老师细心地做了流程图,这里直接贴上来</p>
<div class="pdf-container" data-target="/assets/ml/RNN.pdf" data-height="800px"></div>
<p>LSTM的缺点是过于复杂导致计算成本高,因此有Gated Recurrent Unit (GRU)这样的简化版本(三个gate)</p>
<h3 id="问题">问题</h3>
<ol type="1">
<li><p>RNN会复用之前的模型,例如其权值w,这会导致层数上来后,后面神经的权值会产生幂函数关系,使loss surface非常陡峭<br />
更准确地说,由于幂函数的特性,w小会很容易梯度消失,w大则非常陡峭,LSTM可以一定程度上解决前一个问题,因为它能存储历史信息更长时间<br />
而后一个问题,工程上最实用的方法是clip设置上界</p></li>
<li><p>鉴于rnn的特性,处理不定长的输入(向量)是很方便的,但如何处理不定长的输出呢?<br />
例如语音识别,对若干音频输入,简单的想法是每个音频输出一个字符,结果把每个输出的重复部分拿掉,但如何处理叠词呢?<br />
可以用一个φ符号代表null,也就是分割符,φ间的有意义输出作为识别结果,下一个问题是,音频可能切的很碎,不能保证对应关系具体应该怎么排<br />
为此需要<code>Connectionist Temporal Classification (CTC)</code>简单地说就是穷举可能的排列(实际会用dp优化),选取其中最多的一种(即概率最大的排列/对应关系)</p></li>
</ol>
<p>以上讨论的语音识别其实有一个预设--识别结果的字符数≤音频样本数,对没有这种条件的问题,例如机器翻译该怎么做呢?<br />
由于是循环的,rnn可以不断地产生输出,只需要一个特殊的表示结束的符号就可以,例如<code>===</code></p>
<h2 id="深度学习">深度学习</h2>
<p>Q: 为什么要深度,为什么要用那么多隐藏层而不是一个很宽的单层网络?<br />
A: 深度学习能增加预测函数的弹性,这是因为它可以复杂的不同线性关系去拟合数据,那么为什么要用很深的网络?实际上,相同神经数量且较浅的网络预测效果会不如dl,也就是dl能用相对更少的参数拟合数据,因此更不容易overfitting,有更好的准确率;而dl的这种高效其实类似编程中的依赖关系,例如某个节点的后继节点都可以依赖于前一个节点,而整段程序只需要保留这个被多重依赖的节点的一个副本,节省大量空间,dl中其实也可能存在对某个前继神经的依赖关系,也就是dl是一个有结构上关系的网络</p>
<h3 id="概念">概念</h3>
<p>对于复杂的网络,会使用神经网络块(block)来描述若干个网络层的组合,一般来说,块有自己的参数,前向传播,反向传播函数,这是一个逻辑概念,torch中可以用模块或者seq来实现</p>
<div class="note info"><ul>
<li><code>torch.nn.Module</code>: Base class for all neural network modules.Your models should also subclass this class.即所有模型的基类</li>
<li><code>torch.nn.Sequential</code>: Modules will be added to it in the order they are passed in the constructor. 有顺序的<code>module</code>的容器,与<code>ModuleList</code>的是它提供对内置模块的顺序调用,也就是已经实现了前向传播,因此它很适合用来定义一个block</li>
<li><code>torch.nn.ModuleList</code>:Holds submodules in a list. 模块的list,和普通的list没什么区别,有索引顺序,但并没有逻辑上的顺序</li>
</ul>
</div>
<div class="note info">
</div>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/" rel="tag"><i class="fa fa-tag"></i> 课程笔记</a>
              <a href="/tags/python/" rel="tag"><i class="fa fa-tag"></i> python</a>
              <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"><i class="fa fa-tag"></i> 人工智能</a>
              <a href="/tags/%E5%9B%BD%E7%AB%8B%E5%8F%B0%E6%B9%BE%E5%A4%A7%E5%AD%A6/" rel="tag"><i class="fa fa-tag"></i> 国立台湾大学</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
          </div>

        
  <div class="social-like a2a_kit a2a_kit_size_32 a2a_default_style">
    <a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a>
      <a class="a2a_button_facebook"></a>
      <a class="a2a_button_twitter"></a>
      <a class="a2a_button_wechat"></a>
      <a class="a2a_button_qzone"></a>
  </div>

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/thinklive/16959/" rel="prev" title="基于哈佛cs50的计算机通识笔记">
                  <i class="fa fa-angle-left"></i> 基于哈佛cs50的计算机通识笔记
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/thinklive/37185/" rel="next" title="强化学习笔记 all in one">
                  强化学习笔记 all in one <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2023 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">thinklive</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">595k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">36:05</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 技术支持
  </div><script defer src="/lib/three.js"></script><script defer src="/lib/lines.js"></script><script defer src="/lib/waves.js"></script>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.24/fancybox/fancybox.umd.js" integrity="sha256-oyhjPiYRWGXaAt+ny/mTMWOnN1GBoZDUQnzzgC7FRI4=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>


  <script src=""></script>
  <script src="/%5Bobject%20Object%5D"></script>
  <script src="/%5Bobject%20Object%5D"></script>
  <script src="/%5Bobject%20Object%5D"></script>
  <script src="/%5Bobject%20Object%5D"></script>


<script>
var options = {
  bottom: '64px', // default: '32px'
  right: 'unset', // default: '32px'
  left: '32px', // default: 'unset'
  time: '0.5s', // default: '0.3s'
  mixColor: '#fff', // default: '#fff'
  backgroundColor: '#fff',  // default: '#fff'
  buttonColorDark: '#100f2c',  // default: '#100f2c'
  buttonColorLight: '#fff', // default: '#fff'
  saveInCookies: true, // default: true,
  label: '🌓', // default: ''
  autoMatchOsTheme: true // default: true
}
const darkmode = new Darkmode(options);
darkmode.showWidget();
</script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>


  <script class="next-config" data-name="wavedrom" type="application/json">{"enable":true,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/wavedrom/3.3.0/wavedrom.min.js","integrity":"sha256-IRMDzTC+wK5stMucZ/XSXkeS5VNtxZ+/Bm8Mcqfoxdo="}}</script>
  <script class="next-config" data-name="wavedrom_skin" type="application/json">{"enable":true,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/wavedrom/3.3.0/skins/default.js","integrity":"sha256-fduc/Zszk5ezWws2uInY/ALWVmIrmV6VTgXbsYSReFI="}}</script>
  <script src="/js/third-party/tags/wavedrom.js"></script>

  <script src="/js/third-party/fancybox.js"></script>

  <script src="/js/third-party/pace.js"></script>

  <script src="/js/third-party/addtoany.js"></script>

  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":false,"archive":false,"delay":true,"timeout":3000,"priority":true,"url":"https://thinklive1.github.io/thinklive/48061/"}</script>
  <script src="/js/third-party/quicklink.js"></script>
<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

<script>
var options = {
  bottom: '64px',
  right: '32px',
  left: 'unset',
  time: '0.5s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: true,
  label: '🌓',
  autoMatchOsTheme: true
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
</script>
<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"thinklive1/blog_comments","issue_term":"pathname","theme":"photon-dark"}</script>
<script src="/js/third-party/comments/utterances.js"></script>
<script>
    const snowflakes = ["❄", "❄", "❆", "❅", "✥","❄", "❄", "❆", "❅", "✥","✻"];
    // 创建雪花
    function createSnowflake() {
        const snowflake = document.createElement("span");
        snowflake.classList.add("snowflake");
        const randomIndex = Math.floor(Math.random() * snowflakes.length);
        snowflake.textContent = snowflakes[randomIndex];
        
        // 起始位置
        /* 80%概率 生成在页面两侧 30% 的位置
        const probability = Math.random();
        let startPosition = Math.random() * 100;

        if (probability < 0.8) {
            startPosition = Math.random() < 0.5 ? Math.random() * 30 : (Math.random() * 30) + 70;
        }
        snowflake.style.left = `${startPosition}vw`;
        */
        snowflake.style.left = `${Math.random() * 100}vw`;
        snowflake.style.top = `-30px`;
        // 雪花大小与透明度
        const size = Math.random() * 18 + 10;
        snowflake.style.fontSize = `${size}px`;
        const opacity = Math.random() * 0.6 + (size > 18 ? 0.4 : 0);
        snowflake.style.setProperty("--opacity", opacity);
        // 动画持续时间
        const fallDuration = Math.random() * 10 + 10;
        // 旋转持续时间
        const rotateDuration = Math.random() * 3 + 1;

        snowflake.style.animationDuration = `${fallDuration}s, ${fallDuration}s`; // 向 CSS 添加淡出动画的持续时间
        // 横向幅度
        const translateX = (Math.random() * 500 - 200);
        snowflake.style.setProperty("--translateX", `${translateX}px`);
        // 纵向幅度
        snowflake.style.setProperty("--translateY", `${window.innerHeight}px`);

        document.body.appendChild(snowflake);
        // 移除雪花
        setTimeout(() => {
            snowflake.remove();
        }, fallDuration * 1000);
    }
    
    function snowfallAnimation() {
        // 载入时若边栏是隐藏状态则不加载雪花
        const sidebarnav = document.querySelector('.sidebar');
        const sidebarnavdisplay = window.getComputedStyle(sidebarnav).getPropertyValue('display'); 
        if (sidebarnavdisplay !== 'none') {
            createSnowflake();
        }
        setTimeout(snowfallAnimation, 150); // 生成速度，毫秒
    }
    snowfallAnimation();
function toggleMode() {
    console.log("change color!");
    const root1 = document.documentElement;

    // 检查当前 color-scheme
    const isLightMode = getComputedStyle(root1).getPropertyValue('--content-bg-color').trim() === '#fff';

    if (isLightMode) {
        // 切换到暗模式
        root1.style.setProperty('--content-bg-color', '#000');
        root1.style.setProperty('--text-color', '#fff');
        root1.style.setProperty('--highlight-background', '#444');
        root1.style.setProperty('--highlight-foreground', '#bbb');
        root1.style.setProperty('--btn-default-bg', '#777');
        root1.style.setProperty('--menu-item-bg-color', '#777');
        root1.style.setProperty('--note-warning-bg-color', '#777');
        root1.style.setProperty('--note-bg-color', '#555');
        root1.style.setProperty('--note-info-bg-color', '#777');
        root1.style.setProperty('--table-row-odd-bg-color', '#777');
        root1.style.transition = 'all 0.5s ease';

    }

    else {
        root1.style.setProperty('--content-bg-color', '#fff');
        root1.style.setProperty('--text-color', '#555');
        root1.style.setProperty('--highlight-background', '#eaeef3');
        root1.style.setProperty('--highlight-foreground', '#00193a');
        root1.style.setProperty('--btn-default-bg', '#fff');
        root1.style.setProperty('--menu-item-bg-color', '#f5f5f5');
        root1.style.setProperty('--note-warning-bg-color', '#fdf8ea');
        root1.style.setProperty('--note-bg-color', '#f9f9f9');
        root1.style.setProperty('--note-info-bg-color', '#eef7fa');
        root1.style.setProperty('--table-row-odd-bg-color', '#f9f9f9');
        root1.style.transition = 'all 0.5s ease';
    }
}

function DarkTrigger() {
    console.log('dark!!')
    let isDarkMode = getComputedStyle(document.documentElement).getPropertyValue('--content-bg-color').trim() === '#000';
    console.log(isDarkMode)
    if (isDarkMode) {
        // 切换到暗模式
        const warningNotes = document.querySelectorAll('.post-body .note.warning');
        // 修改背景颜色
        warningNotes.forEach(note => {
        note.style.background = '#666';
        });

        const infoNotes = document.querySelectorAll('.post-body .note.info');
        // 修改背景颜色
        infoNotes.forEach(note => {
        note.style.background = '#666';
        });
    }
}


</script>

 <!--js: 线条特效-->
  <script type="text/javascript" color="255,255,255" opacity='1' zIndex="-1" count="300" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>

<button style="background: #868686;
  width: 3rem;
  height: 3rem;
  position: fixed;
  border-radius: 50%;
  border: none;
  right: unset;
  bottom: 2rem;
  left: 2rem;
  cursor: pointer;
  transition: all 0.5s ease;
  display: flex;
  justify-content: center;
  align-items: center;" class="darkmode-toggle" role="checkbox" onclick="toggleMode()">🌓</button>

  <video autoplay loop muted playsinline style="position:fixed;top:50%;opacity: 0.8;left:50%;min-width:100%;min-height:100%;transform:translateX(-50%)translateY(-50%);z-index:-2;">
  <source src="/images/red.mp4" type="video/mp4">
<!-- hexo injector body_end start --><script src="/assets/mmedia/mmedia-loader.js"></script><!-- hexo injector body_end end --></body>
</html>
