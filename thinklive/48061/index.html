<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.24/fancybox/fancybox.css" integrity="sha256-vQkngPS8jiHHH0I6ABTZroZk8NPZ7b+MUReOFE9UsXQ=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"thinklive1.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"flat"},"fold":{"enable":true,"height":500},"bookmark":{"enable":true,"color":"#66CCFF","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":"utterances","storage":true,"lazyload":false,"nav":null,"activeClass":"utterances"},"stickytabs":true,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="李宏毅机器学习 动手学深度学习">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习笔记 all in one">
<meta property="og:url" content="https://thinklive1.github.io/thinklive/48061/index.html">
<meta property="og:site_name" content="thinklive">
<meta property="og:description" content="李宏毅机器学习 动手学深度学习">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/Pasted%20image%2020250401210138.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/Pasted%20image%2020250402152918.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/Pasted%20image%2020250402152932.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/pk7amw95.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/o7aq5jae.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/output_8_0.svg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Csigma_x%5E2%3D%5Cfrac%7B1%7D%7Bn-1%7D%5Csum_%7Bi%3D1%7D%5En%5Cleft%28x_i-%5Cbar%7Bx%7D%5Cright%29%5E2&amp;consumer=ZHI_MENG">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Csigma%5Cleft%28x%2Cy%5Cright%29%3D%5Cfrac%7B1%7D%7Bn-1%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Cleft%28x_i-%5Cbar%7Bx%7D%5Cright%29%5Cleft%28y_i-%5Cbar%7By%7D%5Cright%29&amp;consumer=ZHI_MENG">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/Pasted%20image%2020250902150544.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/Pasted%20image%2020250831171404.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/seq2seq_v9.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/seq2seq_v9_1.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-4.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/relation.png">
<meta property="article:published_time" content="2025-09-09T07:53:29.840Z">
<meta property="article:modified_time" content="2025-09-21T12:21:37.744Z">
<meta property="article:author" content="thinklive">
<meta property="article:tag" content="课程笔记">
<meta property="article:tag" content="python">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="国立台湾大学">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://thinklive1.github.io/assets/ml/Pasted%20image%2020250401210138.png">


<link rel="canonical" href="https://thinklive1.github.io/thinklive/48061/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://thinklive1.github.io/thinklive/48061/","path":"thinklive/48061/","title":"机器学习笔记 all in one"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>机器学习笔记 all in one | thinklive</title>
  







<script type="text/javascript" async src="/js/fairyDustCursor.js"></script>
<script type="text/javascript" async src="/js/tab-title.js"></script>
<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
  <script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>
<script>
const options = {
  bottom: '64px', // default: '32px'
  right: 'unset', // default: '32px'
  left: '32px', // default: 'unset'
  time: '0.5s', // default: '0.3s'
  mixColor: '#fff', // default: '#fff'
  backgroundColor: '#fff',  // default: '#fff'
  buttonColorDark: '#100f2c',  // default: '#100f2c'
  buttonColorLight: '#fff', // default: '#fff'
  saveInCookies: false, // default: true,
  label: '🌓', // default: ''
  autoMatchOsTheme: true // default: true
}

const darkmode = new Darkmode(options);
</script>
<!-- hexo injector head_end start --><script> let HEXO_MMEDIA_DATA = { js: [], css: [], aplayerData: [], metingData: [], artPlayerData: [], dplayerData: []}; </script><!-- hexo injector head_end end --><link rel="alternate" href="/atom.xml" title="thinklive" type="application/atom+xml">
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>
<script src="/js/tab-title.js"></script>
<script type="text/javascript" async src="/js/text.js"></script>

<!--pjax：防止跳转页面音乐暂停-->
 <script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script>
<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>
<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>


  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">thinklive</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">dirichlet library</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索 | search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-主页-|-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>主页 | home</a></li><li class="menu-item menu-item-标签-|-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签 | tags</a></li><li class="menu-item menu-item-分类-|-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类 | categories</a></li><li class="menu-item menu-item-归档-|-archive"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档 | archive</a></li><li class="menu-item menu-item-相册-|-photo"><a href="/photos/" rel="section"><i class="fa fa-camera fa-fw"></i>相册 | photo</a></li><li class="menu-item menu-item-留言-|-guestbook"><a href="/guestbook/" rel="section"><i class="fa fa-book fa-fw"></i>留言 | guestbook</a></li><li class="menu-item menu-item-感谢-|-thank"><a href="/thanks/" rel="section"><i class="fa custom thanks fa-fw"></i>感谢 | thank</a></li><li class="menu-item menu-item-游戏-|-game"><a href="/game/bad1.html" rel="section"><i class="fa fa-gamepad fa-fw"></i>游戏 | game</a></li><li class="menu-item menu-item-神龛-|-shrine"><a href="/cyberblog/" rel="section"><i class="fa fa-microchip fa-fw"></i>神龛 | shrine</a></li><li class="menu-item menu-item-资源地图-|-resourcemap"><a href="/webstack/" rel="section"><i class="fa fa-list fa-fw"></i>资源地图 | resourcemap</a></li><li class="menu-item menu-item-思维导图-|-mindmap"><a href="/mindmap/index.html" rel="section"><i class="fa fa-map fa-fw"></i>思维导图 | mindmap</a></li><li class="menu-item menu-item-网站地图-|-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>网站地图 | sitemap</a></li><li class="menu-item menu-item-蒸汽-|-steam"><a href="/steamgames/index.html" rel="section"><i class="fa fa custum steam fa-fw"></i>蒸汽 | steam</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索 | search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">


<!--网易云音乐插件-->
<!-- require APlayer -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css">
<script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script>
<!-- require MetingJS-->
<script src="https://cdn.jsdelivr.net/npm/meting@2.0.1/dist/Meting.min.js"></script> 
<!--网易云playlist外链地址-->   
<meting-js
    server="netease"
    type="playlist" 
    id="2762741085"
    mini="false"
    fixed="false"
    list-folded="true"
    autoplay="false"
    volume="0.2"
    theme="#4c4c4c"
    order="random"
    loop="all"
    preload="auto"
    lrc-type="2"
    mutex="true">
    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E6%9C%9F%E5%B7%A5%E4%BD%9C"><span class="nav-number">1.</span> <span class="nav-text">前期工作</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E5%AD%A6"><span class="nav-number">1.1.</span> <span class="nav-text">数学</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%9F%E8%AE%A1"><span class="nav-number">1.1.1.</span> <span class="nav-text">统计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E8%AE%BA"><span class="nav-number">1.1.2.</span> <span class="nav-text">概率论</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E5%B8%83"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">分布</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%A1%E6%81%AF%E8%AE%BA"><span class="nav-number">1.1.3.</span> <span class="nav-text">信息论</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%86%B5"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">熵</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="nav-number">1.1.4.</span> <span class="nav-text">线性代数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%8C%83%E6%95%B0"><span class="nav-number">1.1.4.1.</span> <span class="nav-text">范数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B7%A5%E5%85%B7"><span class="nav-number">1.2.</span> <span class="nav-text">工具</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#cuda"><span class="nav-number">1.2.1.</span> <span class="nav-text">cuda</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%B9%E5%99%A8"><span class="nav-number">1.2.2.</span> <span class="nav-text">容器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#python"><span class="nav-number">1.2.3.</span> <span class="nav-text">python</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%90%86%E8%AE%BA"><span class="nav-number">1.3.</span> <span class="nav-text">理论</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F"><span class="nav-number">1.3.1.</span> <span class="nav-text">张量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="nav-number">1.3.2.</span> <span class="nav-text">神经网络的定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98"><span class="nav-number">1.3.3.</span> <span class="nav-text">常见问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95-optimizer"><span class="nav-number">1.3.4.</span> <span class="nav-text">优化算法 optimizer</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96-normalization"><span class="nav-number">1.3.4.1.</span> <span class="nav-text">归一化 Normalization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">1.3.4.2.</span> <span class="nav-text">梯度下降</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">1.3.4.3.</span> <span class="nav-text">反向传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#momentum"><span class="nav-number">1.3.4.4.</span> <span class="nav-text">Momentum</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#adagrad"><span class="nav-number">1.3.4.5.</span> <span class="nav-text">AdaGrad</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">1.3.5.</span> <span class="nav-text">正则化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7"><span class="nav-number">1.3.6.</span> <span class="nav-text">训练技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E6%9D%83%E5%80%BC"><span class="nav-number">1.3.6.1.</span> <span class="nav-text">初始权值</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#batch-normalization"><span class="nav-number">1.3.6.2.</span> <span class="nav-text">Batch Normalization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8A%91%E5%88%B6%E8%BF%87%E6%8B%9F%E5%90%88"><span class="nav-number">1.3.6.3.</span> <span class="nav-text">抑制过拟合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0"><span class="nav-number">1.3.6.4.</span> <span class="nav-text">超参数</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.</span> <span class="nav-text">神经网络模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#python-%E7%9B%B8%E5%85%B3"><span class="nav-number">2.1.</span> <span class="nav-text">python 相关</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E7%A4%BA%E4%BE%8B"><span class="nav-number">2.1.1.</span> <span class="nav-text">基础训练方法示例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.2.</span> <span class="nav-text">线性模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">2.2.1.</span> <span class="nav-text">线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#python-%E5%AE%9E%E7%8E%B0"><span class="nav-number">2.2.1.1.</span> <span class="nav-text">python 实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E7%B1%BB"><span class="nav-number">2.2.2.</span> <span class="nav-text">分类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-number">2.2.2.1.</span> <span class="nav-text">感知机</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92discriminative-model"><span class="nav-number">2.2.2.2.</span> <span class="nav-text">逻辑回归(discriminative model)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%A6%82%E7%8E%87%E7%9A%84%E5%88%86%E7%B1%BB%E6%96%B9%E6%B3%95generative-model"><span class="nav-number">2.2.2.3.</span> <span class="nav-text">基于概率的分类方法(generative model)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#softmax"><span class="nav-number">2.2.2.4.</span> <span class="nav-text">softmax</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7"><span class="nav-number">2.2.3.</span> <span class="nav-text">优化技巧</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#normalization"><span class="nav-number">2.2.4.</span> <span class="nav-text">Normalization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-cnn"><span class="nav-number">2.3.</span> <span class="nav-text">卷积神经网络 CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#spatial-transformer-layer"><span class="nav-number">2.3.1.</span> <span class="nav-text">spatial transformer layer</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#self-attention"><span class="nav-number">2.4.</span> <span class="nav-text">self-attention</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%98%E5%BD%A2%E9%87%91%E5%88%9A"><span class="nav-number">2.4.1.</span> <span class="nav-text">变形金刚</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bert"><span class="nav-number">2.4.2.</span> <span class="nav-text">bert</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rnn"><span class="nav-number">2.5.</span> <span class="nav-text">RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#long-short-term-memory-lstm"><span class="nav-number">2.5.1.</span> <span class="nav-text">Long Short-term Memory (LSTM)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%AE%E9%A2%98"><span class="nav-number">2.5.2.</span> <span class="nav-text">问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="nav-number">2.6.</span> <span class="nav-text">深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E5%BF%B5"><span class="nav-number">2.6.1.</span> <span class="nav-text">概念</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%8A%80%E6%9C%AF"><span class="nav-number">3.</span> <span class="nav-text">技术</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.1.</span> <span class="nav-text">扩散模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#vae-%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8"><span class="nav-number">3.1.1.</span> <span class="nav-text">VAE 变分自编码器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#flow-based-model"><span class="nav-number">3.1.2.</span> <span class="nav-text">flow-based model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#normalizing-flow"><span class="nav-number">3.1.2.1.</span> <span class="nav-text">Normalizing flow</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#flow-matching"><span class="nav-number">3.1.3.</span> <span class="nav-text">Flow Matching</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E5%AD%A6-1"><span class="nav-number">3.1.3.1.</span> <span class="nav-text">数学</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E5%9C%BA%E4%B8%8E%E6%B5%81"><span class="nav-number">3.1.3.2.</span> <span class="nav-text">向量场与流</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%9E%E7%BB%AD%E5%BD%92%E4%B8%80%E5%8C%96%E6%B5%81-continuous-normalizing-flows-cnfs"><span class="nav-number">3.1.3.3.</span> <span class="nav-text">连续归一化流 (Continuous Normalizing Flows, CNFs)</span></a></li></ol></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="thinklive"
      src="/images/thive.png">
  <p class="site-author-name" itemprop="name">thinklive</p>
  <div class="site-description" itemprop="description">起初，世界是一团思索，它向所有的方向迈了一步，于是万物由此而生</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">45</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">49</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/thinklive1" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;thinklive1" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/t469631989@gmail.com" title="E-Mail → t469631989@gmail.com" rel="noopener me"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/38099250?spm_id_from=333.1007.0.0" title="bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;38099250?spm_id_from&#x3D;333.1007.0.0" rel="noopener me" target="_blank"><i class="fa custom bilibili fa-fw"></i>bilibili</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://steamcommunity.com/id/thinkliving" title="steam → https:&#x2F;&#x2F;steamcommunity.com&#x2F;id&#x2F;thinkliving" rel="noopener me" target="_blank"><i class="fa custom steam fa-fw"></i>steam</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml" rel="noopener me"><i class="fa fa-rss fa-fw"></i>RSS</a>
      </span>
  </div>

<div style="Text-align:center;width:100%"><div style="margin:0 auto"><canvas id="canvas" style="width:60%" height="100" width="700">当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>!function(){function t(t,e){for(var a=0;a<l[e].length;a++)for(var n=0;n<l[e][a].length;n++)1==l[e][a][n]&&(h.beginPath(),h.arc(14*(g+2)*t+2*n*(g+1)+(g+1),2*a*(g+1)+(g+1),g,0,2*Math.PI),h.closePath(),h.fill())}function e(){var t=[],e=/(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date),a=[];a.push(e[1],e[2],10,e[3],e[4],10,e[5],e[6]);for(var r=c.length-1;r>=0;r--)a[r]!==c[r]&&t.push(r+"_"+(Number(c[r])+1)%10);for(var r=0;r<t.length;r++)n.apply(this,t[r].split("_"));c=a.concat()}function a(){for(var t=0;t<d.length;t++)d[t].stepY+=d[t].disY,d[t].x+=d[t].stepX,d[t].y+=d[t].stepY,(d[t].x>i+g||d[t].y>f+g)&&(d.splice(t,1),t--)}function n(t,e){for(var a=[1,2,3],n=["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"],r=0;r<l[e].length;r++)for(var o=0;o<l[e][r].length;o++)if(1==l[e][r][o]){var h={x:14*(g+2)*t+2*o*(g+1)+(g+1),y:2*r*(g+1)+(g+1),stepX:Math.floor(4*Math.random()-2),stepY:-2*a[Math.floor(Math.random()*a.length)],color:n[Math.floor(Math.random()*n.length)],disY:1};d.push(h)}}function r(){o.height=100;for(var e=0;e<c.length;e++)t(e,c[e]);for(var e=0;e<d.length;e++)h.beginPath(),h.arc(d[e].x,d[e].y,g,0,2*Math.PI),h.fillStyle=d[e].color,h.closePath(),h.fill()}var l=[[[0,0,1,1,1,0,0],[0,1,1,0,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,0,1,1,0],[0,0,1,1,1,0,0]],[[0,0,0,1,1,0,0],[0,1,1,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[1,1,1,1,1,1,1]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,0,0,1,1],[1,1,1,1,1,1,1]],[[1,1,1,1,1,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,0,1,1,1,0],[0,0,1,1,1,1,0],[0,1,1,0,1,1,0],[1,1,0,0,1,1,0],[1,1,1,1,1,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,1,1]],[[1,1,1,1,1,1,1],[1,1,0,0,0,0,0],[1,1,0,0,0,0,0],[1,1,1,1,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[1,1,1,1,1,1,1],[1,1,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,1,1,0,0,0,0]],[[0,0,0,0,0,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,0,0,0]]],o=document.getElementById("canvas");if(o.getContext){var h=o.getContext("2d"),f=100,i=700;o.height=f,o.width=i,h.fillStyle="#f00",h.fillRect(10,10,50,50);var c=[],d=[],g=o.height/20-1;!function(){var t=/(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date);c.push(t[1],t[2],10,t[3],t[4],10,t[5],t[6])}(),clearInterval(v);var v=setInterval(function(){e(),a(),r()},50)}}()</script></div>
<img class= 'logo' src="/images/thinklive_cyber.png"; z-index: '0'; style="max-width: 100%; width: auto; height: auto;background-color: --content-bg-color;">

        </div>
      </div>
    </div>

    
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://thinklive1.github.io/thinklive/48061/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/thive.png">
      <meta itemprop="name" content="thinklive">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="thinklive">
      <meta itemprop="description" content="起初，世界是一团思索，它向所有的方向迈了一步，于是万物由此而生">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="机器学习笔记 all in one | thinklive">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习笔记 all in one
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-09-09 15:53:29" itemprop="dateCreated datePublished" datetime="2025-09-09T15:53:29+08:00">2025-09-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-21 20:21:37" itemprop="dateModified" datetime="2025-09-21T20:21:37+08:00">2025-09-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">课程笔记</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>19k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1:09</span>
    </span>
</div>

        
        </div>
      </header>
   

    
    
    
    <div class="post-body" itemprop="articleBody"><p><a target="_blank" rel="noopener" href="https://speech.ee.ntu.edu.tw/~hylee/ml/2022-spring.php">李宏毅机器学习</a> <a target="_blank" rel="noopener" href="https://zh.d2l.ai/">动手学深度学习</a></p>
<span id="more"></span>
<h1 id="前期工作">前期工作</h1>
<h2 id="数学">数学</h2>
<h3 id="统计">统计</h3>
<p>机器学习的本质是寻找一个理想的函数, 这个理想可以定义为在训练集和全集上表现几乎一致, 使用统计的原理可以尽可能理解这其中的一些关系:<br />
定义: <code>|𝐿 ℎ, 𝒟𝑡𝑟𝑎𝑖𝑛 − 𝐿 ℎ, 𝒟𝑎𝑙𝑙 | &gt; 𝜀</code> 时 <code>Dtrain</code> 是差的, 那么:</p>
<p><span class="math display">\[P(\mathcal{D}_{t r a i n}\;i s\;b a d)\leq|\mathcal H|\cdot2e x p(-2N\varepsilon^{2})\]</span></p>
<p>其中, N 是训练样本数, H 为备选函数的集合, 也就是如果想要这个理想条件, H 和 N 会存在这种约束条件<br />
需要注意的是, 这个理想条件并不包括具体的 loss 值, 如果 H 过小, 那么 loss 就可能会很大</p>
<h3 id="概率论">概率论</h3>
<p>数学期望:</p>
<p><span class="math display">\[E[X]=\sum_{x}x P(X=x).\]</span></p>
<p>当函数 f(x)的输入是从分布 P 中抽取的随机变量时, f(x)的期望值为:</p>
<p><span class="math display">\[E_{x\sim P}[f(x)]=\sum_{x}f(x)P(x).\]</span></p>
<p>方差：</p>
<p><span class="math display">\[\operatorname{Var}[X]=E\left[(X-E[X])^{2}\right]=E[X^{2}]-E[X]^{2}.\]</span></p>
<h4 id="分布"><a target="_blank" rel="noopener" href="https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/distributions.html">分布</a></h4>
<h3 id="信息论"><a target="_blank" rel="noopener" href="https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html">信息论</a></h3>
<h4 id="熵">熵</h4>
<p>在不同的领域中，熵被表示为混乱程度，不确定性，惊奇程度，不可预测性，信息量等，但在机器学习中我们只需要对信息论中的熵有基本了解<br />
信息论中熵的概念首次被香农提出，目的是寻找一种高效/无损地编码信息的方法，即 <strong>无损编码事件信息的最小平均编码长度</strong> 即信息熵 H(X) = -Σ p(x) log2 p(x) (p 为概率)</p>
<p>接下来说明这个公式，假设我们用二进制的哈夫曼编码，一个信息出现概率是 1/2, 即其他所有情况加起来也是 1/2，那么我们会发现其编码长度必然是-log(1/2), 也就是 1，恰好和我们的香农熵定义一致了，这是为什么呢？<br />
严谨的数学证明超出了 cs 专业范围，这里只说一下直观理解，熵有两个性质：</p>
<ul>
<li>概率越小信息量越大(如果一个小概率事件发生了，就排除了非常多其他可能性)</li>
<li>假设两个随机变量 x, y 相互独立，那么分别观测两个变量得到的信息量应该和同时观测两个变量的信息量是相同的，<code>h(x+y)=h(x)+h(y)</code></li>
</ul>
<p>如此一来对数函数的负数完美符合条件，基数则无所谓，直观地理解，基数对应用几进制编码，而要最短化编码，越小概率就应该用更长的位数，把短位数腾出来给大概率事件用，当然实际中编码的位数是离散的，而且相比对数取负只能多不能少，因此香农熵是一个理论最优值，熵编码就指无损情况下的编码方式，最常用的就是哈夫曼编码，所有熵编码方式的编码长度大于等于香农熵</p>
<p>现实中常用二进制编码信息，例如对 8 种不同的信息，最直观的编码是三位二进制，每三位表示一个独特信息。<br />
我们可以用概率表示每种信息出现的可能，例如 8 种信息，每个都等可能出现，那么以概率为权的哈夫曼编码就会用所有的 3 位二进制编码这 8 种信息，熵就是 3，而其他情况熵可以当做哈夫曼树的总编码长度算<br />
那么如何理解熵能反映混乱度呢？如果熵比较大(即平均编码长度较长)，意味着这一信息有较多的可能状态，相应的每个状态的可能性比较低；因此每当来了一个新的信息，我们很难对其作出准确预测，即有着比较大的混乱程度/不确定性/不可预测性</p>
<p><strong>交叉熵</strong>：<br />
交叉熵用于评估估计概率得到的熵与真实熵的差距，交叉的含义很直观，就是使用 P 计算期望，使用 Q 计算编码长度<br />
为什么这么选而不是反过来呢？这取决于我们的目的，一般来说，我们希望估计的编码长度和理论最优的熵差距较小，要比对取优的主要是模型的编码长度即 logQ，可以这么理解，熵公式中的对数函数视为视为对一个特定概率事件的编码长度，由于现实的概率分布实际上是确定的，那么需要评估的也就是编码方式的效率<br />
由于熵是给定概率分布下的最优值，<strong>交叉熵只可能大于等于熵</strong>，两者差越小或者说交叉熵越小表示模型估计越准<br />
例如在最极端的 one-hot 编码中，交叉熵等价于 <strong>对应正确解标签的输出的自然对数</strong></p>
<h3 id="线性代数">线性代数</h3>
<h4 id="范数">范数</h4>
<p>范数是具有“长度”概念的函数，用于衡量一个矢量的大小（测量矢量的测度）<br />
由于不是数学系的，这里就极为不严谨地记录一下范数的理解：</p>
<ul>
<li>0 范数，向量中非零元素的个数</li>
<li>1 范数，为绝对值之和</li>
<li>2 范数，就是通常意义上的模</li>
</ul>
<p>正则化的目的可以理解为限制权重向量的复杂度，实际做法为在损失函数中加入与权重向量复杂度有关的惩罚项，而范数在某种意义上可以反映这点，因此可作为选取正则项的依据<br />
顺便一提 a star 算法也会用类似的测度估计距离</p>
<h2 id="工具">工具</h2>
<h3 id="cuda">cuda</h3>
<p>Compute Unified Device Architecture (CUDA): 简单地说，就是允许软件调用 gpu 来计算的一个接口<br />
CUDA Runtime API vs. CUDA Driver API</p>
<ul>
<li>驱动版本需要 ≥ 运行时 api 版本</li>
<li>driver user-space modules 需要和 driver kernel modules 版本一致</li>
<li>当我们谈论 cuda 时，往往是说 runtime api</li>
</ul>
<p>以下是 nvida 的介绍原文：</p>
<div class="note default"><p>It is composed of two APIs:</p>
<ul>
<li>A low-level API called the CUDA driver API,</li>
<li>A higher-level API called the CUDA runtime API that is implemented on top of the CUDA driver API.</li>
</ul>
<p>The CUDA runtime eases device code management by providing implicit initialization, context management, and module management. The C host code generated by nvcc is based on the CUDA runtime (see Section 4.2.5), so applications that link to this code must use the CUDA runtime API.</p>
<p>In contrast, the CUDA driver API requires more code, is harder to program and debug, but offers a better level of control and is language-independent since it only deals with cubin objects (see Section 4.2.5). In particular, it is more difficult to configure and launch kernels using the CUDA driver API, since the execution configuration and kernel parameters must be specified with explicit function calls instead of the execution configuration syntax described in Section 4.2.3. Also, device emulation (see Section 4.5.2.9) does not work with the CUDA driver API.</p>
</div>
<p>简单地说, driver 更底层，更抽象但性能和自由度更好，runtime 则相反</p>
<h3 id="容器">容器</h3>
<p><img src="/assets/ml/Pasted%20image%2020250401210138.png" /><br />
infrastructure(基础设施)<br />
简单地说，虚拟机的隔离级别比容器更高，虚拟机会模拟出一个系统及其系统 api，而 docker 依旧调用宿主机的 api，因此 docker 更为轻量级<br />
docker 是处理复杂环境问题的良策，比虚拟机更为轻量<br />
其他常用的容器: Slurm and Kubernetes</p>
<p><a target="_blank" rel="noopener" href="https://hub.docker.com/r/pytorch/pytorch/tags">Docker Hub repository of PyTorch</a></p>
<h3 id="python">python</h3>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  </span><br><span class="line"><span class="comment"># 生成数据  x = np.arange(0, 6, 0.1) # 以 0.1 为单位, 生成 0 到 6 的数据 </span></span><br><span class="line"></span><br><span class="line">y1 = np.sin(x)</span><br><span class="line">y2 = np.cos(x)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制图形  </span></span><br><span class="line">plt.plot(x, y1, label=<span class="string">&quot;sin&quot;</span>) plt.plot(x, y2, linestyle = <span class="string">&quot;--&quot;</span>, label=<span class="string">&quot;cos&quot;</span>) </span><br><span class="line"><span class="comment"># 用虚线绘制 </span></span><br><span class="line">plt.xlabel(<span class="string">&quot;x&quot;</span>) </span><br><span class="line"><span class="comment"># x 轴标签 </span></span><br><span class="line">plt.ylabel(<span class="string">&quot;y&quot;</span>) </span><br><span class="line"><span class="comment"># y 轴标签 </span></span><br><span class="line">plt.title(<span class="string">&#x27;sin &amp; cos&#x27;</span>) </span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="理论">理论</h2>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/tensors.html">官方文档</a><br />
<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/index.html">Official Pytorch Tutorials</a><br />
<a target="_blank" rel="noopener" href="https://github.com/wkentaro/pytorch-for-numpy-users">pytorch-for-numpy-users</a></p>
<h3 id="张量">张量</h3>
<p><strong>张量 tensor</strong>: 用于表示 n 维数据的一种概念，例如一维张量是向量，二维是矩阵……</p>
<p><code>dim in PyTorch == axis in NumPy</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&quot;</span>, torch.cuda.is_available())</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        device = torch.device(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;There are <span class="subst">&#123;torch.cuda.device_count()&#125;</span> GPU(s) available.&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Device name:&quot;</span>, torch.cuda.get_device_name(<span class="number">0</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;No GPU available, using the CPU instead.&quot;</span>)</span><br><span class="line">        device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">test()</span><br></pre></td></tr></table></figure>
<pre><code> True
There are 1 GPU(s) available.
Device name: NVIDIA GeForce RTX 2070</code></pre>
<p>以下是一些朴素的张量操作:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>, -<span class="number">1</span>], [-<span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">x = torch.from_numpy(np.array([[<span class="number">1</span>, -<span class="number">1</span>], [-<span class="number">1</span>, <span class="number">1</span>]]))</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[ 1, -1],
        [-1,  1]])
tensor([[ 1, -1],
        [-1,  1]])</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros([<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">x = torch.ones([<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>]) </span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[0., 0.],
        [0., 0.]])
tensor([[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]])</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line"></span><br><span class="line">x = x.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([2, 3])  
torch.Size([3, 2])</code></pre>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">A = torch.arange(<span class="number">20</span>, dtype=torch.float32).reshape(<span class="number">5</span>, <span class="number">4</span>) </span><br><span class="line">B = A.clone() <span class="comment"># 通过分配新内存, 将 A 的一个副本分配给 B  </span></span><br><span class="line"></span><br><span class="line">A*B<span class="comment">#按元素乘法的 Hadamard 积，即矩阵相同位置的元素相乘，得到结果矩阵该位置的元素，数学符号 ⊙</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">结果如下：</span></span><br><span class="line"><span class="string">tensor([[ 0., 1., 4., 9.], </span></span><br><span class="line"><span class="string">        [ 16., 25., 36., 49.], </span></span><br><span class="line"><span class="string">        [ 64., 81., 100., 121.],</span></span><br><span class="line"><span class="string">        [144., 169., 196., 225.],</span></span><br><span class="line"><span class="string">        [256., 289., 324., 361.]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">A_sum_axis0 = A.<span class="built_in">sum</span>(axis=<span class="number">0</span>)  <span class="comment">#沿0 轴对 A 求和</span></span><br><span class="line">A_sum_axis0, A_sum_axis0.shape</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">numpy的对某一轴求和，就是把输入轴所有值加起来，投影到剩下的轴上，这个轴不是消失了，而是现在被投影到一起，例如二维矩阵里，沿0轴求和，就是沿着所有行求和，所有行的对应元素加起来，投影到一行，结果行数为1可以降低一个维度为向量，作为sum的输出(更高维情况下除了输入轴都不变，最后维度-1)</span></span><br><span class="line"><span class="string">在这个例子中，结果是：</span></span><br><span class="line"><span class="string">(tensor([40., 45., 50., 55.]), torch.Size([4]))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">如果让sum的参数keepdims=True，就不会执行维度减一的操作，结果会是个行数1，列数4的矩阵</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>沿某个轴计算 A 元素的累积总和, 比如 <code>axis=0(按行计算)</code>, 可以调用 <code>cumsum</code> 函数。此函数不会沿任何轴降低输入张量的维度。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">A.cumsum(axis=<span class="number">0</span>)  </span><br><span class="line">tensor([[ <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],  </span><br><span class="line">        [ <span class="number">4.</span>, <span class="number">6.</span>, <span class="number">8.</span>, <span class="number">10.</span>],  </span><br><span class="line">        [<span class="number">12.</span>, <span class="number">15.</span>, <span class="number">18.</span>, <span class="number">21.</span>],  </span><br><span class="line">        [<span class="number">24.</span>, <span class="number">28.</span>, <span class="number">32.</span>, <span class="number">36.</span>],  </span><br><span class="line">        [<span class="number">40.</span>, <span class="number">45.</span>, <span class="number">50.</span>, <span class="number">55.</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="神经网络的定义">神经网络的定义</h3>
<p>对每一层神经网络，输入的 x 乘以权重向量 w(eight)，加上一个标量 b(ias)后就是输出 y<br />
例如训练一个将 32 维向量转化为 64 维向量输出的模型，权值矩阵规模是 64×32, 输入向量是 32×1，输出是 64×1<br />
算出线性的权值和之后，增加一层激活函数 <strong>Activation Function</strong>, 激活函数常是非线性的，用于增强网络学习能力，如果没有激活函数，网络就是单纯的有很多层数的线性回归<br />
最基础的神经网络是 <strong>全连接前馈神经网络(fully-connected feed-forward network)</strong>: 前馈表示从前到后训练，全连接表示相邻的两层中，所有的神经元都是相连的; 网络第一层称为输入层（input layer），最后一层称为输出层（output layer），中间的层数被称为隐藏层(hidden layer)</p>
<div class="note info"><p><em>前向和反向，forward &amp;&amp; backward: 理解这两个词应该看英文，forward 这个前指的是时间上从前往后，也就是训练时的正常时间顺序，backward 与其相反，就是从结果推开头</em></p>
</div>
<p>深度学习：这个定义非常简单粗暴，意思是隐藏层很多，一般可能有三位数起步，并非越多层就越好，这需要合适的数据集，合适的提取特征方式，即 <strong>特征工程 feature engineering</strong>，才能定义一个较好的网络</p>
<p>常见激活函数：</p>
<ul>
<li>Sigmoid 函数也叫 Logistic 函数，用于隐层神经元输出，取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好，图像类似一个 S 形曲线:
<ul>
<li> $ f(x)=\frac{1}{1+e^{-x}} $  </li>
</ul></li>
<li>ReLU 函数又称为修正线性单元（Rectified Linear Unit），是一种分段线性函数，弥补了 sigmoid 函数的梯度消失问题(即该函数两端非常平滑，导数趋近 0，遇到数值偏两端的数据，loss 很难传播):
<ul>
<li> $f(x)={\left\{\begin{array}{l l}{x}&{,x\gt =0}\\ {0}&{,x\lt 0}\end{array}\right.} $ </li>
</ul></li>
</ul>
<p><img src="/assets/ml/Pasted%20image%2020250402152918.png" /> <img src="/assets/ml/Pasted%20image%2020250402152932.png" /></p>
<p><strong>损失函数 loss function</strong>：评估训练成果的一个标准，越小越好<br />
loss = criterion(model_output, expected_value) #nn: neural network</p>
<ul>
<li><code>criterion = nn.MSELoss()</code>: Mean Squared Error</li>
<li><code>criterion = nn.CrossEntropyLoss()</code>: Cross Entropy 交叉熵</li>
</ul>
<h3 id="常见问题">常见问题</h3>
<ol type="1">
<li>模型偏差：模型在训练资料上的损失函数很大时，可能是因为在这个问题中选择的模型太过简单，以至于无论用这个给模型选择什么样的参数 θ，损失函数 f(θ)都不会变得很小</li>
<li>优化问题，梯度下降看上去很美好，但常常会卡在一个局部最优（local minima）点，这个局部最优可能和全局最优（global minima）差得很远，因此需要选取更好的优化算法如 Adam，RMSProp，AdaGrad 等</li>
<li>过拟合，训练集 Loss 很小，测试集却很大，需要注意的是首先得满足前一个条件，不然也可能是 1.2.问题
<ol type="1">
<li>最有效的一种解决方案是增加训练资料，但很多时候是无法做到的</li>
<li>第二种方法就是数据增广（Data Augmentation），常用于图像处理。既然不能增加数据，那就更好地利用现有数据；例如：对图像左右镜像，改变比例等等，需要注意不能过度改变数据特征，例如上下颠倒图片</li>
<li>增加对模型的限制，常见如早停止，正则化，丢弃部分不合理数据等等</li>
</ol></li>
</ol>
<h3 id="优化算法-optimizer">优化算法 optimizer</h3>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html">torch.optim</a><br />
e.g.</p>
<ol type="1">
<li>optimizer.zero_grad()：重设梯度(即训练完一段后梯度置 0, 截断反向传播再继续训练)</li>
<li>Call loss.backward() 反向传播以减少损失</li>
<li>Call optimizer.step() 调整模型参数</li>
</ol>
<h4 id="归一化-normalization">归一化 Normalization</h4>
<p>训练中的数据很多情况下大小完全不统一，基于直觉的想法是：同一维度的数据我们只关心其相对大小关系，不同维度的数据我们认为它们的地位平等，尺度应到一致，所以需要归一化<br />
简单地说就是把一个维度的数据大小都调整到 <code>[0-1]</code> 这个区间，例如 softmax 函数就用于将一个向量转化成概率分布(令其总和为 1)</p>
<div class="note info"><p>归一化 <strong>normalization</strong> 容易和正则化 <strong>regularization</strong> 搞混，来看看词典解释：</p>
<ul>
<li>normalization: to start to consider something as normal, or to make something start to be considered as normal</li>
<li>regularization: the act of changing a situation or system so that it follows laws or rules, or is based on reason</li>
</ul>
<p>也就是，归一化偏向于“正常”，正则化偏向于“规则”，差别非常微妙，但硬要说的话，0-1 的数据看起来是比其他的更“正常”一点</p>
<p>神经网络中进行的处理有推理(inference)和学习两个阶段。神经网络中未被正规 化(归一化)的输出结果有时被称为“得分”。也就是说, 当神经网络的推理只需要给出一个答案的情况下, 因为此时只对得分最大值感兴趣, 所以不需要 Softmax 层。 不过, 神经网络的学习阶段则需要 Softmax 层</p>
</div>
<h4 id="梯度下降">梯度下降</h4>
<p><strong>梯度下降法</strong>：<br />
有点类似于牛顿法(牛顿法理论是二阶收敛，梯度则为一阶，牛顿法速度更快计算量更大)，所谓的梯度就是一个多元函数中，对一个点求各个元的偏导数取值组成的一个表示方向的向量(即偏导数乘对应元的单位向量)<br />
这个梯度一般指向当前点增加最快的方向，把他乘以-1 就会得到下降最快的方向(一般用于最小化损失函数)，梯度只表示方向，因此还需要选择合适的步长 α，乘以方向向量后就得到移动的路径，步长太长了会跨过极小值然后来回震荡，太短了效率会很差<br />
梯度下降算法需要设置一个 <strong>学习率(learning rate)</strong>，每次迭代中，未知参数的梯度下降的步长取决于学习率的设置，这种由人为设定的参数被称为 <strong>超参（hyperparameters）</strong><br />
如果我们的数据集很大，计算梯度会相当复杂，则可以分为 n 个 batch, 每个 batch 有 B 个资料，即 <strong>mini-batch 梯度下降（MBGD，Mini-Batch GradientDescent</strong><br />
在 numpy 里用形如 <code>np.random.choice(scope, num)</code> 的方法就可以在 scope 内随机选取 num 个索引作为 mini batch</p>
<h4 id="反向传播">反向传播</h4>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/charlotte77/p/5629865.html">通俗讲解反向传播</a><br />
反向传播是梯度下降在神经网络上的具体实现方式，与前向训练过程相反，用结果的损失值修正训练中的权值，在数学上就是求损失对前一层权重的偏导数<br />
但是损失函数的参数是本层的输出值，因此需要链式法则求偏导，先求出损失函数对输出的偏导，再乘以本层输出对前一层权重的偏导(当然我们知道本层神经元收到上层的输出后还要算个激活函数才能得到最后的输出，因此中间还有一步本层输出对上层输出求偏导，只有上层输出直接和上层权重有关)</p>
<p><img src="/assets/ml/pk7amw95.png" /> 如图，权重结合上层 out 得到 net, net 经过激活函数得到本层 out，本层 out 用来计算损失值，因此反向传播会反过来算<br />
对着这些复杂的名称求偏导数看起来有点奇怪，但这和常见的 yx 没什么不同<br />
得到偏导函数后，接下来正如在梯度下降中学到的，我们希望通过不断调整上层权重来最小化结果层的损失函数，每次用一个“学习速率 <span class="math inline">\(\eta\)</span> ”乘以偏导数来更新权重</p>
<p><img src="/assets/ml/o7aq5jae.png" /></p>
<p>涉及隐藏层时稍微有所不同，在由于隐藏层的下一层会连到不同的结果，也就会产生多个损失值，在 out 部分需要对不同的损失值求偏导，即总误差对 outh1 的偏导是 E(o1)和 E(o2)对 outh1 偏导的和</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">dataset = MyDataset(file)</span><br><span class="line">tr_set = DataLoader(dataset, <span class="number">16</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">model = MyModel().to(device)</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> tr_set:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        x, y = x.to(device), y.to(device)</span><br><span class="line">        pred = model(x)</span><br><span class="line">        loss = criterion(pred, y)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step() <span class="comment">#更新模型的参数</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<div class="note info"><p><strong>梯度消失</strong>：</p>
<p>如果我们用最常见的 sigmoid 函数，且在某层上输入值在函数的两端，此时导数非常小，也就是梯度非常小，这层上的反向传播几乎无法更新，不仅如此，更之前的层级也很难继续传播，这就是所谓的梯度消失问题。<br />
除了更新缓慢问题外，这还会导致浅层和深层的更新程度区别巨大，让模型变得“不均匀”，此外，由于 sig 函数的梯度区间较小，模型深了几乎必然有这种问题</p>
<p>ReLU 用于解决梯度消失问题，其梯度要么是 0 要么是 1，只在负端消失，这样有个可能的好处，如果负端的数据其实是噪声或者是一些我们不关注的特征，那么扔掉反而会让模型效果更好<br />
这种激活函数的缺点是，梯度非负数，对于一层所有的 w，梯度的符号都是一样的，只能一起增大或者减小，这可能减少模型的准确度<br />
通常，激活函数的输入值有一项偏置项(bias)，假设 bias 变得太小，以至于输入激活函数的值总是负的，那么反向传播过程经过该处的梯度恒为 0, 对应的权重和偏置参数此次无法得到更新。如果对于所有的样本输入，该激活函数的输入都是负的，那么该神经元再也无法学习，称为神经元”死亡“问题</p>
<p>LeakyReLU 的提出就是为了解决神经元”死亡“问题，其输入小于 0 的部分，值为负，且有微小的梯度，除了避免死亡还有一个可能的好处是，该微小的梯度可能让训练有一些微小的振动，在特定情况能跳出局部最优解</p>
<p>python 实现:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>): </span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x)) <span class="comment">#NumPy 的广播特性: 如果在标量和 NumPy 数组之间进行运算, 则标量会和 NumPy 数组的各个元素进行运算</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">x</span>): </span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>, x)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</div>
<p>参考项目:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers">Huggingface Transformers</a> (transformer models: BERT, GPT, ...)</li>
<li><a target="_blank" rel="noopener" href="https://github.com/pytorch/fairseq">Fairse</a> (sequence modeling for NLP &amp; speech)</li>
<li><a target="_blank" rel="noopener" href="https://github.com/espnet/espnet">ESPnet</a> (speech recognition, translation, synthesis, ...)</li>
</ul>
<h4 id="momentum">Momentum</h4>
<p>梯度下降简单易懂，但当然也存在问题，考虑如下的函数：</p>
<p><span class="math display">\[f(x,y)=\frac{1}{20}x^{2}+y^{2}\]</span></p>
<p>其 x 轴梯度远小于 y 轴梯度，函数像一个山岭，但底部是个起伏不大的抛物线，更新路径是一个个之字形，y 轴更新地快不断震荡，x 轴则慢慢向真正的底部前进<br />
也就是说，对非均向(anisotropic)的函数，梯度下降效率有限</p>
<p>Momentum(动量)是一种改进的优化方式，这里不管其物理含义，具体更新方法如下所示：</p>
<p><span class="math display">\[v\leftarrow\alpha v-\eta{\frac{\partial L}{\partial W}}\]</span> <span class="math display">\[W\leftarrow W+v\]</span></p>
<p>其他变量我们都知道了，这个 v 对应物理上的速度，表示物体在梯度方向受力，α 模拟类似摩擦力导致的减速，一般比 1 略小，如 0.9，也就是说，原来的梯度下降可视为无阻力的运动，动量法让其速度越来越慢，这可以有效减少路径的折线情况</p>
<h4 id="adagrad">AdaGrad</h4>
<p>学习率衰减(learning rate decay), 即随着学习的进行, 使学习率逐渐减小是一种常见的思路；AdaGrad 进一步发展了这个想法, 针对不同参数, 赋予相互独立的学习率，即 Adaptive Grad</p>
<p><span class="math display">\[h\leftarrow h+{\frac{\partial L}{\partial W}}\leftarrow{\frac{\partial L}{\partial W}}\]</span> <span class="math display">\[W\leftarrow W-\eta\frac{1}{\sqrt{h}}\frac{\partial L}{\partial W}\]</span></p>
<p>h 保存了以前的所有梯度值的平方和, 这样能够按参数的元素进行学习率衰减, 使变动大的参数的学习率逐渐减小，直至无限接近 0；RMSProp 方法会舍弃一些比较早的梯度避免这个问题<br />
为了避免棘手的除 0 问题，h 的平方根可以加一个微小值如 1e-7</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1412.6980">adam</a> 是 2015 年提出的新方法, 直观但不准确地说就是融合了 Momentum 和 AdaGrad 的方法</p>
<h3 id="正则化">正则化</h3>
<p>正则化指为解决适定性问题或过拟合而加入额外信息的过程，在机器学习中，常见的就是为损失函数添加一些调整项<br />
根据前文所述，学习的过程就是根据损失函数与权重的关系不断调整权重以最小化损失，而正则化的目的是不要让权重关系太复杂以致于没有普适性。我们将原始的损失函数画一个图，正则项再画一个图，需要找的就是两个函数同样权重基础上的最小和<br />
用比较简单的双权重，均方误差损失函数来说，w1, w2 就是 xy 轴，最后的 loss 作为 z 轴，原始损失函数 L 可能千奇百怪，但最后要找的是其与正则项的最小和.这个值通常在两个函数的交点取，而(二维层面上)L1 的图像是菱形，l2 是个圆，符合直观地推导，前者交点很容易在坐标轴上取，后者容易在离坐标轴近的地方取，即 l1 容易让权重稀疏，L2 容易让它们的值的绝对值较小且分布均匀<br />
数学上讲，抛开不确定的损失函数，l1 正则项的导数是 w× 正负信号量，迭代时如果 w 大于 0 会减少，大于 0 会增加，最后很容易变成 0；而 l2 的导数是 w 的一次函数且一次项系数小于 1，迭代让 w 不断减小，这个减小量与 w 本身有关，因此一般来说不容易减到 0</p>
<h3 id="训练技巧">训练技巧</h3>
<h4 id="初始权值">初始权值</h4>
<p>什么是最好的初始权重？这个问题很难回答，不如反过来举一些反例<br />
相同的权重肯定是最坏的选择，由于随机梯度下降法对相同或者相似的权值会有非常相似的传导效果，最终模型的权值也会趋同，降低其表达力</p>
<p>一般来说，我们用高斯分布来初始权重，例如常用的 Xavier 初始值根据上层(以及下层)的节点数量确定初始权重的分布，例如在与前一层有 n 个节点连接时, 初始值使用标准差为 <span class="math inline">\(\frac{1}{\sqrt{n}}\)</span> 的分布<br />
Xavier 初始值是以激活函数是线性函数为前提而推导出来的。因为 sigmoid 函数和 tanh 函数左右对称, 且中央附近可以视作线性函数, 所以适合使用 Xavier 初始值。但当激活函数使用 ReLU 时, 一般推荐使用 ReLU 专用的初始值, 也就是 Kaiming He 等人推荐的初始值, 也称为“He 初始值”<br />
当前一层的节点数为 n 时, He 初始值使用标准差为 <span class="math inline">\(\sqrt{\frac{2}{n}}\)</span> 的高斯分布。当 Xavier 初始值是 <span class="math inline">\({\sqrt{\frac{1}{n}}}\)</span> 时,(直观上)可以解释为, 因为 ReLU 的负值区域的值为 0, 为了使它更有广度, 所以需要 2 倍的系数</p>
<h4 id="batch-normalization">Batch Normalization</h4>
<p>Batch Norm 的思路是调整各层的激活值分布使其拥有适当的广度, 具体而言, 就是进行使数据分布的均值为 0、方差为 1 的正规化</p>
<p><span class="math display">\[\mu_{B}\,\leftarrow\,\frac{1}{m}\sum_{i=1}^{m}x_{i}\]</span> <span class="math display">\[\sigma_{B}^{2}\iff\sum_{i=1}^{m}(x_{i}-\mu_{B})^{2}\]</span> <span class="math display">\[\hat{x}_{i}\enspace\leftarrow\ \frac{x_{i}\leftarrow\mu_{B}}{\sqrt{\sigma_{B}^{2}\,+\,\varepsilon}}\,\]</span></p>
<p>ε 是一个微小值，用来防止除 0</p>
<p>Batch Norm 层会对正规化后的数据进行缩放和平移的变换:</p>
<p><span class="math display">\[y i\longleftarrow\gamma\hat{x}_{i}+\beta\]</span></p>
<p>γ、β 是参数，初始为 1、0，随后根据学习来调整</p>
<h4 id="抑制过拟合">抑制过拟合</h4>
<p><strong>权值衰减</strong>：为损失函数加上权重的平方范数(L2 范数)，即让正则项为 <span class="math inline">\(\textstyle{ {\frac{1}{2}}\lambda W^{2} }\)</span> ，其中 λ 是控制正则化强度的超参数，其梯度也会加上一个 λW</p>
<p><strong>dropout</strong>：在学习的过程中随机删除神经元，停止向前传递信号</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Dropout</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dropout_ratio=<span class="number">0.5</span></span>):</span><br><span class="line">        self.dropout_ratio = dropout_ratio</span><br><span class="line">        self.mask = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, train_flg=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="keyword">if</span> train_flg:</span><br><span class="line">            self.mask = np.random.rand(*x.shape) &gt; self.dropout_ratio<span class="comment">#*x.shape 将 x 的形状解包</span></span><br><span class="line">            <span class="keyword">return</span> x * self.mask</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> x * (<span class="number">1.0</span> - self.dropout_ratio)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dout</span>):</span><br><span class="line">        <span class="keyword">return</span> dout * self.mask</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>集成学习</strong>：让多个模型单独进行学习, 推理时再取多个模型的输出的平均值。Dropout 可以理解为：通过在学习过程中随机删除神经元, 从而每一次都让不同的模型进行学习；推理时, 通过对神经元的输出乘以删除比例, 可以取得模型的平均值。</p>
<h4 id="超参数">超参数</h4>
<p>超参数(hyper-parameter)：如各层的神经元数量、batch 大小、参 数更新时的学习率或权值衰减等<br />
需要注意的是，不能使用测试数据评估超参数的性能，否则会让超参数的值会对测试数据发生过拟合，一般用验证数据来评估性能 <a target="_blank" rel="noopener" href="https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf">有报告显示</a>，在进行神经网络的超参数的最优化时, 与网格搜索等有规律的搜索相比, 随机采样的搜索方式效果更好。这是因为在 多个超参数中, 各个超参数对最终的识别精度的影响程度不同 大致的步骤是：</p>
<ol type="1">
<li>设定超参数的范围</li>
<li>从设定的超参数范围中随机采样</li>
<li>使用 1.中采样到的超参数的值进行学习, 通过验证数据评估识别精度(但是要将 epoch 设置得很小)</li>
<li>重复 1. 2. 不断缩小参数到一个合理的值</li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">weight_decay = <span class="number">10</span> ** np.random.uniform(-<span class="number">8</span>, -<span class="number">4</span>)<span class="comment">#uniform 生成指定范围内的随机数, 默认 0 维度，可以指定维度</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="神经网络模型">神经网络模型</h1>
<h2 id="python-相关">python 相关</h2>
<ul>
<li><code>DataLoader(train_date,shuffle=True)</code> 中 shuffle 表示打乱数据集，符合直觉的想法是：这对避免过拟合有帮助<br />
</li>
<li>epoch 是一个单位。一个 epoch 表示学习中所有训练数据均被使用过一次时的更新次数。比如, 对于 10000 笔训练数据, 用大小为 100 笔数据的 mini-batch 进行学习时, 重复随机梯度下降法 100 次, 所有的训练数据就都被“看过”了。此时,100 次就是一个 epoch</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">My_model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self，inputdim</span>):</span><br><span class="line">        <span class="built_in">super</span>(My_model,self).__init__()</span><br><span class="line">        self.layers=nn.sequential(<span class="comment">#将多个层按顺序组合在一起</span></span><br><span class="line">            nn.Linear(input_dim,<span class="number">64</span>), </span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>，<span class="number">32</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">32</span>，<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.layers(x)</span><br><span class="line">        x = x.sgueeze(<span class="number">1</span>)<span class="comment">#(B, l) -&gt; B</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment">#Training loop</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3000</span>):</span><br><span class="line">    model.train() <span class="comment"># Set your model to train mode.</span></span><br><span class="line">    <span class="comment"># tgdm is a package to visualize your training progress</span></span><br><span class="line">    train_pbar = tqdm(train_loader, position=<span class="number">0</span>, leave=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> train_pbar:</span><br><span class="line">        x, y = x.to(<span class="string">&#x27;cuda&#x27;</span>), y.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">        pred = model(x)</span><br><span class="line">        loss = criterion(pred, y)</span><br><span class="line">        loss.backward()<span class="comment"># Compute gradient(backpropagation).</span></span><br><span class="line">        optimizer.step()<span class="comment"># Update parameters.</span></span><br><span class="line">        optimizer.zero_grad() <span class="comment"># Set gradient to zero.</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/">PyTorch Documentation</a></p>
<p>广播机制(broadcasting mechanism):</p>
<ol type="1">
<li>通过适当复制元素来扩展一个或两个数组, 以便操作的不同张量具有相同的形状;<br />
</li>
<li>对生成的数组执行按元素操作</li>
</ol>
<p>例如，a 和 b 分别是 3 × 1 和 1 × 2 矩阵，广播会成为一个更大的 3 × 2 矩阵: 矩阵 a 将复制列, 矩阵 b 将复制行, 然后再按元素相加 广播机制有一些实用的技巧:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">A = torch.arange(<span class="number">20</span>, dtype=torch.float32).reshape(<span class="number">5</span>, <span class="number">4</span>) </span><br><span class="line">sum_A = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">A / sum_A <span class="comment">#sum_A 没有降维，仍然是个矩阵，可以通过广播将 A 除以 sum_A。</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">结果如下：</span></span><br><span class="line"><span class="string">tensor([[0.0000, 0.1667, 0.3333, 0.5000],  </span></span><br><span class="line"><span class="string">        [0.1818, 0.2273, 0.2727, 0.3182],</span></span><br><span class="line"><span class="string">        [0.2105, 0.2368, 0.2632, 0.2895],  </span></span><br><span class="line"><span class="string">        [0.2222, 0.2407, 0.2593, 0.2778],  </span></span><br><span class="line"><span class="string">        [0.2286, 0.2429, 0.2571, 0.2714]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">这是一种很方便的归一化操作</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h3 id="基础训练方法示例">基础训练方法示例</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_array</span>(<span class="params">data_arrays, batch_size, is_train=<span class="literal">True</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;构造一个PyTorch数据迭代器&quot;&quot;&quot;</span></span><br><span class="line">    dataset = data.TensorDataset(*data_arrays)</span><br><span class="line">    <span class="keyword">return</span> data.DataLoader(dataset, batch_size, shuffle=is_train)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">data_iter = load_array((features, labels), batch_size)</span><br></pre></td></tr></table></figure>
<p>使用 <code>iter</code> 构造 Python 迭代器，并使用 <code>next</code> 从迭代器中获取第一项</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">next</span>(<span class="built_in">iter</span>(data_iter))</span><br></pre></td></tr></table></figure>
<pre><code>[tensor([[-0.0714, -1.8597],
         [-0.4744,  0.4050],
         [ 0.2402,  0.5660],
         [ 1.6367, -0.9899],
         [ 0.6723,  0.1904],
         [ 0.5322, -0.4337],
         [-0.5749,  0.6719],
         [-0.0317,  1.3456],
         [ 1.0865, -1.3968],
         [-0.0130, -0.9245]]),
 tensor([[10.3747],
         [ 1.8749],
         [ 2.7762],
         [10.8325],
         [ 4.9005],
         [ 6.7224],
         [ 0.7743],
         [-0.4169],
         [11.1058],
         [ 7.3157]])]</code></pre>
<p><code>Sequential</code> 类将多个层串联在一起, 并自动让其前向传播<br />
在 PyTorch 中，全连接层在 <code>Linear</code> 类中定义。<br />
值得注意的是，我们将两个参数传递到 <code>nn.Linear</code> 中，第一个指定输入特征形状，即 2，第二个指定输出特征形状，输出特征形状为单个标量，因此为 1。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nn 是神经网络的缩写</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>可以直接访问参数以设定它们的初始值，如通过 <code>net[0]</code> 选择网络中的第一个图层，然后使用 <code>weight.data</code> 和 <code>bias.data</code> 方法访问参数。<br />
我们还可以使用替换方法 <code>normal_</code> 和 <code>fill_</code> 来重写参数值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">0</span>].weight.data.normal_(<span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">net[<span class="number">0</span>].bias.data.fill_(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>损失函数与优化算法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)<span class="comment">#使用小批量随机梯度下降，第一个参数是优化对象，lr 则是学习率</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        l = loss(net(X) ,y)</span><br><span class="line">        trainer.zero_grad()</span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step()</span><br><span class="line">    l = loss(net(features), labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l:f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>epoch 1, loss 0.000247
epoch 2, loss 0.000110
epoch 3, loss 0.000110</code></pre>
<p>比较生成数据集的真实参数和通过有限数据训练获得的模型参数<br />
要访问参数，我们首先从 <code>net</code> 访问所需的层，然后读取该层的权重和偏置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w = net[<span class="number">0</span>].weight.data</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w的估计误差：&#x27;</span>, true_w - w.reshape(true_w.shape))</span><br><span class="line">b = net[<span class="number">0</span>].bias.data</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b的估计误差：&#x27;</span>, true_b - b)</span><br></pre></td></tr></table></figure>
<pre><code>w 的估计误差： tensor([0.0005, 0.0006])
b 的估计误差： tensor([0.0006])</code></pre>
<h2 id="线性模型">线性模型</h2>
<p>回归问题常常用若干输入产生一个连续值作为输出，线性回归（Linear Regression）和逻辑回归（Logistics Regression）是常见的线性模型</p>
<h3 id="线性回归">线性回归</h3>
<p>线性回归，即 y = wx + b , 是最简单的回归模型，但纯一次项的拟合能力较为受限，这种情况下就需要多项式回归<br />
我们将 w 视为权值向量，x 视为从一次 x'到 n 次 x'组成的向量，那么多项式模型依旧可以用原先的线性公式表示<br />
增加多项式的次数可以更好拟合训练集，但对测试集的效果就未必了，很容易出现过拟合问题, 如果出现，依旧需要正则化，增加数据数量或者维度等优化 线性模型优化是个纯粹的数学问题，其解析解在线代课上就会讲到，即：</p>
<p><span class="math display">\[\mathbf{w}^{*}=(\mathbf{X}^{\mathsf{T}}\mathbf{X})^{-1}\mathbf{X}^{\mathsf{T}}\mathbf{y}.\]</span></p>
<h4 id="python-实现">python 实现</h4>
<p>求梯度更像一个数学问题，这里就用 pytorch 的自动求导功能，实际上也可以自己通过计算图实现<br />
简单生成一个有噪声项 <span class="math inline">\(\epsilon\)</span> 的数据集，噪声项有标准差 0.01, 均值为 0 的正态分布生成</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w, b, num_examples</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成y=Xw+b+噪声&quot;&quot;&quot;</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, <span class="built_in">len</span>(w)))根据正态分布随机计算初始权值向量</span><br><span class="line">    y = torch.matmul(X, w) + b</span><br><span class="line">    y += torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape)</span><br><span class="line">    <span class="keyword">return</span> X, y.reshape(-<span class="number">1</span>,<span class="number">1</span>)<span class="comment">#-1 表示不固定形状，根据总元素数和其他维度数量计算该维度</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])<span class="comment">#w形状与 features 单行的形状相同</span></span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = synthetic_data(true_w, true_b, <span class="number">1000</span>)<span class="comment">#labels 被 reshape 为单列的向量</span></span><br><span class="line">labels.shape</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([1000])</code></pre>
<p>通过生成第二个特征 <code>features[:, 1]</code> 和 <code>labels</code> 的散点图， 可以直观观察到两者之间的线性关系。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d2l.set_figsize()<span class="comment">#控制图像大小</span></span><br><span class="line">d2l.plt.scatter(features[:, <span class="number">1</span>].detach().numpy(), labels.detach().numpy(), <span class="number">1</span>);<span class="comment">#detach() 用于从计算图中分离张量</span></span><br></pre></td></tr></table></figure>
<figure>
<img src="/assets/ml/output_8_0.svg" alt="" /><figcaption>svg</figcaption>
</figure>
<p>为了提高效率，设置一个划分小批量的工具函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    <span class="comment"># 这些样本是随机读取的，没有特定的顺序</span></span><br><span class="line">    random.shuffle(indices)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        batch_indices = torch.tensor(</span><br><span class="line">            indices[i: <span class="built_in">min</span>(i + batch_size, num_examples)])</span><br><span class="line">        <span class="keyword">yield</span> features[batch_indices], labels[batch_indices]</span><br></pre></td></tr></table></figure>
<p>定义模型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X, w, b</span>): </span><br><span class="line">    <span class="string">&quot;&quot;&quot;线性回归模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X, w) + b</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>): </span><br><span class="line">    <span class="string">&quot;&quot;&quot;均方损失&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.reshape(y_hat.shape)) ** <span class="number">2</span> / <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;小批量随机梯度下降&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param -= lr * param.grad / batch_size</span><br><span class="line">            param.grad.zero_()</span><br><span class="line"></span><br><span class="line">w = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(<span class="number">2</span>,<span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        l = loss(net(X, w, b), y)  <span class="comment"># X 和 y 的小批量损失</span></span><br><span class="line">        <span class="comment"># 因为 l 形状是(batch_size,1)，而不是一个标量。l 中的所有元素被加到一起， 并以此计算关于 [w, b] 的梯度</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()</span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用参数的梯度更新参数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        train_l = loss(net(features, w, b), labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>epoch 1, loss 0.030660
epoch 2, loss 0.000105
epoch 3, loss 0.000046</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;w的估计误差: <span class="subst">&#123;true_w - w.reshape(true_w.shape)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;b的估计误差: <span class="subst">&#123;true_b - b&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>w 的估计误差: tensor([ 0.0001, -0.0006], grad_fn = &lt;SubBackward0&gt;)
b 的估计误差: tensor([0.0004], grad_fn = &lt;RsubBackward1&gt;)</code></pre>
<h3 id="分类">分类</h3>
<p>分类问题最简单的解决方法莫过于对回归产生的结果进行筛选，一个区间对应一个类别，但这么做会很难处理区间的划分情况，因此需要其他的处理方法<br />
分类问题的损失函数与回归不同，可以单纯用分类错误率计算，常用模型有感知机、支持向量机等</p>
<h4 id="感知机">感知机</h4>
<p>感知机接收多个输入信号, 根据权重输出一个信号，例如 0 和 1<br />
如果我们用最简单的感知机(依旧输出一个连续值，通过激活函数产生分类)，那么分类任务的重点就是找的一个合适的门槛值 threshold<br />
需要注意的是，感知机本质上是个线性的界限，通过权重向量和偏置值划分不同的输入，设想这样的情况：</p>
<ol type="1">
<li>有多种输入需要分成两类</li>
<li>其中一类有两个输入连成直线 L1, 另一类中有两个输入可以连成直线 L2</li>
<li>如果 L1 和 L2 相交，那么我们不可能在中间画一条线把两个直线分开(证明就不管了)</li>
</ol>
<p>事实上，例如异或门就无法通过感知机实现，因为我们要分开(1,0)(0,1)以及(0,0)(1,1)，这两类的连线相交，准确地说，这是说单层感知机，因为曲线就可以划分这两类，也就是“<strong>单层感知机无法分离非线性空间</strong>”<br />
例如，对异或门这个问题。加一层神经就能将分界线拓展为抛物线，也就是次数+1，这样就能进行非线性划分(多层感知机) 此外，也可以通过加一层 feature 转化层，将原来的 x 映射为可以被线性划分的 x'</p>
<div class="note info"><p>python 中定义阶跃函数(输入超过阈值, 就切换输出):</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">step_function</span>(<span class="params">x</span>): </span><br><span class="line">    y=x&gt;<span class="number">0</span> <span class="keyword">return</span> <span class="comment">#产生 bool 数组</span></span><br><span class="line">    y.astype(np.<span class="built_in">int</span>) <span class="comment"># 将 bool 数组转换为 1 与 0 的 int 数组</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">step_function</span>(<span class="params">x</span>): </span><br><span class="line">    <span class="keyword">return</span> np.array(x &gt; <span class="number">0</span>, dtype=np.<span class="built_in">int</span>) </span><br><span class="line"></span><br><span class="line">x = np.arange(-<span class="number">5.0</span>, <span class="number">5.0</span>, <span class="number">0.1</span>) </span><br><span class="line">y = step_function(x) </span><br><span class="line">plt.plot(x, y) </span><br><span class="line">plt.ylim(-<span class="number">0.1</span>, <span class="number">1.1</span>) <span class="comment"># 指定 y 轴的范围 </span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</div>
<h4 id="逻辑回归discriminative-model">逻辑回归(discriminative model)</h4>
<p>逻辑回归的目标是预测一个二元变量（例如，0 或 1、是或否）。通过逻辑函数（sigmoid 函数）将线性组合的输入转换为概率值<br />
相比线性回归，其最大的特点是输出在 0-1 之间，可以理解为概率，一般用于处理分类（二分）问题</p>
<p>经过简单（并不）的梯度运算，可以得到常用 loss 函数的梯度为：</p>
<ol type="1">
<li>交叉熵: <span class="math inline">\(\sum_{n}-{\bigg(}{ {\hat{y}^{n}-f_{w,b}(x^{n})} { } } {\bigg)}x_{i}^{n}\)</span></li>
<li>均方根: <span class="math inline">\(2(f_{w,b}(x)-\hat{y})f_{w,b}(x)\left(1-f_{w,b}(x)\right)x_{i}\)</span></li>
</ol>
<p>这样一来就有个问题，均方根的梯度在损失较大和较小时都很小，只有交叉熵符合远更新快近更新慢的条件</p>
<h4 id="基于概率的分类方法generative-model">基于概率的分类方法(generative model)</h4>
<p>如果有两个类 c1, c2 用于分类，抽到一个样本 x, 这就像高中数学地抽小球问题，随机抽个样本是某类小球的概率取决于在抽取的黑盒子里不同类别的分布。<br />
此时我们的目的是根据参数预测分类，也就是说不同类别的分布只能猜想，于是假设在最简单的两个参数情况下，概率密度函数满足基于这两个参数的高斯(正态)分布，训练时，我们希望分别通过样本得到两类各自的分布情况，也就是这个高斯分布的均值和协方差矩阵</p>
<div class="note info"><p>极大似然估计可以理解为利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值，例如抽球(放回式)问题中抽一百次，七十次是白球, 三十次为黑球，若抽到白球的概率是 p，这个结果的概率是 p<sup>70(1-p)</sup>30，符合直觉的猜想是 p = 0.7，这是因为我们下意识用了极大似然估计。<br />
要令出现此情况的概率最大，只需要求导算一次极值就会得到 p = 0.7，由此产生了一个估计</p>
</div>
<p>使用极大似然估计，类似抽球问题，得到目前结果的概率其实结果所有取样点概率的积，省略怎么计算，最后我们能得到两组(μ, ∑)<br />
此时模型确定了，我们可以得到 P(x|Ci), i = 1, 2，这是种先验概率（Prior Probability），通过贝叶斯公式就能算出后验概率（Posterior Probability）：P(Ci|x)</p>
<p><span class="math display">\[ P(A | B) = \frac{P(B | A) \cdot P(A)}{P(B)} \]</span></p>
<p>这么一来，分类就可以根据最大后验概率对应的种类来选<br />
优化：<br />
为了避免过拟合可以统一 ∑，模型概率变为： <span class="math display">\[\operatorname{L}(u^{1},\vert u^{2},\Sigma)=\prod_{i=1}^{79}f_{\mu^{1},\Sigma}(x^{i})\times\prod_{j=1}^{61}f_{\mu^{2},\Sigma}(x^{79+j})\]</span></p>
<p>协方差矩阵可以直接按样本数量加权和<br />
统一协方差矩阵后，其实如果进行化简会发现此时依旧是一个线性模型，即 wx+b 形式的模型</p>
<div class="note info"><p>在统计学中，方差是用来度量单个随机变量的离散程度，而协方差则一般用来刻画两个随机变量的相似程度<br />
<img src="https://www.zhihu.com/equation?tex=%5Csigma_x%5E2%3D%5Cfrac%7B1%7D%7Bn-1%7D%5Csum_%7Bi%3D1%7D%5En%5Cleft%28x_i-%5Cbar%7Bx%7D%5Cright%29%5E2&amp;consumer=ZHI_MENG" /> <img src="https://www.zhihu.com/equation?tex=%5Csigma%5Cleft%28x%2Cy%5Cright%29%3D%5Cfrac%7B1%7D%7Bn-1%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Cleft%28x_i-%5Cbar%7Bx%7D%5Cright%29%5Cleft%28y_i-%5Cbar%7By%7D%5Cright%29&amp;consumer=ZHI_MENG" /></p>
</div>
<p>以上两种 model，其实逻辑回归的准确率在宝可梦数据集上略好于概率分布，这可能是因为概率模型会预先假设数据符合一种概率分布，因此概率模型相对更适合模型数量少或者有一定 noise 的情况</p>
<h4 id="softmax">softmax</h4>
<p>前两个模型考虑了二分的情况，那么多个类别呢？这时仍然需要模型输出概率，但是概率总和需要小于等于 1，然后选一个最高的作为输出；问题变成了如何得到这样的概率，可以用 <strong>softmax</strong><br />
对每个输出值 <span class="math inline">\(o_j\)</span> 预测概率值 <span class="math inline">\(\hat y_j\)</span> 可以这么得到：<br />
<span class="math display">\[ {\hat{y} }_{j}={\frac{\exp(o_{j})}{\sum_{k}\exp(o_{k})} } \]</span><br />
<span class="math inline">\(\hat y_j\)</span> 可以视为对给定任意输入 x 的每个类的条件概率，即 P(y = 某类 | x)<br />
整个数据集的条件概率为：<br />
<span class="math display">\[P({\bf Y}\mid{\bf X})=\prod_{i-1}^{n}P({\bf y}^{(i)}\mid{\bf x}^{(i)}).\]</span><br />
根据最大似然估计, 我们最大化 P(Y|X), 相当于最小化所有子概率的负对数和<br />
<span class="math display">\[ -\log P(\mathbf{Y}\mid\mathbf{X})=\sum_{i=1}^{n}-\log P(\mathbf{y}^{(i)}\mid\mathbf{x}^{(i)})=\sum_{i=1}^{n}l(\mathbf{y}^{(i)},{\hat{\mathbf{y} } }^{(i)}) \]</span><br />
其中的损失函数是交叉熵<br />
<span class="math display">\[l({\bf y},\hat{\bf y})=-\sum_{j=1}^{q}y_{j}\log\hat{y}_{j}.\]</span> 

$$\begin{align}
l({\bf y},\hat{\bf y})=  -\sum_{j = 1}^{q}y_{j}\log{\frac{\exp(o_{j})}{\sum_{k = 1}^{q}\exp(o_{k})}}   \\ 
=\sum_{j = 1}^{q}y_{j}\log\sum_{k = 1}^{q}\exp(o_{k})-\sum_{j = 1}^{q}y_{j}o_{j} \\
=\log\sum_{k = 1}^{q}\exp(o_{k})-\sum_{j = 1}^{q}y_{j}o_{j}.

\end{align}$$

$$ \partial_{o_{j}l}({\bf y},\hat{\bf y})=\frac{\exp(o_{j})}{\sum_{k=1}^{q}\exp(o_{k})}-y_{j}=\mathrm{sofmax}({\bf o})_{j}-y_{j} $$

</p>
<p>softmax 是一个非线性函数, 但 softmax 回归的输出仍然由输入特征的仿射变换(保持点、直线和面之间相对关系的变换)决定，因此, softmax 回归是一个线性模型<br />
看上去很巧，导数就是 softmax 模型分配的概率与实际发生的情况(由独热标签向量表示)之间的差异 <a href="./#分布">数学原理</a></p>
<h3 id="优化技巧">优化技巧</h3>
<p>训练效果取决于很多因素，常见的排查思路有：</p>
<ul>
<li>Model Bias: 训练数据有一定倾向性，实际的 function set 过于小以致于没有理想的函数，此时可能需要增加参数或者增加数据量</li>
<li>局部最小值和鞍点: 可以用泰勒公式估算附近的函数值，这两种点的梯度(一阶导)都是 0，区别在于二阶导数，如果恒非正/非负，就是极值，否则就是鞍点
<ul>
<li>令附近点与求导的点之间的向量为 v, 泰勒公式的二阶项可以写成 <span class="math inline">\(v^THv\)</span> 的形式(H 是对各个 w 的二阶导数项组成的句子)，用线代的指示，该式恒非正/非负等价于 H 的特征值恒非正/非负
<ul>
<li>这样一来，对鞍点，设 H 的特征向量为 u， <span class="math inline">\(u^THu = \lambda||u||^2\)</span> , 沿着为负的特征向量方向就能继续下降</li>
</ul></li>
<li>另一种思路是所谓的动量，动量本质上是之前的梯度的加权和，类比物理上的动量，能一定程度上保持训练整体的倾向, 将其与当前梯度相加, 能一定程度上帮助跳出局部最低点</li>
</ul></li>
<li>有时静态的更新很难达到最低点(更新快会遇到不同参数收敛速度不同导致的震荡，慢会龟速爬行)，因此需要动态的更新机制: <span class="math inline">\(\theta_{i}^{t+1}\leftarrow\theta_{i}^{t}-\frac{\eta}{\sigma_{i}^{t} }\,g_{i}^{t}\)</span> 其中 <span class="math inline">\(\sigma_{i}^{t}=\sqrt{\frac{1}{t+1} \sum_{i=0}^{t}(g_{i}^{t})^{2} }\)</span> <span class="math inline">\(g_{i}^{t}=\frac{\partial L}{\partial\theta_{i}}|_{\theta=\theta^{t}}\)</span> (adagrad)
<ul>
<li>但依然有问题: 同一个参数在不同的取值范围内收敛速度也不同，因此需要进一步的动态机制(RMS Prop): <span class="math inline">\(\theta_{i}^{t+1}\leftarrow\theta_{i}^{t}-\frac{\eta}{\sigma_{i}^{t}}\,g_{i}^{t}\quad\sigma_{i}^{t}=\sqrt{\alpha\!\left(\sigma_{i}^{t-1}\right)^{2}+(1-\alpha)\!\left(g_i^{t}\right)^{2}}\)</span> 从而让最近的梯度影响更大</li>
<li>著名的 adam 就是同时用了 <code>momentum</code> 和 <code>RMS Prop</code>, 动量与 <span class="math inline">\(\sigma\)</span> 的不同是: 动量考虑方向, 而后者只考虑大小</li>
</ul></li>
<li>上述方法还是会有震荡, 只不过震荡会最后收敛, 因此考虑能否动态调整学习率
<ul>
<li>最常见的做法是学习率衰减 <code>learning rate decay</code></li>
<li><code>warm up</code>, 也就是让学习率先快后慢, 很难解释, 但先快后慢的 warmup 在很多知名模型里表现很好</li>
</ul></li>
</ul>
<p>一般来说, adam 比 SGDM(使用动量的 sgd)速度快, 但最后收敛的效果差一点, 这两者可以说是两个极端, 常用的优化策略会在两者间折中或者微调, 但可解释性嘛, 都很难说 以下简单的记录一些相关方法:</p>
<p>adam 部分:</p>
<ul>
<li>SWATS: 朴素的方法, 先用 adam 快速逼近目标, 再用 sgdm 慢慢收敛</li>
<li>AMSGrad: 某些局部的梯度可能会超过几个数量级的高, 因但由于 adam 算的是平方均根, 这个局部梯度影响会非常有限,(例如 a 比 b 小 100 倍, 平方根只差 10 倍,100 个 a 的梯度却大于一个 b 的梯度) 因此可以记住一个历史最大值, 从而相对扩大大梯度的影响力, 这样的话连续遇到小梯度学习率不会减少, <span class="math inline">\({ {\theta_{t}=\theta_{t-1}-\frac{p}{\sqrt{\hat{v}_{t}+\varepsilon}}m_{t}\space\space } } { {\hat{v}_{t}=\operatorname*{max}(\hat{v}_{t-1},v_{t})} }\)</span> 但这其实没有解决 adagrad 学习率不断衰减的问题, 只是延缓了</li>
<li>AdaBound: 给 learning rate 设置上下限(clip 函数), 简单粗暴, 可解释性也很迷</li>
<li>cyclicalLR: 让 lr 周期性波动, 简单的线性波动或者 cos 函数都可以, lr 大是探索性的, 小是寻找收敛点, 类似的有 <code>SGDR</code>, <code>One-cycle LR</code> 等</li>
</ul>
<p>RAdam: 用于解决训练初期梯度方差大, 先用 sgdm 积累足够的样本, 再转到类似 adam 的方法, 同时让非初期的梯度有更高影响力<br />
假设梯度是取样于一种分布, 因此参数只和取样次数 t 有关, 学习率满足以下条件, 其中 rt 是恒增的</p>
<p><span class="math display">\[\begin{array}{c}{ {\rho_{t}=\rho_{\infty}-\frac{2t\beta_{2}^{\ t} }{1-\beta_{2}^{\ 2} } } }\\ { {\rho_{\infty}=\displaystyle\frac{2}{1-\beta_{2}^{\prime} }-1} }\\ r_{t}={\sqrt{\frac{(\rho_{t}-4)(\rho_{t}-2)\rho_{\infty} } {(\rho_{\infty}-4)(\rho_{\infty}-2)\rho_{t} } } } \end{array}\]</span></p>
<p><span class="math display">\[\theta_{t}=\theta_{t-1}-\eta\hat{m}_{t} \space when \space 𝜌𝑡 ≤ 4\]</span></p>
<p><span class="math display">\[\theta_{t}=\theta_{t-1}-{\frac{\eta r_{t} }{ {\sqrt{\hat{v} }_{t}+\varepsilon} } }\,{\hat{m} }_{t} \space when \space 𝜌𝑡 &gt; 4\]</span></p>
<p>这是一个比较保守的策略, 防止太过激进的学习</p>
<p>动量相关:</p>
<ul>
<li>Nesterov accelerated gradient (NAG): 与普通动量法区别是, 用动量来预测下一个参数位置, 通过预测位置的梯度更新参数 <span class="math inline">\(m_{t}={\lambda}m_{t-1}+\eta\nabla L(\theta_{t-1}-\lambda m_{t-1})\)</span></li>
</ul>
<p>其他:</p>
<ul>
<li>Lookahead: 一种很抽象的方法, 不管用什么优化方法, 每轮中走 k 步到一个理论上的终点, 在起点和终点间找一个点作为实际终点<br />
</li>
<li>Shuffling, Dropout, Gradient noise: 这些都是增加随机性的方法</li>
<li>Warm-up, Curriculum learning(先学容易的数据), Fine-tuning(使用预训练的模型)</li>
</ul>
<p>经验上, cv 用 sgdm 多一点; nlp, gan 用 adam 多一点</p>
<h3 id="normalization">Normalization</h3>
<p>为了处理不同维度上输入规模不一的问题, 需要归一化<br />
Feature Normalization: 我们把统一维度的 x 输入视为正态分布的, 令 <span class="math inline">\(\widetilde{x}_{i}^{r}\leftarrow{\frac{x_{i}^{r}-m_{i} }{\sigma_{i}} }\)</span> 其中 m 为平均数, 𝜎 是标准差<br />
当然也可以对与 w 的加权和 z 向量做归一化: <span class="math inline">\({\tilde{z}}^{i}=\frac{z^{i}-\mu}{\sigma}\)</span><br />
然后令 <span class="math inline">\(z^{i}=\gamma\Theta\widetilde{z}^{i}+\beta\)</span> , 其中 γ 初始为 1, β 初始为 0, 这两个是学习参数, 用于在之后调整分布<br />
这样的归一化计算量较大, 实际中一般只对 batch 做归一化</p>
<p><img src="/assets/ml/Pasted%20image%2020250902150544.png" /></p>
<h2 id="卷积神经网络-cnn">卷积神经网络 CNN</h2>
<p>名词解释:</p>
<ul>
<li>Receptive Field（感受野）: 字面意思, 就是神经元的视野, cv 中我们希望神经元各自只捕捉一个局部特征, 一般感受野会是方形矩阵
<ul>
<li>最常见的 rf 是覆盖所有维度(channel)的, 也就是所有色彩空间, 常见的 rf 是 3×3, 且会有复数的神经元</li>
<li>共享参数: 即两个神经元用相同的参数, 常见的共享方法是: 每个 rf 有 n 组神经元, 不同的 rf 相同组序号的神经元共享参数; 也可以理解为用不同的 filter 矩阵(也就是共享参数的神经元)做卷积</li>
</ul></li>
<li>stride(步幅): 感受野之间的步幅, 例如一个 rf 对上的最左上元素是(i, j), 下一个对上的左上元素就是(stride+i, j)</li>
<li>padding: 由于内核矩阵以及步幅未必会让最后一步正好够运算, 有时需要填充若干行/列, 最常用的是直接填 0</li>
<li>pooling(池化): 在不改变关键信息的前提下尽可能简化输入规模, 例如对规模是 m×m 的矩阵, 对每个 n×n 的子矩阵取一个最大值, 最后得到边长 m/n 的方阵</li>
<li>flatten: 将最后的结果拉长为一维向量, 用于之后的模型学习</li>
</ul>
<p>基于以上限制, cnn 的 <code>model bias</code> 其实相对较大, 但在影像辨识中不是坏事</p>
<h3 id="spatial-transformer-layer">spatial transformer layer</h3>
<p>鉴于 CNN 的特点, 它对图片缩放, 旋转, 镜像后的数据, 不会依旧保有识别能力.当然想处理这个问题很简单, 把经过变化的图片也塞进训练集就可以了, 但是这样会严重影响训练效率, 而更好的做法是, 增加一个图片变化层, 用于转化图片到训练集的对应数据<br />
对图片的每个像素, 一个 2×2 的权重矩阵加一个 2 维的偏移向量就可以得出其对应像素, 而对于非整数的对应坐标, 出于可微性考虑, 将其与周边点的距离积为权值乘以周边点的值, 最后相加, 如下所示:<br />
<span class="math display">\[\begin{array}{l}{ {a_{22}^{l}=(1-0.4)\times(1-0.4)\times{}(1-0.4)\times a_{22}^{l-1} } }\\ { {+(1-0.6)\times(1-0.4)\times(1-0.6)\times a_{12}^{l-1} } }\\ { {+(1-0.6)\times(1-0.6)\times(1-0.6)\times a_{23}^{l-1} } } \\ { +(1-\,0.4)\times(1-0.6)\times a_{23}^{l-1} } \end{array}\]</span> 由于这样的取值是可微的, 也就可以用梯度下降进行学习, 将其嵌入神经网络中则可提高对变化后图片的识别效果</p>
<h2 id="self-attention">self-attention</h2>
<p>sequence labeling: 即为输入序列中的每个元素分配一个标签, 例如 nlp 中标注词性.这种问题的难点在于: 由于输入的向量长度不定, 难以确定应该用什么规格的网络, 于是需要注意力这种机制来让一个输入向量 a 能获取一些序列中的上下文信息形成向量 b 再进入网络<br />
那么怎么让 a 携带上下文信息呢, 常见的思路有做加权和(additive)或者加权积(dot-product), 后者更常用, 也就是输入向量各种乘以一个(互不相同的)权重, 然后乘在一起<br />
s-a 层所做的运算如下图:</p>
<p><img src="/assets/ml/Pasted%20image%2020250831171404.png" /></p>
<p>实际上, 该层学习的就是三个权值矩阵</p>
<p>相关技术:</p>
<ul>
<li>Multi-head Self-attention: 简单地说就是用两个独立的注意力神经, 其产物算加权和作为最终输出</li>
<li>Positional Encoding: 为了弥补注意力没有位置信息的问题, 最早的处理是对输入 ai 加上一个 ei 偏置, 后续发展中有不同的添加位置信息的方法</li>
<li>Truncated Self-attention: 对语音这种规模很大的输入, 出于效率考虑, 可以减少注意力计算的范围, 只看很小的上下文</li>
<li>CNN: cnn 可以视为注意力的一种特例</li>
<li>RNN: 相比 rnn, 注意力在并行性, 以及对上下文信息的利用能力上都相对更好</li>
</ul>
<h3 id="变形金刚">变形金刚</h3>
<p>transformer, 下文简称 tf, 是一种 seq2seq 模型, 这种模型有相当广泛的应用, 语音辨识/合成, 句法分析, 目标检测, 以及现在热门的对话生成都可以使用</p>
<p><img src="/assets/ml/seq2seq_v9.png" /></p>
<p>tf 的架构相当复杂, 这里简单地描述一下: 其主要用 en/decoder 组成, encoder 中有很多 <code>block</code> 用于生成中间向量, 每层 <code>block</code> 先做一次注意力(结果向量加上输入向量, 这叫做 <code>residual connection</code>, 随后做一次 <code>layer normalization</code>), 再用全连接(fc)网络计算, 这个 fc 网络也会用 <code>residual</code><br />
<code>decoder</code> 除了接受 <code>encoder</code> 数据外, 还要接受一个表示开始产生输出的符号(bos), 这个符号可以用 one-hot 表示, 类似 rnn, dec 不断把自己的输出当做下一轮的输入, 如果输出一个终止符来结束, 就叫 <code>Autoregressive</code>; 而 <code>Non-autoregressive</code> 则一次生成所有输出, 一个输入对应一个输出, 为此需要一个分类器来产生长或者一个足够长的默认长度, 让机器自己选一个槽位输出终止符, 这样的好处是并行化且容易控制长度, 缺点则是表现差<br />
关于 dec 的架构, 观察其与 enc 不同, 首先是注意力层多了 masked 前缀, 这个掩码就是让所有输入维度的注意力只能注意自己极其以前的输入(由于 decoder 输出有顺序, 这样很符合直觉)<br />
关于两者的交互, 看下面的图比较直观, 不太准确的说就是同时用双方的数据不断地做 masked 注意力(输出会不断作为下一轮输入加进来), 这叫做 <code>Cross Attention</code><br />
原始论文中 dec 不断从 enc 的最后一层拿数据, 但也有论文会对应着拿<br />
<img src="/assets/ml/seq2seq_v9_1.png" /></p>
<p>相关技术:</p>
<ul>
<li>Teacher Forcing: 简单地说就是训练时直接将答案作为 dec 输入</li>
<li>Copy Mechanism: 从输入中复制文字给输出用的能力</li>
<li>Guided Attention: 对一些规则严格的场景, 可以直接对训练中的模型加以限制, 例如语音合成</li>
<li>Beam Search: 一种常用于序列预测任务的搜索算法, 能在一定程度上预测相对更优的序列/路径</li>
<li>exposure bias: dec 辨识错误的能力差, 导致一步错步步错, 解决方法是增加一些 Noise</li>
<li>Scheduled Sampling: 在训练过程中, 逐步减小使用真实目标序列的概率</li>
</ul>
<h3 id="bert">bert</h3>
<p>bert, 目前很火的预训练 seq2seq 模型, 是一种无监督学习模型, 也就是没有 label 数据, 其特点是训练中使用掩码数据(Masking Input), 也就是遮住输入的部分字词, 而加上遮住数据的完整输入就是我们希望 bert 能输出的结果, 而损失函数也可以比较方便地用交叉熵(可以理解为以所有字符为类别的分类问题)<br />
bert 是基于 tf 的, 其架构和 tf 类似, 区别就是用掩码机制来无监督学习, 由于不需要标注的数据集, bert 很容易得到规模非常庞大的数据, 因此有着很好的表现 除此以外有一些其他训练方法, 如 Next Sentence Prediction: 预测两个句子是否相接; Sentence order prediction: 判断句子的顺序关系<br />
在以下这些常见的一些基准测试中, bert 都有不俗的表现:</p>
<ul>
<li>Corpus of Linguistic Acceptability (CoLA)</li>
<li>Stanford Sentiment Treebank (SST-2)</li>
<li>Microsoft Research Paraphrase Corpus (MRPC)</li>
<li>Quora Question Pairs (QQP)</li>
<li>Semantic Textual Similarity Benchmark (STS-B)</li>
<li>Multi-Genre Natural Language Inference (MNLI)</li>
<li>Question-answering NLI (QNLI)</li>
<li>Recognizing Textual Entailment (RTE)</li>
<li>Winograd NLI (WNLI)</li>
</ul>
<p>尽管 BERT 的预训练是无监督的，但在特定下游任务（如文本分类、语法分析等）中(对这些下游任务来说, 可以简单地给 bert 接上分类或者线性模型)，BERT 可以进行微调, 这个过程是监督学习。微调阶段使用标注好的数据集，通过已知的标签来优化模型参数.所以如果想准确一点, 可以叫半监督学习(semi)<br />
bert 的优异性能常常被归因于注意力对上下文的捕获能力以及大量的训练资料, 但神奇的是用于做蛋白质分类效果也很好, 英语 Bert 用在中文上效果也很好, 这或许可以理解为这些有规律的编码作为语言其实在词义向量以及结构上有相似之处, 根据李老师自己的实验, 这种神奇的能力只会出现在足够大的训练集上</p>
<h2 id="rnn">RNN</h2>
<p>相关场景:</p>
<p>slot filling: 类似一个分类问题, 将给定输入向量(一句话)中的词语分类到特定的槽位去</p>
<p>rnn 用于解决输入向量间有顺序关系的问题, 普通的前馈网络所有输入的词语都是地位相同的, 因此很难捕捉文字的前后语义关系, 于是产生了 rnn 这种方法, 也就是把前面的计算结果作为之后的输入, 常见的类型有:</p>
<ul>
<li>简单 rnn
<ul>
<li>elman network: 先前的隐藏层计算结果存起来, 后面被下一个神经元的隐藏层调用</li>
<li>jordan network: 将前一个神经元的输出存起来, 被下一个神经元调用</li>
</ul></li>
<li>其他
<ul>
<li>Bidirectional RNN: 训练正向和反向的两个 rnn, 最后的输出算加权和</li>
</ul></li>
</ul>
<h3 id="long-short-term-memory-lstm">Long Short-term Memory (LSTM)</h3>
<p>简单地说就是用两个阀门控制是否存入或放出历史信息, 一个阀门控制是否遗忘已有的信息, 阀门的开闭让网络学习<br />
其训练过程相对来说比较繁琐, 还好李老师细心地做了流程图, 这里直接贴上来</p>
<div class="pdf-container" data-target="/assets/ml/RNN.pdf" data-height="800px"></div>
<p>LSTM 的缺点是过于复杂导致计算成本高, 因此有 Gated Recurrent Unit (GRU)这样的简化版本(三个 gate)</p>
<h3 id="问题">问题</h3>
<ol type="1">
<li><p>RNN 会复用之前的模型, 例如其权值 w, 这会导致层数上来后, 后面神经的权值会产生幂函数关系, 使 loss surface 非常陡峭<br />
更准确地说, 由于幂函数的特性, w 小会很容易梯度消失, w 大则非常陡峭, LSTM 可以一定程度上解决前一个问题, 因为它能存储历史信息更长时间<br />
而后一个问题, 工程上最实用的方法是 clip 设置上界</p></li>
<li><p>鉴于 rnn 的特性, 处理不定长的输入(向量)是很方便的, 但如何处理不定长的输出呢?<br />
例如语音识别, 对若干音频输入, 简单的想法是每个音频输出一个字符, 结果把每个输出的重复部分拿掉, 但如何处理叠词呢?<br />
可以用一个 φ 符号代表 null, 也就是分割符, φ 间的有意义输出作为识别结果, 下一个问题是, 音频可能切的很碎, 不能保证对应关系具体应该怎么排<br />
为此需要 <code>Connectionist Temporal Classification (CTC)</code> 简单地说就是穷举可能的排列(实际会用 dp 优化), 选取其中最多的一种(即概率最大的排列/对应关系)</p></li>
</ol>
<p>以上讨论的语音识别其实有一个预设--识别结果的字符数 ≤ 音频样本数, 对没有这种条件的问题, 例如机器翻译该怎么做呢?<br />
由于是循环的, rnn 可以不断地产生输出, 只需要一个特殊的表示结束的符号就可以, 例如 <code>===</code></p>
<h2 id="深度学习">深度学习</h2>
<p>Q: 为什么要深度, 为什么要用那么多隐藏层而不是一个很宽的单层网络?<br />
A: 深度学习能增加预测函数的弹性, 这是因为它可以复杂的不同线性关系去拟合数据, 那么为什么要用很深的网络?实际上, 相同神经数量且较浅的网络预测效果会不如 dl, 也就是 dl 能用相对更少的参数拟合数据, 因此更不容易 overfitting, 有更好的准确率; 而 dl 的这种高效其实类似编程中的依赖关系, 例如某个节点的后继节点都可以依赖于前一个节点, 而整段程序只需要保留这个被多重依赖的节点的一个副本, 节省大量空间, dl 中其实也可能存在对某个前继神经的依赖关系, 也就是 dl 是一个有结构上关系的网络</p>
<h3 id="概念">概念</h3>
<p>对于复杂的网络, 会使用神经网络块(block)来描述若干个网络层的组合, 一般来说, 块有自己的参数, 前向传播, 反向传播函数, 这是一个逻辑概念, torch 中可以用模块或者 seq 来实现</p>
<div class="note info"><ul>
<li><code>torch.nn.Module</code>: Base class for all neural network modules.Your models should also subclass this class.即所有模型的基类</li>
<li><code>torch.nn.Sequential</code>: Modules will be added to it in the order they are passed in the constructor. 有顺序的 <code>module</code> 的容器, 与 <code>ModuleList</code> 的是它提供对内置模块的顺序调用, 也就是已经实现了前向传播, 因此它很适合用来定义一个 block</li>
<li><code>torch.nn.ModuleList</code>: Holds submodules in a list. 模块的 list, 和普通的 list 没什么区别, 有索引顺序, 但并没有逻辑上的顺序</li>
</ul>
</div>
<h1 id="技术">技术</h1>
<h2 id="扩散模型">扩散模型</h2>
<h3 id="vae-变分自编码器">VAE 变分自编码器</h3>
<p>对于生成任务，有一套直白的思路： 对一群数据提取特征，编码为一种分布，然后在生成时从分布中采样作为解码<br />
若输入的数据为 x, 编码出的隐式数据为 x, p(z|x)称为后验概率，也就是给定发生了什么得到的估计；p(x|z)称为似然，也就是根据已有知识，对应该发生什么的估计<br />
现在的问题是，如何得到一个性质良好的 z，这个良好指两方面:</p>
<ol type="1">
<li>解码(重建)时还原程度高</li>
<li>在通过对隐空间采样来反向预测 x 时也有良好的表现</li>
</ol>
<p>为此，VAE 用高斯分布建模一个隐式空间，且定义其 loss 如下:</p>
<p><span class="math inline">\(\mathcal{L}(x)=\mathbb{E}_{q(z|x)}\left[\log p(x|z)\right]-{ { {\operatorname{KL}(q(z|x)\ |\ p(z))}}}\)</span></p>
<p>这个 loss 想实现的目的是，在分布散度差距和还原损失之间取得平衡<br />
我们要学习的参数是方差和均值，那么怎么做呢？这里引入一个 <span class="math inline">\(\epsilon\)</span> 表示对标准正态分布的随机采样，将其乘方差再加上均值，就得到了一个模拟的采样结果可用于求梯度</p>
<h3 id="flow-based-model">flow-based model</h3>
<h4 id="normalizing-flow">Normalizing flow</h4>
<p>主要参考李宏毅老师的视频以及一篇 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/718997970">博文</a></p>
<p>在数学上，生成模型其实生成的是概率分布，一般将输入数据视为对某种分布的采样，用来学习一种分布 G，最后再对学习的分布 G 随机采样来得到新的生成数据<br />
这种所谓的随机采样，可以理解为对一种分布例如常见的正态分布采样，将这个采样的数据映射到我们学习到的分布 G 上，我们希望 G 的分布和真实数据的分布(这个分布是假想的)越接近越好，例如上面的 VAE 就会不断约束 G 的重构损失和 KL 散度<br />
如果我们想直接优化分布之间的映射关系呢？</p>
<p>接下来需要引入一些前置知识:</p>
<div class="note info"><p><strong>Jacobian matrix</strong> 是一个数学概念，主要用于多变量微积分和向量微分。在函数的多变量情况下，Jacobian 矩阵描述了函数的局部线性近似<br />
对于一个从 <span class="math inline">\({R}^n\)</span> 到 <span class="math inline">\({R}^m\)</span> 的函数 ，其 Jacobian 矩阵 ( J ) 定义为函数在某一点的偏导数矩阵。矩阵的元素形式如下：</p>
<p><span class="math display">\[ \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_1}{\partial x_2} &amp; \cdots &amp; \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_2} &amp; \cdots &amp; \frac{\partial f_2}{\partial x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial f_m}{\partial x_1} &amp; \frac{\partial f_m}{\partial x_2} &amp; \cdots &amp; \frac{\partial f_m}{\partial x_n}
\end{bmatrix}
\]</span> 此外，逆函数的 J 矩阵也互逆</p>
<p>行列式学过线代的都知道，其几何意义可以粗略理解为一个任意维度空间中的，矩阵所描绘的几何体的 "体积"</p>
</div>
<p>那么，开始讨论概率分布的转化方法，给定随机变量 z，及其概率密度函数 π(z)，将其的分布转化为另一种分布实际上就是一种映射，设这个映射函数为 f, 即 <span class="math inline">\(x=f(z)\)</span> ，X 是转化出的新变量，其概率密度函数为 p(x)<br />
那么对这个映射，我们知道所有概率函数的积分总和为 1，也就是面积在映射中是不变的，那么基于常识让 <span class="math inline">\(p(x)dx=\pi (z)dz\)</span><br />
而对向量的映射，其实也类似，只不过 z 对应向量的微分乘积会对应到 x 对应向量矩阵的行列式<br />
代入和化简后得到:<br />
<span class="math display">\[p(x^{\prime})=\pi(z^{\prime})\left|\frac{1}{d e t(J_{f})}\right|\]</span> 由于互逆矩阵的 det 乘积为 1 且 J 矩阵的逆矩阵和对应逆函数的 J 矩阵相同, 也可以写成</p>
<p><span class="math display">\[p(x^{\prime})=\pi(z^{\prime}){\big|}d e t{\big(}J_{f^{-1}}{\big)}{\big|}\]</span></p>
<p>其实这个式子很直观，J 矩阵里是 x 对 z 的微分，那么其逆矩阵就是 z 对 x 的微分，那么其行列式可以理解为映射中，z 的“底面积 "对应 x”底面积" 的比例关系(而“高 " 就对应概率本身)<br />
基于这个数学式的生成模型称为 G，即 G 将 π(z)(通常是对正态分布取样)转化为一个概率分布 p(x), 可以形象地写作 <span class="math inline">\(x=G(z) \space | \space z=G^{-1}(x)\)</span></p>
<p>先前的表达式的对数形式为:<br />
<span class="math inline">\(l o g p_{G}(x^{i})=l o g\pi\left(G^{-1}(x^{i})\right)+l o g|d e t(J_{G^{-1}})|\)</span><br />
由于 π 一般是一个正态分布，第一项会倾向于在中间(标准正态分布就是 0 向量附近)取点，第二项表示的可以某种意义上理解为映射后微分空间的分散的程度的对数，如果总在中间取点，几乎没有分散，则对数是很小的负数，产生很大的惩罚; 因此这样的形式能一定程度上约束分布不要过于集中</p>
<p>目前为止我们得到了一个优雅的数学式，但与常见的 ml 模型不同的是，在这个情况下(需要 x, z 相互可逆)，x、z 有相同的长度，这产生了两个问题：</p>
<ol type="1">
<li>计算量大</li>
<li>对模型的约束较大，可能会降低泛化能力</li>
</ol>
<p>这就是为什么名字叫 flow 了，因为我们会用一串 G 来弥补模型弹性不足的问题, 为什么可以这样做？对 z 的变化可以说是链式的，在微分层面可以很直接地表示为多个 J 矩阵行列式对 p(z)连乘，用一个对数函数就可以转化为连加, 因此实际上用一个 G 和一连串 G 除了计算量没有本质区别，以下讨论一个 G 的情况<br />
对一个具体的 G，实际训练时考虑到计算量，会使用 <code>coupling layer</code> 这样的技术，它巧妙地牺牲一部分输入数据(不做变换仅复制)让涉及到的行列式和逆矩阵都很容易计算; 也可以用一对一的卷积，对其内核矩阵(涉及的论文里是 3×3)算行列式以及微分</p>
<p><img src="/assets/ml/image-4.png" /></p>
<h3 id="flow-matching">Flow Matching</h3>
<h4 id="数学-1">数学</h4>
<p>常微分方程: 形如 <span class="math inline">\({\frac{d y(t)}{d t}}\,=\,f(y(t),t)\)</span></p>
<p>其中, t 为时间，f 描述 y 与 t 的关系且可微分，一般来说 y 是需要求解的量<br />
例如如果已知一个初始值，可以写作: <span class="math inline">\(y(t_{1})=y_{t_{0}}+\int_{t_{0}}^{t_{1}}f(y(t),t)d t\)</span><br />
对复杂的神经网络，很多时候不能也没有必要求解析解，可以用一些数值方法估算：</p>
<ul>
<li><code>Euler Method</code>: 直接用区间面积的和估算，形如 <span class="math inline">\(y(t_{1})\approx y(t_{0})+\sum_{i=0}^{N-1}h f(y(t_{0}+i h),t_{0}+i h)\)</span> 其中 h 是区间宽度</li>
<li><code>Neural ODE</code>: 使用例如 <code>residual network</code> 之类的网络求解，这里不详细介绍</li>
</ul>
<h4 id="向量场与流">向量场与流</h4>
<p><a target="_blank" rel="noopener" href="https://xyfjason.top/blog-main/2024/06/22/Flow-Matching">参考博文1</a> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1946706315792619506">参考博文2</a></p>
<p>上文介绍的标准流是通过组合一系列的生成器获得表达能力的，而每个生成器通过微分矩阵来评估映射的性质，因此命名为 flow 很直观<br />
而对 flow 来说，想到向量也很直观，CNF 正是一种基于向量场的方法<br />
为什么要用向量场呢？想象分布 p 是一群点，当然它可以是无数个，这些点我们希望能都被一一映射到另一个分布里去，这就像初中物理课做的磁场实验，改变电磁铁的通电流向(磁性)，在磁场的作用下铁屑们就会纷纷自己组成另一种形状<br />
在这种场作用下的 <strong>概率分布 p</strong> 的运动轨迹成为概率密度路径，与之前不同的是运动需要时间，这里引入一个时间 t 变量，范围是 <code>[0,1]</code>, 此外概率函数自己的性质是 p(x)对 x 的积分必定是 1，即 <span class="math inline">\(\textstyle\int p_{t}(\mathbf{x})\mathrm{d}\mathbf{x}=1\)</span><br />
接下来定义向量场为: <span class="math inline">\(\mathbf{u}:[0,1]\times\mathbb{R}^{d}\to\mathbb{R}^{d}\)</span> u 也可以理解为瞬时速度<br />
那么 u 和 p 什么关系，作为计算机专业不用懂原理，但存在这么一个方程式:</p>
<p><span class="math display">\[\frac{\partial}{\partial t}p_{t}({\bf x})+\nabla\cdot(p_{t}({\bf x}){\bf u}_{t}({\bf x}))=0\]</span></p>
<p>需要注意概率密度路径与速度场不是一一对应的，不同的速度场可以产生相同的概率密度路径</p>
<p>接下来回归标题，标题的流 flow 正是运动时的轨迹，定义为: <span class="math inline">\(\frac{\partial}{\partial t}\phi_{t}({\bf x})={\bf u}_{t}(\phi_{t}({\bf x}))\)</span> , 其中 <span class="math inline">\(\phi _t(x)\)</span> 初值为 x 的粒子在 t 时刻运动到的位置, 也就是对其在 <code>[0,t]</code> 积分就会得到 t 点位置和初始位置的位移<br />
那么显然 p 和 <span class="math inline">\(\phi\)</span> 之间也有关系，这里之间放出来:</p>
<p><span class="math display">\[ p_t(\mathbf x)=p_0(\boldsymbol\phi_t^{-1}(\mathbf x))\left|\det\left[\frac{\partial\boldsymbol\phi_t^{-1}}{\partial\mathbf x}(\mathbf x)\right]\right| \]</span></p>
<p>这称为 push-forward 方程，其中 t 时刻位于 x 位置处的粒子在 0 时刻的出发位置是 <span class="math inline">\(\boldsymbol\phi_t^{-1}(\mathbf x)\)</span></p>
<p><img src="/assets/ml/relation.png" /></p>
<h4 id="连续归一化流-continuous-normalizing-flows-cnfs">连续归一化流 (Continuous Normalizing Flows, CNFs)</h4>
<p>终于说到生成模型了，令 <span class="math inline">\(p_t(x)\)</span> 为概率密度路径，p0 是一种简单分布(例如标准正态分布), p1 是输入数据的分布, <span class="math inline">\(u_t(x)\)</span> 为 p0 运动到 p1 对应的向量场，那么流匹配的模板就是用网络构建向量场 <span class="math inline">\(\mathbf v_{t}^{\theta}(\mathbf x)\)</span> 去近似 <span class="math inline">\(u_t(x)\)</span><br />
虽然一上来就定义了很多变量，但经过前文介绍，其实其目的很直观，接下来得到一个同样直观的损失函数:<br />
<span class="math display">\[\mathcal L_\text{FM}(\theta)=\mathbb E_{t,p_t(\mathbf x)}\left[\Vert\mathbf v_{t}^{\theta}(\mathbf x)-\mathbf u_t(\mathbf x)\Vert^2\right]\]</span></p>
<p>当然，真实速度场是未知的，这是生成模型的经典问题，同样有一个经典解决方案，也就是每次取一个样本去近似，不断更新训练，这就需要 <strong>条件速度场</strong><br />
给定某特定样本 x1 ，称 <span class="math inline">\(p_t(x|x1)\)</span> 为条件概率路径，含义为在终点为 x1 的前提下粒子的概率路径, t=0时<code>p(x|x1)=p(x)</code>, t=1时 <span class="math inline">\(p_{1}(x|x_{1})=\mathcal{N}(x|x_{1},\sigma_{m i n}^{2}I)\)</span> , 即此时粒子离x1极近</p>
<p><span class="math display">\[p_t(\mathbf x)=\int p_t(\mathbf x\vert\mathbf x_1)q(\mathbf x_1)\mathrm d\mathbf x_1=\mathbb E_{q(\mathbf x_1)}[p_t(\mathbf x\vert\mathbf x_1)]\]</span> <span class="math display">\[\mathbf u_t(\mathbf x)=\int \mathbf u_t(\mathbf x\vert\mathbf x_1)\frac{p_t(\mathbf x\vert\mathbf x_1)q(\mathbf x_1)}{p_t(\mathbf x)}\mathrm d\mathbf x_1=\mathbb E_{p(\mathbf x_1\vert\mathbf x)}[\mathbf u_t(\mathbf x\vert\mathbf x_1)]\]</span></p>
<p>使用这种条件向量场更新的流匹配称为 <strong>conditional flow matching</strong><br />
其损失函数为:</p>
<p><span class="math display">\[\mathcal L_\text{CFM}(\theta)=\mathbb E_{t,q(\mathbf x_1),p_t(\mathbf x\vert\mathbf x_1)}\left[\Vert\mathbf v_{t}^{\theta}(\mathbf x)-\mathbf u_t(\mathbf x\vert\mathbf x_1)\Vert^2\right]\]</span></p>
<p>接下来只剩下具体的条件向量场设计问题了，详见 <a target="_blank" rel="noopener" href="https://xyfjason.top/blog-main/2024/06/22/Flow-Matching/#%E6%9D%A1%E4%BB%B6%E9%80%9F%E5%BA%A6%E5%9C%BA%E7%9A%84%E8%AE%BE%E8%AE%A1">博文</a></p>
<p>由于这里面有相当多的公式推导，可理解不是那么强，这么简单总结一下作者想表达什么:<br />
<strong>使用条件向量以及上述的损失函数来更新模型理论上和不使条件的情况是一致的(即每次的参数更新梯度一致)</strong> 更好理解的推导可见以下视频</p>
<iframe src="//player.bilibili.com/player.html?isOutside=true&amp;aid=112692861864835&amp;bvid=BV1Wv3xeNEds&amp;cid=28321514161&amp;p=1&amp;autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true">
</iframe>
<p>接下来是我们最关心的问题：代码怎么写，换句话说，条件向量场 <span class="math inline">\(u_t\)</span> 以及条件概率路径 <span class="math inline">\(p_t(x|x1)\)</span> 怎么表示( <span class="math inline">\(q_{x1}\)</span> 可以采样), 理论上讲，可以有无数种方法，这里这说最常用的高斯分布<br />
对给定样本 x1, 我们希望其拥有的性质:</p>
<ol type="1">
<li>t = 0 时，p(x|x1)服从一个标准正态分布</li>
<li>t = 1 时，p(x|x1)服从一个均值 x1, 方差较小的正态分布</li>
</ol>
<p>令 <span class="math inline">\(p_{t}({\bf x}|{\bf x_{1}})={\cal N}\left({\bf x};\mu_{t}({\bf x_{1}}),\sigma_{t}^{2}({\bf x_{1}}){\bf I}\right)\)</span><br />
流可以写作: <span class="math inline">\(\psi_{t}({\bf x}) = \sigma_{t}({\bf x}_{1}){\bf x}+\mu_{t}({\bf x}_{1})\)</span></p>
<p>可能有读者能猜到，经典的扩散模型例如 DDPM 可以视为这种形式的某种特例,即: <span class="math display">\[u_t(x|x1) =  \frac{\alpha_{1-t}^{\prime}}{1-\alpha_{1-t}^{2}}(\alpha_{1-t}x-x_{1})\]</span></p>
<p>实际中更常用的是最优传输(线性表示):<br />
<span class="math display">\[\mu_t(\alpha)=t x_{1},\;\sigma_{t}(x)=1-(1-\sigma_{m i n})t\]</span><br />
<span class="math display">\[u_{t}(x|x_{1})={\frac{x_{1}-(1-\sigma_{m i n})x}{1-(1-\sigma_{m i n})t}}\]</span><br />
<span class="math display">\[\psi_{t}(x)=(1-(1- \sigma_{m i n})t)x+t x_{1}\]</span><br />
优化为:<br />
<span class="math display">\[\begin{array}{c}{ {u_{t}(\psi_{t}(x_{0};x_{1})|x_{1})=\frac{\mathrm{d}\psi_{t}(x_{0};x_{1})}{\mathrm{d}t}}}\\ { {=x_{1}-({\mathrm{1}}-\sigma_{m i n})x_{0}}}\end{array}\]</span></p>
<p>需要强调的是，最有传输理论上的“直线”更新路径只在给定采样x1的条件下成立，在真正生成数据时粒子依旧走的是曲线路径</p>
<div class="note info">
</div>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/" rel="tag"><i class="fa fa-tag"></i> 课程笔记</a>
              <a href="/tags/python/" rel="tag"><i class="fa fa-tag"></i> python</a>
              <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"><i class="fa fa-tag"></i> 人工智能</a>
              <a href="/tags/%E5%9B%BD%E7%AB%8B%E5%8F%B0%E6%B9%BE%E5%A4%A7%E5%AD%A6/" rel="tag"><i class="fa fa-tag"></i> 国立台湾大学</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
          </div>

        
  <div class="social-like a2a_kit a2a_kit_size_32 a2a_default_style">
    <a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a>
      <a class="a2a_button_facebook"></a>
      <a class="a2a_button_twitter"></a>
      <a class="a2a_button_wechat"></a>
      <a class="a2a_button_qzone"></a>
  </div>

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/thinklive/16959/" rel="prev" title="基于哈佛cs50的计算机通识笔记">
                  <i class="fa fa-angle-left"></i> 基于哈佛cs50的计算机通识笔记
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/thinklive/37185/" rel="next" title="强化学习笔记 all in one">
                  强化学习笔记 all in one <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2023 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">thinklive</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">605k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">36:41</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 技术支持
  </div><script defer src="/lib/three.js"></script><script defer src="/lib/lines.js"></script><script defer src="/lib/waves.js"></script>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.24/fancybox/fancybox.umd.js" integrity="sha256-oyhjPiYRWGXaAt+ny/mTMWOnN1GBoZDUQnzzgC7FRI4=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>


  <script src=""></script>
  <script src="/%5Bobject%20Object%5D"></script>
  <script src="/%5Bobject%20Object%5D"></script>
  <script src="/%5Bobject%20Object%5D"></script>
  <script src="/%5Bobject%20Object%5D"></script>


<script>
var options = {
  bottom: '64px', // default: '32px'
  right: 'unset', // default: '32px'
  left: '32px', // default: 'unset'
  time: '0.5s', // default: '0.3s'
  mixColor: '#fff', // default: '#fff'
  backgroundColor: '#fff',  // default: '#fff'
  buttonColorDark: '#100f2c',  // default: '#100f2c'
  buttonColorLight: '#fff', // default: '#fff'
  saveInCookies: true, // default: true,
  label: '🌓', // default: ''
  autoMatchOsTheme: true // default: true
}
const darkmode = new Darkmode(options);
darkmode.showWidget();
</script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>


  <script class="next-config" data-name="wavedrom" type="application/json">{"enable":true,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/wavedrom/3.3.0/wavedrom.min.js","integrity":"sha256-IRMDzTC+wK5stMucZ/XSXkeS5VNtxZ+/Bm8Mcqfoxdo="}}</script>
  <script class="next-config" data-name="wavedrom_skin" type="application/json">{"enable":true,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/wavedrom/3.3.0/skins/default.js","integrity":"sha256-fduc/Zszk5ezWws2uInY/ALWVmIrmV6VTgXbsYSReFI="}}</script>
  <script src="/js/third-party/tags/wavedrom.js"></script>

  <script src="/js/third-party/fancybox.js"></script>

  <script src="/js/third-party/pace.js"></script>

  <script src="/js/third-party/addtoany.js"></script>

  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":false,"archive":false,"delay":true,"timeout":3000,"priority":true,"url":"https://thinklive1.github.io/thinklive/48061/"}</script>
  <script src="/js/third-party/quicklink.js"></script>
<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

<script>
var options = {
  bottom: '64px',
  right: '32px',
  left: 'unset',
  time: '0.5s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: true,
  label: '🌓',
  autoMatchOsTheme: true
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
</script>
<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"thinklive1/blog_comments","issue_term":"pathname","theme":"photon-dark"}</script>
<script src="/js/third-party/comments/utterances.js"></script>
<script>
    const snowflakes = ["❄", "❄", "❆", "❅", "✥","❄", "❄", "❆", "❅", "✥","✻"];
    // 创建雪花
    function createSnowflake() {
        const snowflake = document.createElement("span");
        snowflake.classList.add("snowflake");
        const randomIndex = Math.floor(Math.random() * snowflakes.length);
        snowflake.textContent = snowflakes[randomIndex];
        
        // 起始位置
        /* 80%概率 生成在页面两侧 30% 的位置
        const probability = Math.random();
        let startPosition = Math.random() * 100;

        if (probability < 0.8) {
            startPosition = Math.random() < 0.5 ? Math.random() * 30 : (Math.random() * 30) + 70;
        }
        snowflake.style.left = `${startPosition}vw`;
        */
        snowflake.style.left = `${Math.random() * 100}vw`;
        snowflake.style.top = `-30px`;
        // 雪花大小与透明度
        const size = Math.random() * 18 + 10;
        snowflake.style.fontSize = `${size}px`;
        const opacity = Math.random() * 0.6 + (size > 18 ? 0.4 : 0);
        snowflake.style.setProperty("--opacity", opacity);
        // 动画持续时间
        const fallDuration = Math.random() * 10 + 10;
        // 旋转持续时间
        const rotateDuration = Math.random() * 3 + 1;

        snowflake.style.animationDuration = `${fallDuration}s, ${fallDuration}s`; // 向 CSS 添加淡出动画的持续时间
        // 横向幅度
        const translateX = (Math.random() * 500 - 200);
        snowflake.style.setProperty("--translateX", `${translateX}px`);
        // 纵向幅度
        snowflake.style.setProperty("--translateY", `${window.innerHeight}px`);

        document.body.appendChild(snowflake);
        // 移除雪花
        setTimeout(() => {
            snowflake.remove();
        }, fallDuration * 1000);
    }
    
    function snowfallAnimation() {
        // 载入时若边栏是隐藏状态则不加载雪花
        const sidebarnav = document.querySelector('.sidebar');
        const sidebarnavdisplay = window.getComputedStyle(sidebarnav).getPropertyValue('display'); 
        if (sidebarnavdisplay !== 'none') {
            createSnowflake();
        }
        setTimeout(snowfallAnimation, 500); // 生成速度，毫秒
    }
    snowfallAnimation();
function toggleMode() {
    console.log("change color!");
    const root1 = document.documentElement;

    // 检查当前 color-scheme
    const isLightMode = getComputedStyle(root1).getPropertyValue('--content-bg-color').trim() === '#fff';

    if (isLightMode) {
        // 切换到暗模式
        root1.style.setProperty('--content-bg-color', '#000');
        root1.style.setProperty('--text-color', '#fff');
        root1.style.setProperty('--highlight-background', '#444');
        root1.style.setProperty('--highlight-foreground', '#bbb');
        root1.style.setProperty('--btn-default-bg', '#777');
        root1.style.setProperty('--menu-item-bg-color', '#777');
        root1.style.setProperty('--note-warning-bg-color', '#777');
        root1.style.setProperty('--note-bg-color', '#555');
        root1.style.setProperty('--note-info-bg-color', '#777');
        root1.style.setProperty('--table-row-odd-bg-color', '#777');
        root1.style.transition = 'all 0.5s ease';

    }

    else {
        root1.style.setProperty('--content-bg-color', '#fff');
        root1.style.setProperty('--text-color', '#555');
        root1.style.setProperty('--highlight-background', '#eaeef3');
        root1.style.setProperty('--highlight-foreground', '#00193a');
        root1.style.setProperty('--btn-default-bg', '#fff');
        root1.style.setProperty('--menu-item-bg-color', '#f5f5f5');
        root1.style.setProperty('--note-warning-bg-color', '#fdf8ea');
        root1.style.setProperty('--note-bg-color', '#f9f9f9');
        root1.style.setProperty('--note-info-bg-color', '#eef7fa');
        root1.style.setProperty('--table-row-odd-bg-color', '#f9f9f9');
        root1.style.transition = 'all 0.5s ease';
    }
}

function DarkTrigger() {
    console.log('dark!!')
    let isDarkMode = getComputedStyle(document.documentElement).getPropertyValue('--content-bg-color').trim() === '#000';
    console.log(isDarkMode)
    if (isDarkMode) {
        // 切换到暗模式
        const warningNotes = document.querySelectorAll('.post-body .note.warning');
        // 修改背景颜色
        warningNotes.forEach(note => {
        note.style.background = '#666';
        });

        const infoNotes = document.querySelectorAll('.post-body .note.info');
        // 修改背景颜色
        infoNotes.forEach(note => {
        note.style.background = '#666';
        });
    }
}


</script>

 <!--js: 线条特效-->
  <script type="text/javascript" color="255,255,255" opacity='1' zIndex="-1" count="50" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>

<button style="background: #868686;
  width: 3rem;
  height: 3rem;
  position: fixed;
  border-radius: 50%;
  border: none;
  right: unset;
  bottom: 2rem;
  left: 2rem;
  cursor: pointer;
  transition: all 0.5s ease;
  display: flex;
  justify-content: center;
  align-items: center;" class="darkmode-toggle" role="checkbox" onclick="toggleMode()">🌓</button>

  <video autoplay loop muted playsinline style="position:fixed;top:50%;opacity: 0.8;left:50%;min-width:100%;min-height:100%;transform:translateX(-50%)translateY(-50%);z-index:-2;">
  <source src="/images/red.mp4" type="video/mp4">
<!-- hexo injector body_end start --><script src="/assets/mmedia/mmedia-loader.js"></script><!-- hexo injector body_end end --></body>
</html>
