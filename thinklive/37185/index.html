<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.24/fancybox/fancybox.css" integrity="sha256-vQkngPS8jiHHH0I6ABTZroZk8NPZ7b+MUReOFE9UsXQ=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"thinklive1.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"flat"},"fold":{"enable":true,"height":500},"bookmark":{"enable":true,"color":"#66CCFF","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":"utterances","storage":true,"lazyload":false,"nav":null,"activeClass":"utterances"},"stickytabs":true,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="cs285">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习笔记 all in one">
<meta property="og:url" content="https://thinklive1.github.io/thinklive/37185/index.html">
<meta property="og:site_name" content="thinklive">
<meta property="og:description" content="cs285">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-24.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-23.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-25.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-26.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-27.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-28.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-29.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-30.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-31.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-32.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-33.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/52243716-045d-4e27-ad0c-0e84593fee81.png">
<meta property="og:image" content="https://datawhalechina.github.io/easy-rl/img/ch2/2.10.png">
<meta property="article:published_time" content="2025-09-09T07:53:29.840Z">
<meta property="article:modified_time" content="2025-10-31T05:04:34.308Z">
<meta property="article:author" content="thinklive">
<meta property="article:tag" content="伯克利">
<meta property="article:tag" content="课程笔记">
<meta property="article:tag" content="python">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="国立台湾大学">
<meta property="article:tag" content="cs285">
<meta property="article:tag" content="强化学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://thinklive1.github.io/assets/ml/image-24.png">


<link rel="canonical" href="https://thinklive1.github.io/thinklive/37185/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://thinklive1.github.io/thinklive/37185/","path":"thinklive/37185/","title":"强化学习笔记 all in one"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>强化学习笔记 all in one | thinklive</title>
  







<script type="text/javascript" async src="/js/fairyDustCursor.js"></script>
<script type="text/javascript" async src="/js/tab-title.js"></script>
<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>
<script src="/js/tab-title.js"></script>

<!--pjax：防止跳转页面音乐暂停-->
<script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script>
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
  <script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>
<script>
const options = {
  bottom: '64px', // default: '32px'
  right: 'unset', // default: '32px'
  left: '32px', // default: 'unset'
  time: '0.5s', // default: '0.3s'
  mixColor: '#fff', // default: '#fff'
  backgroundColor: '#fff',  // default: '#fff'
  buttonColorDark: '#100f2c',  // default: '#100f2c'
  buttonColorLight: '#fff', // default: '#fff'
  saveInCookies: false, // default: true,
  label: '🌓', // default: ''
  autoMatchOsTheme: true // default: true
}

const darkmode = new Darkmode(options);
</script>
<!-- hexo injector head_end start --><script> let HEXO_MMEDIA_DATA = { js: [], css: [], aplayerData: [], metingData: [], artPlayerData: [], dplayerData: []}; </script><!-- hexo injector head_end end --><link rel="alternate" href="/atom.xml" title="thinklive" type="application/atom+xml">
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>
<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>


  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">thinklive</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">dirichlet library</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索 | search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-主页-|-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>主页 | home</a></li><li class="menu-item menu-item-标签-|-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签 | tags</a></li><li class="menu-item menu-item-分类-|-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类 | categories</a></li><li class="menu-item menu-item-归档-|-archive"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档 | archive</a></li><li class="menu-item menu-item-相册-|-photo"><a href="/photos/" rel="section"><i class="fa fa-camera fa-fw"></i>相册 | photo</a></li><li class="menu-item menu-item-留言-|-guestbook"><a href="/guestbook/" rel="section"><i class="fa fa-book fa-fw"></i>留言 | guestbook</a></li><li class="menu-item menu-item-感谢-|-thank"><a href="/thanks/" rel="section"><i class="fa custom thanks fa-fw"></i>感谢 | thank</a></li><li class="menu-item menu-item-游戏-|-game"><a href="/game/bad1.html" rel="section"><i class="fa fa-gamepad fa-fw"></i>游戏 | game</a></li><li class="menu-item menu-item-神龛-|-shrine"><a href="/cyberblog/" rel="section"><i class="fa fa-microchip fa-fw"></i>神龛 | shrine</a></li><li class="menu-item menu-item-资源地图-|-resourcemap"><a href="/webstack/" rel="section"><i class="fa fa-list fa-fw"></i>资源地图 | resourcemap</a></li><li class="menu-item menu-item-思维导图-|-mindmap"><a href="/mindmap/index.html" rel="section"><i class="fa fa-map fa-fw"></i>思维导图 | mindmap</a></li><li class="menu-item menu-item-网站地图-|-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>网站地图 | sitemap</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索 | search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">


<!--网易云音乐插件-->
<!-- require APlayer -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css">
<script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script>
<!-- require MetingJS-->
<script src="https://cdn.jsdelivr.net/npm/meting@2.0.1/dist/Meting.min.js"></script> 
<!--网易云playlist外链地址-->   
<meting-js
    server="netease"
    type="playlist" 
    id="2762741085"
    mini="false"
    fixed="false"
    list-folded="true"
    autoplay="false"
    volume="0.2"
    theme="#4c4c4c"
    order="random"
    loop="all"
    preload="auto"
    mutex="true">
    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5"><span class="nav-number">1.</span> <span class="nav-text">基础概念</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="nav-number">1.1.</span> <span class="nav-text">策略梯度</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E5%92%8C%E6%94%B9%E8%BF%9B"><span class="nav-number">1.1.1.</span> <span class="nav-text">问题和改进</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7"><span class="nav-number">1.1.2.</span> <span class="nav-text">重要性采样</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#actor-critic"><span class="nav-number">1.2.</span> <span class="nav-text">actor-critic</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#off-policy-a-c"><span class="nav-number">1.2.1.</span> <span class="nav-text">off-policy a-c</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9B%B4%E5%A5%BD%E7%9A%84-baseline"><span class="nav-number">1.2.2.</span> <span class="nav-text">更好的 baseline</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E4%BB%B7%E5%80%BC%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">1.3.</span> <span class="nav-text">基于价值的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%94%B6%E6%95%9B%E6%80%A7"><span class="nav-number">1.3.1.</span> <span class="nav-text">收敛性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%80%E9%99%90"><span class="nav-number">1.3.2.</span> <span class="nav-text">局限</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%94%B9%E8%BF%9B"><span class="nav-number">1.3.3.</span> <span class="nav-text">改进</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%9E%E7%BB%AD%E5%8A%A8%E4%BD%9C"><span class="nav-number">1.3.4.</span> <span class="nav-text">连续动作</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E8%BF%87%E7%A8%8B"><span class="nav-number">1.4.</span> <span class="nav-text">马尔可夫过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%B3%E7%AD%96"><span class="nav-number">1.4.1.</span> <span class="nav-text">决策</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%87%E4%BB%BD%E5%9B%BE"><span class="nav-number">1.4.2.</span> <span class="nav-text">备份图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%AD%E4%BB%A3"><span class="nav-number">1.4.3.</span> <span class="nav-text">迭代</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3"><span class="nav-number">1.4.3.1.</span> <span class="nav-text">策略迭代</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3"><span class="nav-number">1.4.3.2.</span> <span class="nav-text">价值迭代</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%8D%E6%A8%A1%E5%9E%8B%E6%96%B9%E6%B3%95"><span class="nav-number">1.5.</span> <span class="nav-text">免模型方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B"><span class="nav-number">1.5.1.</span> <span class="nav-text">蒙特卡洛</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86"><span class="nav-number">1.5.2.</span> <span class="nav-text">时序差分</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%90%86%E8%AE%BA%E9%97%AE%E9%A2%98"><span class="nav-number">1.6.</span> <span class="nav-text">理论问题</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.</span> <span class="nav-text">模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0"><span class="nav-number">3.</span> <span class="nav-text">模仿学习</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="thinklive"
      src="/images/thive.png">
  <p class="site-author-name" itemprop="name">thinklive</p>
  <div class="site-description" itemprop="description">起初，世界是一团思索，它向所有的方向迈了一步，于是万物由此而生</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">45</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">50</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/thinklive1" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;thinklive1" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/t469631989@gmail.com" title="E-Mail → t469631989@gmail.com" rel="noopener me"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/38099250?spm_id_from=333.1007.0.0" title="bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;38099250?spm_id_from&#x3D;333.1007.0.0" rel="noopener me" target="_blank"><i class="fa custom bilibili fa-fw"></i>bilibili</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://steamcommunity.com/id/thinkliving" title="steam → https:&#x2F;&#x2F;steamcommunity.com&#x2F;id&#x2F;thinkliving" rel="noopener me" target="_blank"><i class="fa custom steam fa-fw"></i>steam</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml" rel="noopener me"><i class="fa fa-rss fa-fw"></i>RSS</a>
      </span>
  </div>

<div style="Text-align:center;width:100%"><div style="margin:0 auto"><canvas id="canvas" style="width:60%" height="100" width="700">当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>!function(){function t(t,e){for(var a=0;a<l[e].length;a++)for(var n=0;n<l[e][a].length;n++)1==l[e][a][n]&&(h.beginPath(),h.arc(14*(g+2)*t+2*n*(g+1)+(g+1),2*a*(g+1)+(g+1),g,0,2*Math.PI),h.closePath(),h.fill())}function e(){var t=[],e=/(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date),a=[];a.push(e[1],e[2],10,e[3],e[4],10,e[5],e[6]);for(var r=c.length-1;r>=0;r--)a[r]!==c[r]&&t.push(r+"_"+(Number(c[r])+1)%10);for(var r=0;r<t.length;r++)n.apply(this,t[r].split("_"));c=a.concat()}function a(){for(var t=0;t<d.length;t++)d[t].stepY+=d[t].disY,d[t].x+=d[t].stepX,d[t].y+=d[t].stepY,(d[t].x>i+g||d[t].y>f+g)&&(d.splice(t,1),t--)}function n(t,e){for(var a=[1,2,3],n=["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"],r=0;r<l[e].length;r++)for(var o=0;o<l[e][r].length;o++)if(1==l[e][r][o]){var h={x:14*(g+2)*t+2*o*(g+1)+(g+1),y:2*r*(g+1)+(g+1),stepX:Math.floor(4*Math.random()-2),stepY:-2*a[Math.floor(Math.random()*a.length)],color:n[Math.floor(Math.random()*n.length)],disY:1};d.push(h)}}function r(){o.height=100;for(var e=0;e<c.length;e++)t(e,c[e]);for(var e=0;e<d.length;e++)h.beginPath(),h.arc(d[e].x,d[e].y,g,0,2*Math.PI),h.fillStyle=d[e].color,h.closePath(),h.fill()}var l=[[[0,0,1,1,1,0,0],[0,1,1,0,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,0,1,1,0],[0,0,1,1,1,0,0]],[[0,0,0,1,1,0,0],[0,1,1,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[1,1,1,1,1,1,1]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,0,0,1,1],[1,1,1,1,1,1,1]],[[1,1,1,1,1,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,0,1,1,1,0],[0,0,1,1,1,1,0],[0,1,1,0,1,1,0],[1,1,0,0,1,1,0],[1,1,1,1,1,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,1,1]],[[1,1,1,1,1,1,1],[1,1,0,0,0,0,0],[1,1,0,0,0,0,0],[1,1,1,1,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[1,1,1,1,1,1,1],[1,1,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,1,1,0,0,0,0]],[[0,0,0,0,0,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,0,0,0]]],o=document.getElementById("canvas");if(o.getContext){var h=o.getContext("2d"),f=100,i=700;o.height=f,o.width=i,h.fillStyle="#f00",h.fillRect(10,10,50,50);var c=[],d=[],g=o.height/20-1;!function(){var t=/(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date);c.push(t[1],t[2],10,t[3],t[4],10,t[5],t[6])}(),clearInterval(v);var v=setInterval(function(){e(),a(),r()},50)}}()</script></div>
<img class= 'logo' src="/images/thinklive_cyber.png"; z-index: '0'; style="max-width: 100%; width: auto; height: auto;background-color: --content-bg-color;">

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://thinklive1.github.io/thinklive/37185/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/thive.png">
      <meta itemprop="name" content="thinklive">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="thinklive">
      <meta itemprop="description" content="起初，世界是一团思索，它向所有的方向迈了一步，于是万物由此而生">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="强化学习笔记 all in one | thinklive">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          强化学习笔记 all in one
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-09-09 15:53:29" itemprop="dateCreated datePublished" datetime="2025-09-09T15:53:29+08:00">2025-09-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-10-31 13:04:34" itemprop="dateModified" datetime="2025-10-31T13:04:34+08:00">2025-10-31</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">课程笔记</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>11k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>40 分钟</span>
    </span>
</div>

        
        </div>
      </header>
   

    
    
    
    <div class="post-body" itemprop="articleBody"><p><a target="_blank" rel="noopener" href="https://rail.eecs.berkeley.edu/deeprlcourse/">cs285</a></p>
<span id="more"></span>
<h1 id="基础概念">基础概念</h1>
<p>强化学习和监督学习的区别：</p>
<ol type="1">
<li>强化学习输入的样本是序列数据, 监督学习的样本之间相互独立</li>
<li>没有明确的监督者, 通过奖励机制进行学习, 但回馈可能是长期的, 模糊的</li>
</ol>
<p>一些强化学习的演示视频中, ai 会做一些人类看来无意义的动作, 正是这种“玄学”的回馈机制导致的</p>
<ul>
<li>actor: 行为主体
<ul>
<li>action 则可分为离散和连续, 例如 2d 游戏中走格子迷宫就是一个典型的离散动作空间</li>
</ul></li>
<li>observaton o /states s: 观测与状态
<ul>
<li>观测到的情况 o 和现实情况（状态 s）其实有可能不同, 假设可以观察到全景, rl 则成为一个马尔科夫决策过程</li>
</ul></li>
<li>policy π: 行为策略
<ul>
<li>带有参数 θ</li>
</ul></li>
<li>reward：反馈
<ul>
<li>baseline: 避免总是正值的 reward, 增加的偏置值, 例如取期望</li>
</ul></li>
<li>episode: 一轮行动</li>
<li>trajectory τ: <span class="math inline">\(\tau=\{s_{1},a_{1},s_{2},a_{2},\cdots,s_{T},a_{T}\}\)</span></li>
<li>折扣 γ: 直觉上, 最开始的训练回馈可能更重要, 越往后则训练收益越小, 所以对每步的回馈可以乘以一个 <span class="math inline">\(\gamma^{t-1}\)</span>, t 为训练次数, 这个超参数也可以用于控制训练策略偏短期还是偏长期</li>
</ul>
<p>流程： env(s1)-&gt; actor(a1)-&gt; env(s2)……<br />

$\begin{array}{l}{ {p_{\theta}(\tau)} }{ { {}=p(s_{1})p_{\theta}(a_{1}|s_{1})p(s_{2}|s_{1},a_{1})p_{\theta}(a_{2}|s_{2})p(s_{3}|s_{2},a_{2})\cdots} } \\   =p(s_{1})\prod_{t=1}^{T}p_{\theta}(a_{t}|s_{t})p(s_{t+1}|s_{t},a_{t})\end{array}$  
</p>
<p>同一轮中每对(s, a)产生一个 reward, 其期望为: <span class="math inline">\(\bar{R}_{\theta}=\sum_{\tau}R(\tau)p_{\theta}(\tau)=E_{\tau\cap P_{\theta}(\tau)}[R(\tau)]\)</span></p>
<h2 id="策略梯度">策略梯度</h2>
<p>rl 的目标就是找一个最大化期望回报的策略 π, 等价于找到最好的策略参数 θ, 可以使用和机器学习类似的梯度下降, 只不过这个梯度比较特殊，下面引用 cs285 的 ppt:</p>
<p><img src="/assets/ml/image-24.png" /> <img src="/assets/ml/image-23.png" /> 上图 1 中的 J 表示根据确定的 θ 和 π, 得到的轨迹 τ 对应回报的期望, 使用一个 trick, 将概率的梯度转为其自身和 log 梯度的乘积, 这是为了凑出数学期望的形式<br />
再看上图 2, 其实 τ 的概率是一系列 a, s 概率的乘积, 对其 log 转为加法, 得到的项中, 终于根据 s 得到 a 的概率是和 θ 有关的(这也完全符合常识), 于是最后我们所求的梯度其实只有对 π 求微分这个计算量比较大的项<br />
为什么我们想要数学期望的形式？ 因为对此我们可以通过大量采样的平均来估算，例如对 J 就可以用 n 个轨迹回报的均值估算, 而对 J 的梯度, 类似地有:</p>
<p><span class="math display">\[\nabla_{\theta}J(\theta)\approx\frac{1}{N}\sum_{i=1}^{N}\left(\sum_{t=1}^{T}\nabla_{\theta}\log\pi_{\theta}({\bf a}_{i,t}|{\bf s}_{i,t})\right)\left(\sum_{t=1}^{T}r({\bf s}_{i,t},{\bf a}_{i,t})\right)\]</span><br />
这个式子看上去有点吓人，其实它只是单纯地用旧策略的 log 梯度乘对应的回报值, 也就是说在进行若干轮采样后, 总体上好的回报, 就加上(当然这里有一个学习率超参数)对应的好的参数, 坏的回报就减去坏的参数<br />
再考虑一下这个梯度的数学性质, 首先很明显的是, 回报和的绝对值越大, 更新幅度就会越大, 这很符合常识, 但同时, 考虑 log 的性质, 对较小的值其导数的绝对值越大, 也就是说对一个策略 π 来说, 其不怎么倾向于采取的动作反而会得到更多的更新梯度, 这可以视为一种 exploration<br />
此外，对部分可观测的马尔科夫假设, 这里不作具体推导, 但只要把上面的 s 换成 o 就能沿用梯度策略<br />
和 log 函数的很多应用场景一样, 这里实际上可以理解为分类问题, 因为模型输出的动作是有限的, 策略最后产生的是动作空间中的概率分布, 稍微不同的是需要乘回报值, 并且要分很多轮训练</p>
<h3 id="问题和改进">问题和改进</h3>
<p>基于策略梯度的训练方式非常直观，但相应地也有问题, 即它结果会非常不稳定<br />
我们寻找的策略会不断往最高的回报前进, 这取决于我们每轮采样的回报, 这里我们其实是希望得到相对最优的一个目标, 但如果假设我们得到回报的相对倾向性较稳定, 但不同采样的绝对差距较大, 就会导致虽然以最优化策略为目标，最后得到的参数却相差非常大(方差大)的情况</p>
<ol type="1">
<li>因果关系: 这基于一个永恒的真理, 未来不会影响过去, 在 rl 中也就是说： 时间 t'之后得到的回报无论是好是坏都不该影响 t'之前的策略，对应在之前的方法中, 就是让回报总和一项从 t 开始计算:<br />
这听起来有点绕, 举个例子, 打对战游戏的前 t 把都输了, 于是 t+1 把转变思路赢了游戏, 假如按未改进的版本, 由于前 t 把输得太多，最后所有 t+1 局游戏的回报还是负的, 因此第 t+1 把的思路也是错的, 但是 t+1 把的思路的回报是正的, 也就是说这时的思路是正确的, 对游戏策略理应有正确影响，按改进后的方法, 我们认为 t+1 局的游戏思路期望回报为正</li>
</ol>
<p><span class="math display">\[\nabla_{\theta}J(\theta)\approx\frac{1}{N}\sum_{i=1}^{N}\left(\sum_{t=1}^{T}\nabla_{\theta}\log\pi_{\theta}({\bf a}_{i,t}|{\bf s}_{i,t})\right)\left(\sum_{t&#39;=t}^{T}r({\bf s}_{i,t&#39;},{\bf a}_{i,t&#39;})\right)\]</span></p>
<p>由于这里我们减少了回报的绝对值大小，因此其 θ 最后的方差必然会缩减, 但这么做对期望是无偏的(对策略梯度加减任何常数偏移值， 都会保持无偏), 也就是不会改变更新梯度，相当于只改变了其系数(回报和)的大小</p>
<ol start="2" type="1">
<li>baseline</li>
</ol>
<p>我们的目的是找到最优化的策略参数, 实际上只要回报为正, 就会加上对应参数的梯度, 这种更新策略其实是“找到回报为正的参数”, 还是拿打游戏举例子, 如果目标是打到全国第一, 那么和低水平对手的对局, 就算赢了这局的思路也不值得学习<br />
因此可以考虑将总回报减去一个 baseline, 简称为 b, b 一般取回报的平均值<br />
事实上，这对梯度的期望是无偏的， 即减去 b 和不减去两者的梯度一致, 但会减少方差, 最优的 b 可以通过对方差算关于 b 的极值求出来, 由于不太常用省去过程, 最优 b 为:</p>
<p><span class="math display">\[b={\frac{E[g(\tau)^{2}r(\tau)]}{E[g(\tau)^{2}]}}\]</span></p>
<h3 id="重要性采样">重要性采样</h3>
<p>策略梯度是 on-policy 的, 也就是每次需要收集数据才能更新, 数据与策略是绑定的, 这样的缺点是更新很慢，效率低<br />
如果想用 off-policy, 则需要重要性采样</p>
<div class="pdf-container" data-target="/assets/ml/rl_lec5.pdf" data-height="800px"></div>
<p>暂时忽略被划掉的一项(这是因为我们不想让这种指数级别的项增大方差) 尽可能简单地说明为什么要用这种采样方法: 不谈证明, 但是重要性采样有个性质是不会改变采样的均值(期望), 只会改变方差, 一般来说会将其用于改进采样的方差, 和名字一样, 可以理解为用不同的重要性对一个分布采样, 也就是分布不变，只改变采样方式, 来获得符合要求的采样数据<br />
但在策略梯度中, 问题在于我们改进过程中策略的分布和原始数据时的分布是不同的， 想实现 off-policy, 就需要用来自不同策略的数据来改进当前策略, 所以我们需要重要性采样这样的技巧, 不太严谨地说， 将不同策略的数据转化为当前策略的数据并用于训练， 又或者理解为用不同策略的数据计算对当前策略的更新梯度</p>
<p>此外对策略梯度还有的问题是, 对不同的参数 θ, 其梯度的数量级可能完全不同，需要进行一定约束, 例如约束 θ 分布前后的散度, 从而平衡不同方向的更新速率<br />
这方面具体实现上比较复杂，暂时只需要知道大概原理即可</p>
<h2 id="actor-critic">actor-critic</h2>
<p>由类似我们前文提到的 baseline 思想, 定义 A(advantage)= Q-V, 也就是状态动作对的期望奖励-状态的期望奖励, 这个标准能评估对当前状态来说, 采取的动作是好是坏<br />
我们可以将 A 替代掉之前的期望回报, 但需要注意的是, 由于实际中很多时候 A 是估算的, 这会带来方差的优化，但很可能让更新梯度有偏; 尽管如此, 一般我们还是认为拿大量减少方差换一些偏见是划算的<br />
Q 可以写为当前(s, a)的回报加上基于未来状态分布的 V 的期望, 可以用直接的 <span class="math inline">\(s_{t+1}\)</span> 近似这个期望, 写成 <span class="math inline">\(Q^{\pi}(\mathbf{s}_{t},\mathbf{a}_{t})\approx r(\mathbf{s}_{t},\mathbf{a}_{t})+V^{\pi}(\mathbf{s}_{t+1})\)</span> , 于是:<br />
<span class="math display">\[A^{\pi}(\mathbf{s}_{t},\mathbf{a}_{t})\approx r(\mathbf{s}_{t},\mathbf{a}_{t})+V^{\pi}(\mathbf{s}_{t+1})-V^{\pi}(\mathbf{s}_{t})\]</span> <span class="math display">\[\nabla_{\theta}J(\theta)\approx\frac{1}{N}\nabla_{i=1}^{N}\sum_{t=1}^{T}\Sigma_{\theta}\log\pi_{\theta}({\bf a}_{i,t}|{\bf s}_{i,t})A^{\pi}({\bf s}_{i,t},{\bf a}_{i,t})\]</span> 这种近似的好处是, 相比涉及动作空间的 Q, V 更好计算, 近似后我们唯一需要算的就是 V 了, 而之前的 term: <code>J</code>, 可以视为根据初始状态 s1 分布的 V(s1)的期望; 此外, 由于只用一个样本估算, 也能减少方差<br />
对 V 来说最方便的估算就是使用蒙特卡洛方法, 尤其在模拟环境或者电子游戏之类的场景下, 可以大量采样 <span class="math inline">\(V^{\pi}(\mathbf{s}_{t})\approx\sum_{t^{\prime}=t}^{T}r(\mathbf{s}_{t^{\prime}},\mathbf{a}_{t^{\prime}})\)</span><br />
但也可以使用一个估算函数, 即 <code>function approximation</code>, 这是个典型的监督回归问题, 可以用神经网络解决, 数据集为 n 轮训练中的 <span class="math inline">\(s_{i,t}\)</span> 与其对应的从 t 开始的回报和 <span class="math inline">\(\left\{\left({\bf s}_{i,t},r({\bf s}_{i,t},{\bf a}_{i,t})+\hat{V}_{\phi}^{\pi}({\bf s}_{i,t+1})\right)\right\}\)</span> , loss 设置为平方误差和 <span class="math inline">\(\mathcal{L(\phi)}=\frac{1}{2}\sum_{i}\left|\left|\hat{V}_{\phi}^{\pi}(\mathbf{s}_{i})-y_{i}\right|\right|^{2}\)</span><br />
基于神经网络的方法的好处是, 网络能一定程度上捕获数据间的关系, 由一定的泛化能力, 也就是可能能够平滑化输出的结果, 即类似的状态应该有类似的期望回报, 从而减少方差; 缺点则是不能保证是无偏的</p>
<p>以上我们提到的方法就是标题里的 a-c, 如果我们再引入一个折扣因子，就可以表述为以下的流程:</p>
<p><img src="/assets/ml/image-25.png" /></p>
<p>这个版本的 a-c 其实非常好理解, 它就是用一个单独的神经网络预测 V，用于帮助另一个训练策略 π 的网络<br />
但问题在于这两个网络只有输入是相同的(s), 其特征是完全分开的, 这可能让它的性能受限, 一种改进是共享网络, 即用一个网络接受 s，同时生成 V 和 π<br />
目前的这个版本是 on-line 的, 这就意味着, 每次只能用一个数据来更新网络, 如果想实现 batch， 则需要并行化, 具体来说可以分为同步异步两种情况<br />
<img src="/assets/ml/image-26.png" /> 如上图, 同步的情况很好理解, 就是每次让一群线程获取行动后的数据, 每次都同步更新<br />
异步的情况下, 不同线程可能有不同的运行速度, 并各自用收集到的数据更新网络参数, 这样的效率更好, 但缺点是, 较慢的线程更新时可能遇上新版本的 actor, 也就是说会有这种用非当前 actor 的数据更新它的问题, 当然一般来说因为线程之间的速度不会有特别大的差别, 影响相对不大</p>
<h3 id="off-policy-a-c">off-policy a-c</h3>
<p>在 off-policy 情况下, 存储之前的状态转移数据为 <code>replay buffer</code>, 这些数据可能来自于很久之前的策略, 这会带来两个问题:</p>
<ol type="1">
<li>批评家对 V 的评估是基于旧策略的</li>
<li>策略更新时使用的策略梯度也是来自于旧策略的</li>
</ol>
<p>对此, 我们可以把 V 给扔掉, 转向 Q 函数, <span class="math inline">\({ Q}^{\pi}(\mathrm{s}_{t}, a_t)\stackrel{\_}{=}\sum_{t^{\prime}=t}^T E_{\pi_{\theta}}\ [r(\mathrm{s}_{t^{\prime}},a_{t^{\prime}})]\mathrm{s}_{t}, a_{t}]\)</span> 那么为什么要这么做? 考虑一下现在我们有什么数据, 即来自旧策略下的 V 函数, V 函数的意义是, 各个状态的期望回报, 也就是我们知道各个状态的价值, 而这些价值相对来说不会随着策略变动而改变<br />
新策略相比旧策略改动的只有给定 s 条件下 a 的分布, 也就是说给定旧策略的 s, 可以查到在新策略在相同给定状态 s 下产生的动作概率分布, 从而算出新策略对应的 Q 函数<br />
于是 off-line 方法可以写成:</p>
<p><img src="/assets/ml/image-27.png" /> 其中, <span class="math inline">\(a_{i}&#39;\)</span> 与 <span class="math inline">\(a_{i}^{\pi}\)</span> 是根据 <span class="math inline">\(s_{i}&#39;\)</span> 从新策略中产生的动作<br />
实际中一般用 Q 代替之前的 A，这是因为这种方法能提供大量样本, 从而弥补方差的问题<br />
那么 offline 是毫无损失的方法吗? 其实并不是, 因为对来自旧策略的旧状态 s 我们无能为力, 这必然会产生一些偏差, 但基本来说处于可以接受的范围内</p>
<h3 id="更好的-baseline">更好的 baseline</h3>
<p>上面提到, 基于神经网络的评论家能实现有偏下的低方差, 策略梯度则是无偏的高方差, 那么有办法得到无偏的低方差更新梯度吗?<br />
下图展示一种估计策略梯度的方法, 它基于一个原理, 对梯度乘以任何一个值依赖状态而不依赖动作的函数，都能保持无偏, 选取价值函数作为 baseline, 由于我们希望估计的就是 V 的期望，选择价值函数作为 baseline 应当能大量减少方差</p>
<p><img src="/assets/ml/image-28.png" /> 那么如果用 Q 函数作为基线呢? 如果这样的话, 理论上讲我们要求的 A 会在 0 的上下浮动, 这样不是能让方差达到理想情况吗? 但问题在于, 由于这样不无偏, 那就需要加上一个误差修正项, 我们将得到一个吓人的式子:</p>
<p><span class="math display">\[\nabla_{\theta}J(\theta)\approx\frac{1}{N}\sum_{i=1}^{N}\sum_{t=1}^{T}\nabla_{\theta}\log\pi_{\theta}({\bf a}_{i,t}|{\bf s}_{i,t})\left(\hat{Q}_{i,t}-Q_{\phi}^{\pi}({\bf s}_{i,t},{\bf a}_{i,t})\right) +\frac{1}{N}\sum_{i=1}^{N}\sum_{t=1}^{T}\nabla_{\theta}E_{\mathrm{a}\sim\pi_{\theta}({\bf a}_{t}|{\bf s}_{i,t})}\left[Q_{\phi}^{\pi}({\bf s}_{i,t},{\bf a}_{t})\right]\]</span><br />
这个式子看起来似乎根本用不了, 但其实也有技巧能用成本可控的方式估计误差修正项</p>
<p>还有一种比较常见的思路是取 n 步的回报，由于越往后数据越可能发散, 这样也能降低方差, 这里不多介绍</p>
<h2 id="基于价值的方法">基于价值的方法</h2>
<p>最直观的说, 策略就是从动作空间中选一个最佳的动作, 这个最佳一般用 Q 函数表示, 而 Q 依赖于 V, 从 Q 或者 V 的角度出发, 强化学习的问题就是选择一个最佳的 Q 与其对应的 A, 再有足够样本的情况下, 估算 Q 可以用 V 矩阵实现, 因此这称为基于价值的方法(value-based v-b)<br />
很明显, 现实中经常有很大或者无穷的动作空间, 因此基于表格的 V-B 不现实, 类似之前的 a-c, 可以训练一个估计 Q 函数的网络, 定义其 loss 为: <span class="math inline">\(\mathcal{L}(\phi)=\frac{1}{2}\left|\left|V_{\phi}(\mathbf{s})-\operatorname*{max}Q^{\pi}(\mathbf{s},\mathbf{a})\right|\right|^{2}\)</span><br />
但是如果我们想迭代 Q, 还有一个严峻的问题： 状态转移的概率很难得到, 因为 Q 由本轮回报与之后的根据未来状态分布的 V(s')的期望相加组成, 而未来状态的分布, 也就是根据策略 π 采取行动后的 s'我们是不知道的, 于是我们用其他的(a, s', a')样本来估计未来的 Q 期望, 也就是：<br />
<span class="math inline">\(Q^{\pi}({\bf s},{\bf a})\leftarrow r({\bf s},{\bf a})+\gamma E_{ {\bf s}^{\prime}\sim p({\bf s}^{\prime}|{\bf s},{\bf a})}[Q^{\pi}({\bf s}^{\prime},\pi({\bf s}^{\prime}))]\)</span><br />
为什么可以这么做? 或者说, 为什么策略的变化在这样的学习中似乎不重要? 这是因为在这种方法下策略就是选取最大化 Q 的动作, 也就是说策略由 Q 来定义, 而不是 Q 受策略影响, 不严谨地说, 可以认为 Q 的地位高于策略</p>
<p>稍微总结一下训练步骤, 对 V 来说:</p>
<p><span class="math display">\[\begin{array}{l}{ {1.\;\;\mathrm{set}\;{\bf{y}}_{i}\:\leftarrow\;\mathrm{max_{a_{i}}}(r({\bf{s}}_{i},{\bf{a}}_{i})+\gamma E[V_{\phi}({\bf{s}}_{i}^{\prime})])}}\\ { {2.\;\;\mathrm{set}\;\phi\:\leftarrow\;\mathrm{arg~min}_{\phi}\:\frac{1}{2}\:\sum_{i}\|V_{\phi}({\bf{s}}_{i})-{\bf{y}}_{i}\|^{2}}}\end{array}\]</span></p>
<p>于是对 Q 的计算:</p>
<p><span class="math display">\[\begin{array}{l}{ {1.\;\;\mathrm{set}\;\:{\mathrm{y}}_{i}\:\leftarrow\:r(\mathrm{s}_{i},{\bf a}_{i})+\gamma{ E}[V_{\phi}(\mathrm{s}_{i}^{\prime})]}}\\ { {2.\;\;\mathrm{set}\;\:\phi\:\leftarrow\:\mathrm{arg\,min}_{\phi}\:\frac{1}{2}\:\sum_{i}||{\cal Q}_{\phi}(\mathrm{s}_{i},{\bf a}_{i})-{\bf y}_{i}||^{2}}}\end{array}\]</span></p>
<p>也就是说, 由于我们定义策略为选择 Q 最大的 A, 那么相应的状态 s 的动作空间中最佳的 Q 就是状态 s 的期望价值<br />
由于没有麻烦的策略梯度, 以上的 v-b 有较小的方差, 但还有一个问题是, 对神经网络的方法它不保证收敛性</p>
<p>对训练 Q 的神经网络来说, 应用上述的用 Q 估算 V 的思想, 步骤为:</p>
<p><span class="math display">\[\begin{array}{l} {\mathrm{1.~collect~dataset~\{(s_{i},{\bf a}_{i},{\bf s}_{i}^{\prime},r_{i})\}~u{\mathrm{sing~some~policy}}}} \\ { {2.~\mathrm{set~yi}\leftarrow r({\bf s}_{i},{\bf a}_{i})+\gamma\mathrm{max_{a^{\prime}}}\,Q_{\phi}({\bf s}_{i}^{\prime},{\bf a}_{i}^{\prime})}} \\ { {3.~\mathrm{set~}\phi\leftarrow\mathrm{arg\,min}_{\phi}\ {\frac{1}{2}} { \Sigma_{i}|\vert{\cal Q}_{\phi}({\bf s}_{i},{\bf a}_{i})-{\bf y}_{i}}}\vert\vert^{2}}\end{array}\]</span></p>
<p>这种方法称为 <code>full fitted Q-iteration algorithm</code>, 其中 2.3.可以执行 k 次确保其更好地迭代</p>
<p>以上这个简单的方法就是最基础的 off-policy on-line 的 Q 学习, 这个方法有一个非常明显的问题, 由于它的策略不包括任何概率, 是完全决定性的, 而迭代过程是贪心的, 因此在迭代中如果有一些错误或者不好的数据集，很容易就会陷入一个表现较差且无法进一步迭代的循环里<br />
因此, 尤其是对最开始的一批数据, 我们希望能增加一些随机性, 最简单的方法就是对原来的策略加一层随机, 称为 <span class="math inline">\(\epsilon\)</span> 贪心:</p>
<p><span class="math display">\[\pi({\bf a}_{t}|{\bf s}_{t})=\left\{\begin{array}{l}{ {1-\epsilon\mathrm{~if~}{\bf a}_{t}=\arg\mathrm{max}_{ {\bf a}_{t}}Q_{\phi}({\bf s}_{t},{\bf a}_{t})}}\\ { {\epsilon/(|A|-1)\mathrm{~otherwise}}}\end{array}\right.\]</span></p>
<p>另一种常见方法称为 <code>Boltzmann exploration（波尔兹曼探索)</code>, 即 <span class="math inline">\(\pi({\bf a}_{t}|{\bf s}_{t})\propto\exp(Q_{\phi}({\bf s}_{t},{\bf a}_{t}))\)</span> 这种策略的好处是能根据 Q 的好坏加权策略, 如果是 Q 都比较高的行动, 概率不会相差太大, 而 Q 很低的行动则几乎不会被采取</p>
<h3 id="收敛性">收敛性</h3>
<p>接下来尝试证明 V-B 方法是否收敛</p>
<p><img src="/assets/ml/image-29.png" /></p>
<p>首先问题定义如图, B(back-up)操作用于根据给定 V 找到一个最好的动作, <code>V*</code> 则是一个最终的最优解, 也就是其回报与转移状态的价值期望之和在所有动作中最大<br />
也就是说, 如果用 B 操作不断迭代, 能到达 <code>V*</code> 状态, 就说明 v-b 是收敛的<br />
直观地, 我们有以下性质:<br />
<span class="math display">\[{\mathrm{for~any~}}V{\mathrm{~and~}}{\bar{V}},{\mathrm{~we~have~}}||B V-B{\bar{V}}||\infty\leq\gamma||V-{\bar{V}}||\infty\]</span><br />
于是:<br />
<span class="math display">\[\|B V-V^{*}\|_{\infty}\leq\gamma\|V-V^{*}\|_{\infty}\]</span></p>
<p>这样的证明省略了很多东西, 但对 cs 来说已经足够了, 事实上, 基于不断取最大值这个操作, 基于表格(矩阵/向量)的 v-b 收敛是很符合直觉的<br />
那么对基于神经网络回归的 v-b 呢? 这个问题稍微有点麻烦, 将 nn 能产出的价值全集定义为 <span class="math inline">\(\Omega\)</span>, 事实上由于基于线性拟合与目前这个损失函数, nn 做的其实是个最小二乘法, 实际产生的 V'是理想 V 在 Ω 上的一个投影<br />
对任意两个点 a, b 对一条直线的投影点 c, d, ab, cd 直接的距离 l1, l2 来说, 始终有 l1≥l2, 我们可以简单地构建一个直角三角形证明这点, 对 ab 不与直线平行的情况, ab 是斜边, cd 是直角边<br />
我们将投影操作定义为 π, 则有 <span class="math display">\[\|\Pi V-\Pi{\bar{V}}\|^{2}\leq\|V-{\bar{V}}\|^{2}\]</span><br />
而现在的问题是, π 和 B 都有我们很想要的收缩性质, 但如果同时存在这两种操作, 就不保证收缩了<br />
<img src="/assets/ml/image-30.png" /> 上图给出了一个很形象的例子, 由此我们证明基于 nn 的 v-b 不一定收敛<br />
对于 Q 学习, 实际上也差不多<br />
这有点像所谓的合成谬误, 我们有能让策略变好的贪心方法, 还有最小二乘法与梯度下降, 为什么加到一起后结果却不一定变好?<br />
事实上, 我们训练的这个回归网络并不是真正地使用了梯度下降, 见下图<br />
<img src="/assets/ml/image-31.png" /> 对同样和参数有关的 yi, 我们并没有计算梯度, 当然也可以计算(<code>Residual Gradient</code>), 但由于计算量太大, 这么做在工程上很不实用<br />
还有一个悲伤的消息是, 之前的 a-c 也不保证收敛</p>
<h3 id="局限">局限</h3>
<p>目前版本的 q 学习是一个随着时间收集数据的 on-line 方法, 除了效率问题外, 由于数据在时间上过于接近, 很可能产生过拟合问题, 类似策略梯度, 可以对其进行离线改进, 并且由于 q 学习的策略不那么重要, 可以说它很适合离线学习<br />
另一种方法称为 <code>replay buffers</code>, 简单地说就是去掉收集数据的步骤, 对一个巨大的数据池，每次随机取样用来不断进行预测和模型更新, 更新若干轮后再产生一些新数据放入 buffers 池</p>
<p><img src="/assets/ml/image-32.png" /></p>
<p>上图展示了目前为止的 q 学习步骤, 按经验来说一般 k 设置很小, 而 n 较大, 这会带来一个问题, 即更新模型参数的周期中, 早期采样数据 "落后(lag)" 得少, 而随着训练进行就会有很多落后很久的数据, 为了避免这个问题, 使用以下的更新方式<br />
<span class="math inline">\(\begin{array}{c c}{ {\mathrm{update}~\phi^{\prime};~\phi^{\prime}\leftarrow\tau\phi^{\prime}+(1-\tau)\phi}}&amp;{ {\qquad\qquad\tau=0.999~\mathrm{works~well}}}\end{array}\)</span><br />
为了减少上述伪梯度的影响, 对更新梯度, 我们用整体更新前的目标网络生成的 max 参数来更新临时网络参数<br />
以上的伪代码写作单线程迭代的形式, 但其实由于目标网络在一整批数据后才更新, 上述过程可以并行进行, 也就是用于规模庞大的深度学习中, 即(DQN Deep Q-Network) 此外, 对于整个 buffer 的维护, 由于 buffer 存在上限, 那么就必然需要一个“驱逐”机制, 一个简单的思路是采用环形 buffer, 使用以上机制的一个 dpo 流程可以表示为下图:</p>
<p><img src="/assets/ml/image-33.png" /></p>
<p>不同的 q 学习对 3 个 process 各有侧重:</p>
<ol type="1">
<li>online q learning: process1,2,3 have same speed</li>
<li>DQN: process 1,3 have same speed, process 2 is slow, buffer is large</li>
<li>fitted q-iteration: process 3 在 process 2 的内循环中, process 2 在 process1 的内循环中</li>
</ol>
<h3 id="改进">改进</h3>
<p>实际的 q 学习中, 有一个神奇的现象, q 的估计器整体性地高估 q 的值, 而不是在真实值的上下浮动<br />
对任意两个随机变量 x1, x2, 有如下不等式:<br />
<span class="math inline">\(E[\operatorname*{max}(X_{1},X_{2})]\geq\operatorname*{max}(E[X_{1}],E[X_{2}])\)</span><br />
也就是直观地说, 取 max 的范围越大, 这个 max 倾向越大的值, 而我们在取 Q 函数的估计时，会不断用过往的最高 q 值来更新模型, 而这个过往的 q 可能来自于某个误差, 假设这个误差来自于随机高斯分布, 有正有负, 由于我们总取最大值, 那么正误差的值就会被倾向于被用来更新<br />
如何解决这个问题？一个想法是, 将 q 的评估和更新分开, 称为 <code>double q-learning</code> , 其更新步骤为: <span class="math inline">\(Q_{\phi_{A}}(\mathrm{s},\mathbf{a})\leftarrow r+\gamma Q_{\phi_{B}}(\mathrm{s}^{\prime},\mathrm{arg~max}\,Q_{\phi_{A}}(\mathrm{s}^{\prime},\mathbf{a}^{\prime}))\)</span><br />
实际中, 我们正好有目标网络和训练网络两个网络, 可以用作这里的 A 和 B, 当然由于这两个网络其实会经常合并, 这样理论上还有问题, 但一般来说实际中效果尚可<br />
另一个方法称为 <code>multi-step returns</code>, 上面提到过 q 学习的迭代初期可能有非常糟糕的表现, 此时我们更看重采取某个动作后的回报值, 这个问题比较类似 a-c, 可以用多步的回报值解决(在 a-c 中我们用这个办法缓解策略更新的高方差), 即将更新步骤设置为: <span class="math inline">\(y_{j,t}=\sum_{l^{\prime}=t}^{t+N-1}\gamma^{t-t^{\prime}}r_{j,t^{\prime}}+\gamma^{N}\operatorname*{max}_{\Omega_{k},t+N}Q_{\phi^{\prime}}(\mathbf{s}_{j,t+N,\,\Omega_{j,t+N}})\)</span><br />
此外, 这种方法会让训练必须是 on-policy 的, 因为这 n 步的策略必须是被固定的, 当然, 也可以当做这个问题不存在, 毕竟理论上 n 不大的话策略不会改变太多, 又或者实现类似之前的重要性采样</p>
<h3 id="连续动作">连续动作</h3>
<p>连续动作空间的麻烦在于, 如何处理取 Qmax 的操作, 一个基于直觉的想法是梯度下降找机制, 但这样显然计算量很大<br />
为了提高效率, 一个简单的做法是对 Q 随机取 n 个样, 选一个最大值, 这称为 <code>stochastic optimization</code> ; 这种方法有一些改版, 例如通过交叉熵优化采样<br />
另一个不依赖随机的做法是, 用有数值解的函数来表示Q函数, 例如二次函数: <span class="math inline">\(Q_{\phi}({\bf s},{\bf a})=-\frac{1}{2}({\bf a}-\mu_{\phi}({\bf s}))^{T}P_{\phi}({\bf s})({\bf a}-\mu_{\phi}({\bf s}))+V_{\phi}({\bf s})\)</span><br />
从而: <span class="math inline">\(\quad\arg\operatorname*{max}_{\mathbf{a}}Q_{\phi}(\mathbf{s},\mathbf{a})=\mu_{\phi}(\mathbf{s})\qquad\quad\operatorname*{max}_{\mathbf{a}}Q_{\phi}(\mathbf{s},\mathbf{a})=V_{\phi}(\mathbf{s})\)</span><br />
这样以降低表达能力为代价, 得到了高效取max的方法<br />
以及, 训练一个独立的网络来取max, 有点类似于a-c</p>
<h2 id="马尔可夫过程">马尔可夫过程</h2>
<p>马尔可夫过程中, 对一个特定状态则有一个特定的未来状态概率分布（可以理解为策略）, 很容易算出之后的期望回报<br />
即, 如果我们希望知道一个特定状态是好是坏, 可以定义状态价值 V 为其回报的期望, 即从这个状态开始, 折扣 γ 的指数与之后回报乘积的和, V 与回报 G 的区别是, V 绑定一个特定的状态 s, 即某个 s 开始的之后获取的回报的期望<br />
t 时刻后的回报为：<br />
<span class="math display">\[G_{t}=r_{t+1}+\gamma r_{t+2}+\gamma^{2}r_{t+3}+\gamma^{3}r_{t+4}+\ldots+\gamma^{T-t-1}r_{T}\]</span> 状态 s 的价值为：<br />
<span class="math display">\[\begin{array}{l}{ {V^{t}(s)=\mathbb{E}\left[G_{t}\mid s_{t}=s\right]} }\\ { {\ } }\\ { {=\mathbb{E}\left[r_{t+1}+\gamma r_{t+2}+\gamma^{2}r_{t+3}+.\cdot.+\gamma^{T-t-1}r_{T}\mid s_{t}=s\right]} }\end{array}\]</span><br />
即给定 <span class="math inline">\(s_t = s\)</span> 这个条件后的 Gt 期望</p>
<p>实际训练中概率没法直接看出来, 符合常识的想法是靠采样估算, 也就是可以对采样的多组数据取平均值, 这称为 <code>蒙特卡洛(Monte Carlo,MC)采样</code><br />
另一种方法是贝尔曼方程, 这一方程常用于动态规划, 使用下个状态的价值来更新之前状态, 写作：<br />
<span class="math display">\[ V(s)=R(s) + \gamma\sum_{s^{\prime}\in S}p\left(s^{\prime}\mid s\right)V\left(s^{\prime}\right)  \]</span> <code>s′</code> 可以看成未来的某个状态, <code>p(s′|s)</code> 是指从当前状态转移到未来状态的概率。<code>V (s′)</code> 代表的是未来某一个状态的价值, <code>R(s)</code> 为 s 的奖励组成的向量<br />
贝尔曼方程的证明：<br />

$$\begin{array}{l}{ {V(s)=\mathbb{E}\left[G_{t}\mid s_{t}=s\right]} }\\ { {=\mathbb{E}\left[r_{t+1}+\gamma r_{t+2}+\gamma^{2}r_{t+3}+...\mid s_{t}=s\right]} }\\ { {=\mathbb{E}\left[r_{t+1}\right|s_{t}=s]+\gamma\mathbb{E}\left[r_{t+2}+\gamma r_{t+3}+\gamma^{2}r_{t+4}+\dots\mid s_{t}=s\right]} }\\ { {=R(s)+\gamma\mathbb{E}[G_{t+1}\left|s_{t}=s\right]} }\end{array}$$
 下面证明: <span class="math inline">\(\mathbb{E}[V(s_{t+1})|s_{t}]=\mathbb{E}[\mathbb{E}[G_{t+1}|s_{t+1}]|s_{t}]=\mathbb{E}[G_{t+1}|s_{t}]\)</span></p>
<p>条件期望的定义为：<br />
<span class="math display">\[\operatorname{\mathbb{E} }[X\mid Y=y]=\sum_{x}x p(X=x\mid Y=y)\]</span></p>
<p>具体证明：<br />
<img src="/assets/ml/52243716-045d-4e27-ad0c-0e84593fee81.png" /></p>
<p>代入之前的等式, 得：</p>
<p><span class="math display">\[\begin{array}{c}{ {V(s)=R(s)+\gamma\mathbb{E}[V(s_{t+1})|s_{t}=s]} }\\ { {=R(s)+\gamma{\sum\limits_{s^{\prime}\in S} } p\left(s^{\prime}\mid s\right)V\left(s^{\prime}\right)} }\end{array}\]</span></p>
<p>对单个 s 来说, 贝尔曼方程就是一个向量乘积加上偏移值, 所有的 s 可以写成一个矩阵乘法加上奖励向量的形式:</p>
<p><span class="math display">\[\begin{array}{c}{ {V=R+\gamma P V} }\\ { {E V=R+\gamma P V} }\\ { {(E-\gamma P)V=R} }\\ { {V=(I-\gamma P)^{-1}R} }\end{array}\]</span></p>
<p>这个解析解看着很美好, 但涉及到逆矩阵求解, 这是个复杂度 <span class="math inline">\(O(N^3)\)</span> 的问题(单论常用算法), 因此对较大的模型并不实用</p>
<p>得到 V 的两种方法：</p>
<ul>
<li>最简单直接的蒙特卡洛方法, 只需要对某个初始状态采样大量轨迹, 然后取平均值, 既可作为其价值</li>
<li>不断迭代贝尔曼方程, 直到结果趋向收敛, 这个收敛值即可以作为价值, 即“自举”（Bootstrapping）(利用当前的估计或状态值来更新和改进自身的学习过程)</li>
</ul>
<h3 id="决策">决策</h3>
<p>单纯的马尔可夫过程没有策略的空间, 也就是同时根据当前的状态和动作决定之后的状态分布, 策略可以基于概率或者直接产生特定输出, 如果策略已知, 依然可以通过计算 <span class="math inline">\(\sum_{a\in A}\pi(a\mid s)p\left(s^{\prime}\mid s,a\right)\)</span> 获取状态转移的概率<br />
也就是说, 通过采样行动, 我们可以得到使用特定策略的特定状态的价值 由于引进了决策, 定义一个 Q 函数, 也被称为动作价值函数(action-value function)。Q 函数定义的是在某一个状态采取某一个动作, 它有可能得到的回报的一个期望 <span class="math display">\[V_{\pi}(s)=\sum_{a\in A}\pi(a\mid s)Q_{\pi}(s,a)\]</span></p>
<p><span class="math display">\[\begin{array}{r l} {Q(s,a)} &amp;=\mathbb{E}\left[G_{t}\mid s_{t}=s,a_{t}=a\right] \\ &amp;=\mathbb{E}\left[r_{t+1}+\gamma r_{t+2}+\gamma^{2}r_{t+3}+\ldots\mid s_{t}=s,a_{t}=a\right]\\ &amp;{=R(s,a)+\gamma\mathbb{E}[G_{t+1}|s_{t}=s,a_{t}=a]}\\ &amp;{=R(s,a)+\gamma\mathbb{E}[V(s_{t+1})|s_{t}=s,a_{t}=a]}\\ &amp;{=R(s,a)+\gamma\displaystyle\sum_{s^{\prime}\in S}p(s^{\prime}\mid s,a)\,V\left(s^{\prime}\right)} \end{array}\]</span></p>
<p>可以把状态价值函数和 Q 函数拆解成两个部分: 即时奖励和后续状态的折扣价值, 这样分解后得到一个类似于之前马尔可夫奖励过程的贝尔曼方程——贝尔曼期望方程(<code>Bellman expectation equation</code>):</p>
<p><span class="math display">\[V_{\pi}(s)=\mathbb{E}_{\pi}\left[r_{t+1}+\gamma V_{\pi}\left(s_{t+1}\right)\mid s_{t}=s\right]\]</span> <span class="math display">\[Q_{\pi}(s,a)=\mathbb{E}_{\pi}\left[r_{t+1}+\gamma Q_{\pi}\left(s_{t+1},a_{t+1}\right)\right.\mid s_{t}=s,a_{t}=a]\]</span></p>
<h3 id="备份图">备份图</h3>
<p>以上我们得到了状态和动作的价值, 对某一个 s, 它可能对应多种 a, 确定一个 a 后得到下一个 state, 这其实是一种树状的关系, 因此可以用树形图表示 V 的演变, 称为备份(回溯)图<br />
<img src="https://datawhalechina.github.io/easy-rl/img/ch2/2.10.png" /></p>
<p>如图, 每一个空心圆圈代表一个状态, 每一个实心圆圈代表一个状态-动作对<br />
从叶节点开始向上回溯, 可以得到叶节点上一层的 Q, 然后继续递推, 得到 a 上一层 s 的 V, 这样逐层计算就能得出整张图的状态与动作价值</p>
<p><a target="_blank" rel="noopener" href="https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html">可视化学习过程</a></p>
<h3 id="迭代">迭代</h3>
<h4 id="策略迭代">策略迭代</h4>
<p>现在我们有 V 和 Q 这两个工具, 那么假设初始状态, 随便选定一套参数作为策略, 接下来随着训练过程, 不断更新轨迹中各个状态的价值, 由此再得到 Q 函数, 利用 Q 函数更新策略参数, 如此循环, 就可以让策略收敛到较优的性能<br />
优化过程, 简单地说就是先找到所有状态的 V, 然后根据采样结果以及 V 的分布算出对于各个动作 a 的 Q, 那么对于一个特定的 s, 应该采取的策略需要让价值最高的动作有最高的概率, 如果当做分类问题做, 就可以以交叉熵为 loss 函数做梯度下降<br />
这样的优化后, 理论上, 各个状态的 V 以及策略产生的 Q 都会更高, 而作为表现指标的初始状态的 V 也会更好<br />
收敛后, 取让 Q 函数值最大化的动作, Q 函数就会直接变成价值函数, 这称为 <code>贝尔曼最优方程（Bellman optimality equation）</code>: <span class="math inline">\(V_{\pi}(s)=\operatorname*{max}_{a\in A}Q_{\pi}(s,a)\)</span><br />
满足贝尔曼方程就说明, 这个策略对一个状态, 即使采取理论上的最佳行动, 也不会有更好的效果</p>
<h4 id="价值迭代">价值迭代</h4>
<p><code>最优性原理定理（principle of optimality theorem）</code>: 策略在状态 s 达到最优价值, 当且仅当对 s 可达的任何 s', 都已经达到了最优价值, 如果我们找到了初始状态 s 的最优价值, 那么自然就得到了最佳策略<br />
其过程为：</p>
<ol type="1">
<li>初始化: 对所有状态, 设置其价值为 0</li>
<li>循环直至收敛(k 为迭代次数)：
<ol type="1">
<li>对所有 s : <span class="math inline">\(Q_{k+1}(s,a)=R(s,a)+\gamma\sum_{s^{\prime}\in S}p\left(s^{\prime}\mid s,a\right)V_{k}\left(s^{\prime}\right)\)</span></li>
<li><span class="math inline">\(V_{k+1}(s)=\operatorname*{max}_{a}Q_{k+1}(s,a)\)</span></li>
</ol></li>
<li>最优策略 <span class="math inline">\(\pi(s)=\arg\operatorname*{max}_{a}\left[R(s,a)+\gamma\sum_{s^{\prime}\in S}p\left(s^{\prime}\mid s,a\right)V_{H+1}\left(s^{\prime}\right)\right]\)</span></li>
</ol>
<p>这可以单纯视为一个规划问题, 有点类似于计网中路由器网络寻找最短跳数, 由于每个状态的最优解都依赖于能到达它的其他状态是否最优, 而我们不知道具体迭代几次所有状态都能到达最优, 所以对每个状态, 不断地计算其能采取的行动的价值, 找到一个最优的 Q 作为其 V, 如果网络中存在没有达到最优的节点, 那么在某轮迭代中它就会被优化, 直到一轮中没有任何优化发生, 才算抵达了最优解<br />
设想一个类似“终点”的最优状态 s, s 有全局最佳的 V, 只要到达 s 就可以宣告游戏结束, 例如一个跑酷游戏的终点。那么对离 s 一步之遥的 <code>s'</code>, 迭代到它后, s'更新自己的价值为 <code>迈出最后一步(action)的回报+s的价值</code>(为了容易说明, 这里假设迈出这步到达 s 的概率是 1), 如果总共只有这两种状态, 由于 s 不会更好了, s'达到 s 也没有更好的路径, 继续迭代也不会更新, 问题解决了<br />
再复杂化一点, 假设有若干个状态, 其中 s 是 "终点", 到达 s 的行为会得到一个及其巨大的回报, 那么每次迭代, 可以通过行动到达 s 的 s'就能达到最优解, 这样依次递推, 直到离 s 最远的状态也找到了耗费最少的到达 s 的路径<br />
当然, 实际上, 很可能不存在这么一个方便的终点, 各个状态间会是互相传递更优解的情况, 但这种迭代的思路不会变化</p>
<h2 id="免模型方法">免模型方法</h2>
<h3 id="蒙特卡洛">蒙特卡洛</h3>
<p>实际情况中有很多不满足马尔科夫条件, 过程中回报难以获得, 或者 action 连续值无法取最佳之类的问题, 也就是说很难做出系统的理论, 因此需要不通过模型进行 rl 的方法。</p>
<ul>
<li><code>Q表格</code>: 以采样并记录的方法, 使用平均奖励估算每个状态下每个动作的价值<br />
</li>
<li><code>蒙特卡洛方法</code>: 简单地说, 就是大量采样估算状态价值, 简单地列式可得到 <span class="math inline">\(\mu_t=\mu_{t-1}+{\frac{1}{t}}\left(x_{t}-\mu_{t-1}\right)\)</span> 其中 <span class="math inline">\(\mu\)</span> 是到 t 时刻为止的平均值 <span class="math inline">\(x_{t}-\mu_{t-1}\)</span> 称为残差
<ul>
<li><code>增量式蒙特卡洛 （incremental MC）</code> : 用增量的方法更新数据 , <span class="math inline">\(\begin{array}{l}{ {N\left(s_{t}\right)\leftarrow N\left(s_{t}\right)+1}}\\ { {V\left(s_{t}\right)\leftarrow V\left(s_{t}\right)+\frac{1}{N(s_{t})}\left(G_{t}-V\left(s_{t}\right)\right)} }\end{array}\)</span> 其中 N 为状态 s 的访问次数</li>
</ul></li>
</ul>
<h3 id="时序差分">时序差分</h3>
<p>时序差分是免模型的在线算法</p>
<p><span class="math display">\[ V\left(s_{t}\right)\leftarrow V\left(s_{t}\right)+\alpha\left(r_{t+1}+\gamma V\left(s_{t+1}\right)-V\left(s_{t}\right)\right) \]</span></p>
<p>其中 <span class="math inline">\(r_{t+1}+\gamma V(s_{t+1})\)</span> 是称为时序差分目标(TD target)</p>
<p>时序差分误差（TD error）写作:</p>
<p><span class="math display">\[V\left(s_{t}\right)\leftarrow V\left(s_{t}\right)+\alpha\left(G_{i,t}-V\left(s_{t}\right)\right)\]</span></p>
<p>时序差分可以更改步数(走几步更新)</p>
<h2 id="理论问题">理论问题</h2>
<h1 id="模型">模型</h1>
<p>可以简单地讲选择 action 理解为分类问题, 损失函数使用交叉熵, 对不同的 s, 我们可能对执行的 action 有不同的期待, 因此可以设置一个损失函数的系数用于控制倾向性<br />
那么如何定义 reward 呢?最直接的即时 reward 肯定不是最优解, 因此设置一个 Gn 表示 an 行为之后的所有回报和, 但对之后的回报要乘上一个惩罚系数 γ, 其次数逐渐增加; 此外, 可以使用将 G 减去一个 baseline 的值来产生不同行为的倾向性<br />
rl, 准确地说是 on-policy 的 rl 与常见监督学习不同的是, 其训练资料在更新完一次模型后就无用了, 需要再次与环境互动收集资料<br />
off-policy 则有所不同, 它让训练模型和互动的模型分开, 从而让经验可复用, 例如 <code>Proximal Policy Optimization(PPO)</code></p>
<p>Critic actor: 我们希望有一个跟模型参数以及当前状态有关的函数 $ V^(s) $ 表示这种情况下的'价值', 可以理解为某个状态的预期回报<br />
为了得到这个价值函数, 可以怎么做呢:</p>
<ul>
<li>Monte-Carlo(MC): 较为直观的做法, 先训练大量的 s 及其 G 值, 然后用这些资料预测 V</li>
<li>Temporal-difference (TD): V 函数有以下性质:</li>
</ul>
<p><span class="math display">\[\begin{array}{l}{ {V^{\theta}(s_{t})=r_{t}+\gamma r_{t+1}+\gamma^{2}r_{t+2}\ldots} }\\ { {V^{\theta}(s_{t+1})=r_{t+1}+\gamma r_{t+2}+\ldots} }\\ { {V^{\theta}(s_{t})=\gamma V^{\theta}(s_{t+1})+r_{t} } }\end{array}\]</span><br />
那么 <span class="math display">\[V^{\theta}(s_{t})-\gamma V^{\theta}(s_{t+1})\leftrightarrow r_{t}\]</span><br />
通过这种关系就可以训练</p>
<p>现在我们有了 G 和 V, 两者区别是 G 有一个确定的 a, V 则假定 a 是随机的, 那么可以对每个 <span class="math inline">\(\{s_{t},a_{t}\}\)</span> 对, 可以定义一个 <span class="math inline">\(\mathrm{A}_{t}\,=\,G_{t}^{\prime}-V^{\theta}(s_{t})\)</span> , 用于表示这个行为的 "表现"<br />
也可以用 $r_{t}+V^{}(s_{t+1}) $ 代替上式的 $ G^{'}_t $ , 此式相当于采取行为 <span class="math inline">\(a_t\)</span> 后获得一个特定 <span class="math inline">\(r_t\)</span> 后的预期收益, 这称为 <code>Advantage Actor-Critic</code></p>
<p><code>Reward Shaping</code>: 对回馈比较模糊或者非常慢的场景来说, 想要好的表现就需要人为规定一些回报机制, 例如对游戏 ai 来说, 就需要惩罚什么都不做的行为(否则机器为了回报不为负就可能消极游戏); 其中一种机制称为 <code>Curiosity</code>, 也就是奖励机器发现新信息的情况</p>
<p>现实中不是所有情况都可以定义 reward, 此时只能通过人类提供示范来让机器学习, 这样当然会有很多问题, 比如数据集里没有错误示范等, 因此, 可能的思路是通过机器学习来找到 reward 再进行学习<br />
<code>Inverse Reinforcement Learning</code>: 假设老师行为是最优的, 在每次迭代中:</p>
<ol type="1">
<li>actor 与环境互动</li>
<li>定义一个奖励函数, 满足老师的奖励更优这个条件</li>
<li>actor 根据这个奖励函数进行学习</li>
</ol>
<h1 id="模仿学习">模仿学习</h1>
<p>缺点:</p>
<ol type="1">
<li>人类演示数据可能有很多模式, 不确定性较强, 且很可能不符合马尔科夫假设</li>
<li>演示数据往往更多倾向于成功的情况, 令模型没有从错误恢复的能力, 导致错误逐渐积累, 最终效果极差</li>
</ol>
<p>改进:</p>
<ol type="1">
<li>使用 rnn, lstm 等方法，通过上下文学习专家演示数据； 这样的缺点是, 由于 rl 很多场景都有较强的因果特征, 这种情况很可能会错误地归因</li>
<li>对于人类的复杂行为模式，可以使用多个高斯分布模拟，或者使用潜空间捕获特征(例如标准流); 以及 autoregressive discretization(自回归离散化)</li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E4%BC%AF%E5%85%8B%E5%88%A9/" rel="tag"><i class="fa fa-tag"></i> 伯克利</a>
              <a href="/tags/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/" rel="tag"><i class="fa fa-tag"></i> 课程笔记</a>
              <a href="/tags/python/" rel="tag"><i class="fa fa-tag"></i> python</a>
              <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"><i class="fa fa-tag"></i> 人工智能</a>
              <a href="/tags/%E5%9B%BD%E7%AB%8B%E5%8F%B0%E6%B9%BE%E5%A4%A7%E5%AD%A6/" rel="tag"><i class="fa fa-tag"></i> 国立台湾大学</a>
              <a href="/tags/cs285/" rel="tag"><i class="fa fa-tag"></i> cs285</a>
              <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 强化学习</a>
          </div>

        
  <div class="social-like a2a_kit a2a_kit_size_32 a2a_default_style">
    <a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a>
      <a class="a2a_button_facebook"></a>
      <a class="a2a_button_twitter"></a>
      <a class="a2a_button_wechat"></a>
      <a class="a2a_button_qzone"></a>
  </div>

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/thinklive/48061/" rel="prev" title="机器学习笔记 all in one">
                  <i class="fa fa-angle-left"></i> 机器学习笔记 all in one
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/thinklive/24757/" rel="next" title="一些工具的使用备忘录">
                  一些工具的使用备忘录 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2023 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">thinklive</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">624k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">37:51</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 技术支持
  </div><script defer src="/lib/three.js"></script><script defer src="/lib/lines.js"></script><script defer src="/lib/waves.js"></script>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.24/fancybox/fancybox.umd.js" integrity="sha256-oyhjPiYRWGXaAt+ny/mTMWOnN1GBoZDUQnzzgC7FRI4=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>


  <script src=""></script>
  <script src="/%5Bobject%20Object%5D"></script>
  <script src="/%5Bobject%20Object%5D"></script>
  <script src="/%5Bobject%20Object%5D"></script>
  <script src="/%5Bobject%20Object%5D"></script>


<script>
var options = {
  bottom: '64px', // default: '32px'
  right: 'unset', // default: '32px'
  left: '32px', // default: 'unset'
  time: '0.5s', // default: '0.3s'
  mixColor: '#fff', // default: '#fff'
  backgroundColor: '#fff',  // default: '#fff'
  buttonColorDark: '#100f2c',  // default: '#100f2c'
  buttonColorLight: '#fff', // default: '#fff'
  saveInCookies: true, // default: true,
  label: '🌓', // default: ''
  autoMatchOsTheme: true // default: true
}
const darkmode = new Darkmode(options);
darkmode.showWidget();
</script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>


  <script class="next-config" data-name="wavedrom" type="application/json">{"enable":true,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/wavedrom/3.3.0/wavedrom.min.js","integrity":"sha256-IRMDzTC+wK5stMucZ/XSXkeS5VNtxZ+/Bm8Mcqfoxdo="}}</script>
  <script class="next-config" data-name="wavedrom_skin" type="application/json">{"enable":true,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/wavedrom/3.3.0/skins/default.js","integrity":"sha256-fduc/Zszk5ezWws2uInY/ALWVmIrmV6VTgXbsYSReFI="}}</script>
  <script src="/js/third-party/tags/wavedrom.js"></script>

  <script src="/js/third-party/fancybox.js"></script>

  <script src="/js/third-party/pace.js"></script>

  <script src="/js/third-party/addtoany.js"></script>

  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":false,"archive":false,"delay":true,"timeout":3000,"priority":true,"url":"https://thinklive1.github.io/thinklive/37185/"}</script>
  <script src="/js/third-party/quicklink.js"></script>
<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

<script>
var options = {
  bottom: '64px',
  right: '32px',
  left: 'unset',
  time: '0.5s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: true,
  label: '🌓',
  autoMatchOsTheme: true
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
</script>
<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"thinklive1/blog_comments","issue_term":"pathname","theme":"photon-dark"}</script>
<script src="/js/third-party/comments/utterances.js"></script>
<script>
    const snowflakes = ["❄", "❄", "❆", "❅", "✥","❄", "❄", "❆", "❅", "✥","✻"];
    // 创建雪花
    function createSnowflake() {
        const snowflake = document.createElement("span");
        snowflake.classList.add("snowflake");
        const randomIndex = Math.floor(Math.random() * snowflakes.length);
        snowflake.textContent = snowflakes[randomIndex];
        
        // 起始位置
        /* 80%概率 生成在页面两侧 30% 的位置
        const probability = Math.random();
        let startPosition = Math.random() * 100;

        if (probability < 0.8) {
            startPosition = Math.random() < 0.5 ? Math.random() * 30 : (Math.random() * 30) + 70;
        }
        snowflake.style.left = `${startPosition}vw`;
        */
        snowflake.style.left = `${Math.random() * 100}vw`;
        snowflake.style.top = `-30px`;
        // 雪花大小与透明度
        const size = Math.random() * 18 + 10;
        snowflake.style.fontSize = `${size}px`;
        const opacity = Math.random() * 0.6 + (size > 18 ? 0.4 : 0);
        snowflake.style.setProperty("--opacity", opacity);
        // 动画持续时间
        const fallDuration = Math.random() * 10 + 10;
        // 旋转持续时间
        const rotateDuration = Math.random() * 3 + 1;

        snowflake.style.animationDuration = `${fallDuration}s, ${fallDuration}s`; // 向 CSS 添加淡出动画的持续时间
        // 横向幅度
        const translateX = (Math.random() * 500 - 200);
        snowflake.style.setProperty("--translateX", `${translateX}px`);
        // 纵向幅度
        snowflake.style.setProperty("--translateY", `${window.innerHeight}px`);

        document.body.appendChild(snowflake);
        // 移除雪花
        setTimeout(() => {
            snowflake.remove();
        }, fallDuration * 1000);
    }
    
    function snowfallAnimation() {
        // 载入时若边栏是隐藏状态则不加载雪花
        const sidebarnav = document.querySelector('.sidebar');
        const sidebarnavdisplay = window.getComputedStyle(sidebarnav).getPropertyValue('display'); 
        if (sidebarnavdisplay !== 'none') {
            createSnowflake();
        }
        setTimeout(snowfallAnimation, 500); // 生成速度，毫秒
    }
    snowfallAnimation();
function toggleMode() {
    console.log("change color!");
    const root1 = document.documentElement;

    // 检查当前 color-scheme
    const isLightMode = getComputedStyle(root1).getPropertyValue('--content-bg-color').trim() === '#fff';

    if (isLightMode) {
        // 切换到暗模式
        const images = document.querySelectorAll('img'); // 选择所有<img>标签
        images.forEach(img => {
            img.style.filter = 'brightness(50%)'; // 设置亮度为50%
        });

        root1.style.setProperty('--content-bg-color', '#333');
        root1.style.setProperty('--link-color', '#aaa');
        root1.style.setProperty('--text-color', '#fff');
        root1.style.setProperty('--highlight-background', '#444');
        root1.style.setProperty('--highlight-foreground', '#bbb');
        root1.style.setProperty('--btn-default-bg', '#777');
        root1.style.setProperty('--menu-item-bg-color', '#777');
        root1.style.setProperty('--table-row-odd-bg-color', '#444');
        root1.style.setProperty('--note-warning-bg-color', '#555');
        root1.style.setProperty('--note-bg-color', '#555');
        root1.style.setProperty('--note-info-bg-color', '#555');
        root1.style.setProperty('--highlight-gutter-foreground', '#98d9ffff');
        root1.style.setProperty('', '#777');
        root1.style.transition = 'all 0.5s ease';

    }

    else {
        const images = document.querySelectorAll('img'); // 选择所有<img>标签
        images.forEach(img => {
            img.style.filter = 'brightness(100%)'; // 设置亮度为50%
        });
        root1.style.setProperty('--content-bg-color', '#fff');
        root1.style.setProperty('--text-color', '#555');
        root1.style.setProperty('--highlight-background', '#eaeef3');
        root1.style.setProperty('--highlight-foreground', '#00193a');
        root1.style.setProperty('--btn-default-bg', '#fff');
        root1.style.setProperty('--menu-item-bg-color', '#f5f5f5');
        root1.style.setProperty('--note-warning-bg-color', '#fdf8ea');
        root1.style.setProperty('--note-bg-color', '#f9f9f9');
        root1.style.setProperty('--note-info-bg-color', '#eef7fa');
        root1.style.setProperty('--table-row-odd-bg-color', '#f9f9f9');
        root1.style.setProperty('--highlight-gutter-foreground', '#172e4c');
        root1.style.transition = 'all 0.5s ease';
    }
}

function DarkTrigger() {
    console.log('dark!!')
    let isDarkMode = getComputedStyle(document.documentElement).getPropertyValue('--content-bg-color').trim() === '#000';
    console.log(isDarkMode)
    if (isDarkMode) {
        // 切换到暗模式
        const warningNotes = document.querySelectorAll('.post-body .note.warning');
        // 修改背景颜色
        warningNotes.forEach(note => {
        note.style.background = '#666';
        });

        const infoNotes = document.querySelectorAll('.post-body .note.info');
        // 修改背景颜色
        infoNotes.forEach(note => {
        note.style.background = '#666';
        });
    }
}


</script>

 <!--js: 线条特效-->
  <script type="text/javascript" color="255,255,255" opacity='1' zIndex="-1" count="50" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>

<button style="background: #868686;
  width: 3rem;
  height: 3rem;
  position: fixed;
  border-radius: 50%;
  border: none;
  right: unset;
  bottom: 2rem;
  left: 2rem;
  cursor: pointer;
  transition: all 0.5s ease;
  display: flex;
  justify-content: center;
  align-items: center;" class="darkmode-toggle" role="checkbox" onclick="toggleMode()">🌓</button>

  <video autoplay loop muted playsinline style="position:fixed;top:50%;opacity: 0.8;left:50%;min-width:100%;min-height:100%;transform:translateX(-50%)translateY(-50%);z-index:-2;">
  <source src="/images/red.mp4" type="video/mp4">
<!-- hexo injector body_end start --><script src="/assets/mmedia/mmedia-loader.js"></script><!-- hexo injector body_end end --></body>
</html>
