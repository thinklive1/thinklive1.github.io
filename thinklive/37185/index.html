<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.24/fancybox/fancybox.css" integrity="sha256-vQkngPS8jiHHH0I6ABTZroZk8NPZ7b+MUReOFE9UsXQ=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"thinklive1.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"flat"},"fold":{"enable":true,"height":500},"bookmark":{"enable":true,"color":"#66CCFF","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":"utterances","storage":true,"lazyload":false,"nav":null,"activeClass":"utterances"},"stickytabs":true,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="cs285">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习笔记 all in one">
<meta property="og:url" content="https://thinklive1.github.io/thinklive/37185/index.html">
<meta property="og:site_name" content="thinklive">
<meta property="og:description" content="cs285">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-24.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-23.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-25.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-26.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-27.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-28.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-29.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-30.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-31.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-32.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/image-33.png">
<meta property="og:image" content="https://thinklive1.github.io/assets/ml/52243716-045d-4e27-ad0c-0e84593fee81.png">
<meta property="og:image" content="https://datawhalechina.github.io/easy-rl/img/ch2/2.10.png">
<meta property="article:published_time" content="2025-09-09T07:53:29.840Z">
<meta property="article:modified_time" content="2025-11-07T09:15:46.287Z">
<meta property="article:author" content="thinklive">
<meta property="article:tag" content="伯克利">
<meta property="article:tag" content="课程笔记">
<meta property="article:tag" content="python">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="国立台湾大学">
<meta property="article:tag" content="cs285">
<meta property="article:tag" content="强化学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://thinklive1.github.io/assets/ml/image-24.png">


<link rel="canonical" href="https://thinklive1.github.io/thinklive/37185/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://thinklive1.github.io/thinklive/37185/","path":"thinklive/37185/","title":"强化学习笔记 all in one"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>强化学习笔记 all in one | thinklive</title>
  







<script type="text/javascript" async src="/js/fairyDustCursor.js"></script>
<script type="text/javascript" async src="/js/tab-title.js"></script>
<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>
<script src="/js/tab-title.js"></script>

<!--pjax：防止跳转页面音乐暂停-->
<script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script>
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
  <script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>
<script>
const options = {
  bottom: '64px', // default: '32px'
  right: 'unset', // default: '32px'
  left: '32px', // default: 'unset'
  time: '0.5s', // default: '0.3s'
  mixColor: '#fff', // default: '#fff'
  backgroundColor: '#fff',  // default: '#fff'
  buttonColorDark: '#100f2c',  // default: '#100f2c'
  buttonColorLight: '#fff', // default: '#fff'
  saveInCookies: false, // default: true,
  label: '🌓', // default: ''
  autoMatchOsTheme: true // default: true
}

const darkmode = new Darkmode(options);
</script>
<!-- hexo injector head_end start --><script> let HEXO_MMEDIA_DATA = { js: [], css: [], aplayerData: [], metingData: [], artPlayerData: [], dplayerData: []}; </script><!-- hexo injector head_end end --><link rel="alternate" href="/atom.xml" title="thinklive" type="application/atom+xml">
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>
<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>


  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">thinklive</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">dirichlet library</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索 | search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-主页-|-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>主页 | home</a></li><li class="menu-item menu-item-标签-|-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签 | tags</a></li><li class="menu-item menu-item-分类-|-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类 | categories</a></li><li class="menu-item menu-item-归档-|-archive"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档 | archive</a></li><li class="menu-item menu-item-相册-|-photo"><a href="/photos/" rel="section"><i class="fa fa-camera fa-fw"></i>相册 | photo</a></li><li class="menu-item menu-item-留言-|-guestbook"><a href="/guestbook/" rel="section"><i class="fa fa-book fa-fw"></i>留言 | guestbook</a></li><li class="menu-item menu-item-感谢-|-thank"><a href="/thanks/" rel="section"><i class="fa custom thanks fa-fw"></i>感谢 | thank</a></li><li class="menu-item menu-item-游戏-|-game"><a href="/game/bad1.html" rel="section"><i class="fa fa-gamepad fa-fw"></i>游戏 | game</a></li><li class="menu-item menu-item-神龛-|-shrine"><a href="/cyberblog/" rel="section"><i class="fa fa-microchip fa-fw"></i>神龛 | shrine</a></li><li class="menu-item menu-item-资源地图-|-resourcemap"><a href="/webstack/" rel="section"><i class="fa fa-list fa-fw"></i>资源地图 | resourcemap</a></li><li class="menu-item menu-item-思维导图-|-mindmap"><a href="/mindmap/index.html" rel="section"><i class="fa fa-map fa-fw"></i>思维导图 | mindmap</a></li><li class="menu-item menu-item-网站地图-|-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>网站地图 | sitemap</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索 | search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">


<!--网易云音乐插件-->
<!-- require APlayer -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css">
<script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script>
<!-- require MetingJS-->
<script src="https://cdn.jsdelivr.net/npm/meting@2.0.1/dist/Meting.min.js"></script> 
<!--网易云playlist外链地址-->   
<meting-js
    server="netease"
    type="playlist" 
    id="2762741085"
    mini="false"
    fixed="false"
    list-folded="true"
    autoplay="false"
    volume="0.2"
    theme="#4c4c4c"
    order="random"
    loop="all"
    preload="auto"
    mutex="true">
    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5"><span class="nav-number">1.</span> <span class="nav-text">基础概念</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.1.</span> <span class="nav-text">模仿学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#hw1"><span class="nav-number">1.1.1.</span> <span class="nav-text">hw1</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="nav-number">1.2.</span> <span class="nav-text">策略梯度</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E5%92%8C%E6%94%B9%E8%BF%9B"><span class="nav-number">1.2.1.</span> <span class="nav-text">问题和改进</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7"><span class="nav-number">1.2.2.</span> <span class="nav-text">重要性采样</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#actor-critic"><span class="nav-number">1.3.</span> <span class="nav-text">actor-critic</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#off-policy-a-c"><span class="nav-number">1.3.1.</span> <span class="nav-text">off-policy a-c</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9B%B4%E5%A5%BD%E7%9A%84-baseline"><span class="nav-number">1.3.2.</span> <span class="nav-text">更好的 baseline</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E4%BB%B7%E5%80%BC%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">1.4.</span> <span class="nav-text">基于价值的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%94%B6%E6%95%9B%E6%80%A7"><span class="nav-number">1.4.1.</span> <span class="nav-text">收敛性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%80%E9%99%90"><span class="nav-number">1.4.2.</span> <span class="nav-text">局限</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%94%B9%E8%BF%9B"><span class="nav-number">1.4.3.</span> <span class="nav-text">改进</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%9E%E7%BB%AD%E5%8A%A8%E4%BD%9C"><span class="nav-number">1.4.4.</span> <span class="nav-text">连续动作</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E8%BF%87%E7%A8%8B"><span class="nav-number">1.5.</span> <span class="nav-text">马尔可夫过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%B3%E7%AD%96"><span class="nav-number">1.5.1.</span> <span class="nav-text">决策</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%87%E4%BB%BD%E5%9B%BE"><span class="nav-number">1.5.2.</span> <span class="nav-text">备份图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%AD%E4%BB%A3"><span class="nav-number">1.5.3.</span> <span class="nav-text">迭代</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3"><span class="nav-number">1.5.3.1.</span> <span class="nav-text">策略迭代</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3"><span class="nav-number">1.5.3.2.</span> <span class="nav-text">价值迭代</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%8D%E6%A8%A1%E5%9E%8B%E6%96%B9%E6%B3%95"><span class="nav-number">1.6.</span> <span class="nav-text">免模型方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B"><span class="nav-number">1.6.1.</span> <span class="nav-text">蒙特卡洛</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86"><span class="nav-number">1.6.2.</span> <span class="nav-text">时序差分</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%90%86%E8%AE%BA%E9%97%AE%E9%A2%98"><span class="nav-number">1.7.</span> <span class="nav-text">理论问题</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.</span> <span class="nav-text">模型</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="thinklive"
      src="/images/thive.png">
  <p class="site-author-name" itemprop="name">thinklive</p>
  <div class="site-description" itemprop="description">起初，世界是一团思索，它向所有的方向迈了一步，于是万物由此而生</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">45</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">50</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/thinklive1" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;thinklive1" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/t469631989@gmail.com" title="E-Mail → t469631989@gmail.com" rel="noopener me"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/38099250?spm_id_from=333.1007.0.0" title="bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;38099250?spm_id_from&#x3D;333.1007.0.0" rel="noopener me" target="_blank"><i class="fa custom bilibili fa-fw"></i>bilibili</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://steamcommunity.com/id/thinkliving" title="steam → https:&#x2F;&#x2F;steamcommunity.com&#x2F;id&#x2F;thinkliving" rel="noopener me" target="_blank"><i class="fa custom steam fa-fw"></i>steam</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml" rel="noopener me"><i class="fa fa-rss fa-fw"></i>RSS</a>
      </span>
  </div>

<div style="Text-align:center;width:100%"><div style="margin:0 auto"><canvas id="canvas" style="width:60%" height="100" width="700">当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>!function(){function t(t,e){for(var a=0;a<l[e].length;a++)for(var n=0;n<l[e][a].length;n++)1==l[e][a][n]&&(h.beginPath(),h.arc(14*(g+2)*t+2*n*(g+1)+(g+1),2*a*(g+1)+(g+1),g,0,2*Math.PI),h.closePath(),h.fill())}function e(){var t=[],e=/(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date),a=[];a.push(e[1],e[2],10,e[3],e[4],10,e[5],e[6]);for(var r=c.length-1;r>=0;r--)a[r]!==c[r]&&t.push(r+"_"+(Number(c[r])+1)%10);for(var r=0;r<t.length;r++)n.apply(this,t[r].split("_"));c=a.concat()}function a(){for(var t=0;t<d.length;t++)d[t].stepY+=d[t].disY,d[t].x+=d[t].stepX,d[t].y+=d[t].stepY,(d[t].x>i+g||d[t].y>f+g)&&(d.splice(t,1),t--)}function n(t,e){for(var a=[1,2,3],n=["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"],r=0;r<l[e].length;r++)for(var o=0;o<l[e][r].length;o++)if(1==l[e][r][o]){var h={x:14*(g+2)*t+2*o*(g+1)+(g+1),y:2*r*(g+1)+(g+1),stepX:Math.floor(4*Math.random()-2),stepY:-2*a[Math.floor(Math.random()*a.length)],color:n[Math.floor(Math.random()*n.length)],disY:1};d.push(h)}}function r(){o.height=100;for(var e=0;e<c.length;e++)t(e,c[e]);for(var e=0;e<d.length;e++)h.beginPath(),h.arc(d[e].x,d[e].y,g,0,2*Math.PI),h.fillStyle=d[e].color,h.closePath(),h.fill()}var l=[[[0,0,1,1,1,0,0],[0,1,1,0,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,0,1,1,0],[0,0,1,1,1,0,0]],[[0,0,0,1,1,0,0],[0,1,1,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[1,1,1,1,1,1,1]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,0,0,1,1],[1,1,1,1,1,1,1]],[[1,1,1,1,1,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,0,1,1,1,0],[0,0,1,1,1,1,0],[0,1,1,0,1,1,0],[1,1,0,0,1,1,0],[1,1,1,1,1,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,1,1]],[[1,1,1,1,1,1,1],[1,1,0,0,0,0,0],[1,1,0,0,0,0,0],[1,1,1,1,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[1,1,1,1,1,1,1],[1,1,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,1,1,0,0,0,0]],[[0,0,0,0,0,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,0,0,0]]],o=document.getElementById("canvas");if(o.getContext){var h=o.getContext("2d"),f=100,i=700;o.height=f,o.width=i,h.fillStyle="#f00",h.fillRect(10,10,50,50);var c=[],d=[],g=o.height/20-1;!function(){var t=/(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date);c.push(t[1],t[2],10,t[3],t[4],10,t[5],t[6])}(),clearInterval(v);var v=setInterval(function(){e(),a(),r()},50)}}()</script></div>
<img class= 'logo' src="/images/thinklive_cyber.png"; z-index: '0'; style="max-width: 100%; width: auto; height: auto;background-color: --content-bg-color;">

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://thinklive1.github.io/thinklive/37185/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/thive.png">
      <meta itemprop="name" content="thinklive">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="thinklive">
      <meta itemprop="description" content="起初，世界是一团思索，它向所有的方向迈了一步，于是万物由此而生">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="强化学习笔记 all in one | thinklive">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          强化学习笔记 all in one
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-09-09 15:53:29" itemprop="dateCreated datePublished" datetime="2025-09-09T15:53:29+08:00">2025-09-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-07 17:15:46" itemprop="dateModified" datetime="2025-11-07T17:15:46+08:00">2025-11-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">课程笔记</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>12k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>44 分钟</span>
    </span>
</div>

        
        </div>
      </header>
   

    
    
    
    <div class="post-body" itemprop="articleBody"><p><a target="_blank" rel="noopener" href="https://rail.eecs.berkeley.edu/deeprlcourse/">cs285</a></p>
<span id="more"></span>
<h1 id="基础概念">基础概念</h1>
<p>强化学习和监督学习的区别：</p>
<ol type="1">
<li>强化学习输入的样本是序列数据, 监督学习的样本之间相互独立</li>
<li>没有明确的监督者, 通过奖励机制进行学习, 但回馈可能是长期的, 模糊的</li>
</ol>
<p>一些强化学习的演示视频中, ai 会做一些人类看来无意义的动作, 正是这种“玄学”的回馈机制导致的</p>
<ul>
<li>actor: 行为主体
<ul>
<li>action 则可分为离散和连续, 例如 2d 游戏中走格子迷宫就是一个典型的离散动作空间</li>
</ul></li>
<li>observaton o /states s: 观测与状态
<ul>
<li>观测到的情况 o 和现实情况（状态 s）其实有可能不同, 假设可以观察到全景, rl 则成为一个马尔科夫决策过程</li>
</ul></li>
<li>policy π: 行为策略
<ul>
<li>带有参数 θ</li>
</ul></li>
<li>reward：反馈
<ul>
<li>baseline: 避免总是正值的 reward, 增加的偏置值, 例如取期望</li>
</ul></li>
<li>episode: 一轮行动</li>
<li>trajectory τ: <span class="math inline">\(\tau=\{s_{1},a_{1},s_{2},a_{2},\cdots,s_{T},a_{T}\}\)</span></li>
<li>折扣 γ: 直觉上, 最开始的训练回馈可能更重要, 越往后则训练收益越小, 所以对每步的回馈可以乘以一个 <span class="math inline">\(\gamma^{t-1}\)</span>, t 为训练次数, 这个超参数也可以用于控制训练策略偏短期还是偏长期</li>
</ul>
<p>流程： env(s1)-&gt; actor(a1)-&gt; env(s2)……<br />

$\begin{array}{l}{ {p_{\theta}(\tau)} }{ { {}=p(s_{1})p_{\theta}(a_{1}|s_{1})p(s_{2}|s_{1},a_{1})p_{\theta}(a_{2}|s_{2})p(s_{3}|s_{2},a_{2})\cdots} } \\   =p(s_{1})\prod_{t=1}^{T}p_{\theta}(a_{t}|s_{t})p(s_{t+1}|s_{t},a_{t})\end{array}$  
</p>
<p>同一轮中每对(s, a)产生一个 reward, 其期望为: <span class="math inline">\(\bar{R}_{\theta}=\sum_{\tau}R(\tau)p_{\theta}(\tau)=E_{\tau\cap P_{\theta}(\tau)}[R(\tau)]\)</span></p>
<h2 id="模仿学习">模仿学习</h2>
<p>缺点:</p>
<ol type="1">
<li>人类演示数据可能有很多模式, 不确定性较强, 且很可能不符合马尔科夫假设</li>
<li>演示数据往往更多倾向于成功的情况, 令模型没有从错误恢复的能力, 导致错误逐渐积累, 最终效果极差</li>
</ol>
<p>改进:</p>
<ol type="1">
<li>使用 rnn, lstm 等方法，通过上下文学习专家演示数据； 这样的缺点是, 由于 rl 很多场景都有较强的因果特征, 这种情况很可能会错误地归因</li>
<li>对于人类的复杂行为模式，可以使用多个高斯分布模拟，或者使用潜空间捕获特征(例如标准流); 以及 autoregressive discretization(自回归离散化)</li>
</ol>
<p>一个比较常见的算法称为<code>Dagger</code> , 其流程可以写作如下:</p>
<ol type="1">
<li>首先使用专家演示的行为来训练一个初始策略</li>
<li>然后让该策略在环境中执行,并记录下其在整个状态空间中访问过的状态-动作对</li>
<li>这些状态-动作对让专家进行标注, 将得到的新数据放入数据池</li>
<li>使用这个新的训练集,再次训练出一个更加接近专家的策略</li>
<li>重复上述过程,直到策略收敛</li>
</ol>
<h3 id="hw1">hw1</h3>
<p>训练目标可以表示为以下不等式:</p>
<p><span class="math display">\[\mathbb{E}_{p_{\pi}*(s)}\pi_{\theta}(a\not=\pi^{*}(s)\mid s)={\frac{1}{T}}\sum_{t=1}^{T}\mathbb{E}_{p_{\pi}*(s_{t})}\pi_{\theta}(a_{t} \not =\pi^{*}(s_{t})\mid s_{t})\leq\varepsilon,\]</span></p>
<p>即: 给定随机专家轨迹与其状态分布, 在相同状态下训练策略产生的动作期望和专家策略产生的动作期望差值小于 <span class="math inline">\(\epsilon\)</span> , 其中 <span class="math inline">\(p_{\pi}(s_{t})\)</span> 表示给定π在时间t的状态分布</p>
<h2 id="策略梯度">策略梯度</h2>
<p>rl 的目标就是找一个最大化期望回报的策略 π, 等价于找到最好的策略参数 θ, 可以使用和机器学习类似的梯度下降, 只不过这个梯度比较特殊，下面引用 cs285 的 ppt:</p>
<p><img src="/assets/ml/image-24.png" /> <img src="/assets/ml/image-23.png" /> 上图 1 中的 J 表示根据确定的 θ 和 π, 得到的轨迹 τ 对应回报的期望, 使用一个 trick, 将概率的梯度转为其自身和 log 梯度的乘积, 这是为了凑出数学期望的形式<br />
再看上图 2, 其实 τ 的概率是一系列 a, s 概率的乘积, 对其 log 转为加法, 得到的项中, 终于根据 s 得到 a 的概率是和 θ 有关的(这也完全符合常识), 于是最后我们所求的梯度其实只有对 π 求微分这个计算量比较大的项<br />
为什么我们想要数学期望的形式？ 因为对此我们可以通过大量采样的平均来估算，例如对 J 就可以用 n 个轨迹回报的均值估算, 而对 J 的梯度, 类似地有:</p>
<p><span class="math display">\[\nabla_{\theta}J(\theta)\approx\frac{1}{N}\sum_{i=1}^{N}\left(\sum_{t=1}^{T}\nabla_{\theta}\log\pi_{\theta}({\bf a}_{i,t}|{\bf s}_{i,t})\right)\left(\sum_{t=1}^{T}r({\bf s}_{i,t},{\bf a}_{i,t})\right)\]</span><br />
这个式子看上去有点吓人，其实它只是单纯地用旧策略的 log 梯度乘对应的回报值, 也就是说在进行若干轮采样后, 总体上好的回报, 就加上(当然这里有一个学习率超参数)对应的好的参数, 坏的回报就减去坏的参数<br />
再考虑一下这个梯度的数学性质, 首先很明显的是, 回报和的绝对值越大, 更新幅度就会越大, 这很符合常识, 但同时, 考虑 log 的性质, 对较小的值其导数的绝对值越大, 也就是说对一个策略 π 来说, 其不怎么倾向于采取的动作反而会得到更多的更新梯度, 这可以视为一种 exploration<br />
此外，对部分可观测的马尔科夫假设, 这里不作具体推导, 但只要把上面的 s 换成 o 就能沿用梯度策略<br />
和 log 函数的很多应用场景一样, 这里实际上可以理解为分类问题, 因为模型输出的动作是有限的, 策略最后产生的是动作空间中的概率分布, 稍微不同的是需要乘回报值, 并且要分很多轮训练</p>
<h3 id="问题和改进">问题和改进</h3>
<p>基于策略梯度的训练方式非常直观，但相应地也有问题, 即它结果会非常不稳定<br />
我们寻找的策略会不断往最高的回报前进, 这取决于我们每轮采样的回报, 这里我们其实是希望得到相对最优的一个目标, 但如果假设我们得到回报的相对倾向性较稳定, 但不同采样的绝对差距较大, 就会导致虽然以最优化策略为目标，最后得到的参数却相差非常大(方差大)的情况</p>
<ol type="1">
<li>因果关系: 这基于一个永恒的真理, 未来不会影响过去, 在 rl 中也就是说： 时间 t'之后得到的回报无论是好是坏都不该影响 t'之前的策略，对应在之前的方法中, 就是让回报总和一项从 t 开始计算:<br />
这听起来有点绕, 举个例子, 打对战游戏的前 t 把都输了, 于是 t+1 把转变思路赢了游戏, 假如按未改进的版本, 由于前 t 把输得太多，最后所有 t+1 局游戏的回报还是负的, 因此第 t+1 把的思路也是错的, 但是 t+1 把的思路的回报是正的, 也就是说这时的思路是正确的, 对游戏策略理应有正确影响，按改进后的方法, 我们认为 t+1 局的游戏思路期望回报为正</li>
</ol>
<p><span class="math display">\[\nabla_{\theta}J(\theta)\approx\frac{1}{N}\sum_{i=1}^{N}\left(\sum_{t=1}^{T}\nabla_{\theta}\log\pi_{\theta}({\bf a}_{i,t}|{\bf s}_{i,t})\right)\left(\sum_{t&#39;=t}^{T}r({\bf s}_{i,t&#39;},{\bf a}_{i,t&#39;})\right)\]</span></p>
<p>由于这里我们减少了回报的绝对值大小，因此其 θ 最后的方差必然会缩减, 但这么做对期望是无偏的(对策略梯度加减任何常数偏移值， 都会保持无偏), 也就是不会改变更新梯度，相当于只改变了其系数(回报和)的大小</p>
<ol start="2" type="1">
<li>baseline</li>
</ol>
<p>我们的目的是找到最优化的策略参数, 实际上只要回报为正, 就会加上对应参数的梯度, 这种更新策略其实是“找到回报为正的参数”, 还是拿打游戏举例子, 如果目标是打到全国第一, 那么和低水平对手的对局, 就算赢了这局的思路也不值得学习<br />
因此可以考虑将总回报减去一个 baseline, 简称为 b, b 一般取回报的平均值<br />
事实上，这对梯度的期望是无偏的， 即减去 b 和不减去两者的梯度一致, 但会减少方差, 最优的 b 可以通过对方差算关于 b 的极值求出来, 由于不太常用省去过程, 最优 b 为:</p>
<p><span class="math display">\[b={\frac{E[g(\tau)^{2}r(\tau)]}{E[g(\tau)^{2}]}}\]</span></p>
<h3 id="重要性采样">重要性采样</h3>
<p>策略梯度是 on-policy 的, 也就是每次需要收集数据才能更新, 数据与策略是绑定的, 这样的缺点是更新很慢，效率低<br />
如果想用 off-policy, 则需要重要性采样</p>
<div class="pdf-container" data-target="/assets/ml/rl_lec5.pdf" data-height="800px"></div>
<p>暂时忽略被划掉的一项(这是因为我们不想让这种指数级别的项增大方差) 尽可能简单地说明为什么要用这种采样方法: 不谈证明, 但是重要性采样有个性质是不会改变采样的均值(期望), 只会改变方差, 一般来说会将其用于改进采样的方差, 和名字一样, 可以理解为用不同的重要性对一个分布采样, 也就是分布不变，只改变采样方式, 来获得符合要求的采样数据<br />
但在策略梯度中, 问题在于我们改进过程中策略的分布和原始数据时的分布是不同的， 想实现 off-policy, 就需要用来自不同策略的数据来改进当前策略, 所以我们需要重要性采样这样的技巧, 不太严谨地说， 将不同策略的数据转化为当前策略的数据并用于训练， 又或者理解为用不同策略的数据计算对当前策略的更新梯度</p>
<p>此外对策略梯度还有的问题是, 对不同的参数 θ, 其梯度的数量级可能完全不同，需要进行一定约束, 例如约束 θ 分布前后的散度, 从而平衡不同方向的更新速率<br />
这方面具体实现上比较复杂，暂时只需要知道大概原理即可</p>
<h2 id="actor-critic">actor-critic</h2>
<p>由类似我们前文提到的 baseline 思想, 定义 A(advantage)= Q-V, 也就是状态动作对的期望奖励-状态的期望奖励, 这个标准能评估对当前状态来说, 采取的动作是好是坏<br />
我们可以将 A 替代掉之前的期望回报, 但需要注意的是, 由于实际中很多时候 A 是估算的, 这会带来方差的优化，但很可能让更新梯度有偏; 尽管如此, 一般我们还是认为拿大量减少方差换一些偏见是划算的<br />
Q 可以写为当前(s, a)的回报加上基于未来状态分布的 V 的期望, 可以用直接的 <span class="math inline">\(s_{t+1}\)</span> 近似这个期望, 写成 <span class="math inline">\(Q^{\pi}(\mathbf{s}_{t},\mathbf{a}_{t})\approx r(\mathbf{s}_{t},\mathbf{a}_{t})+V^{\pi}(\mathbf{s}_{t+1})\)</span> , 于是:<br />
<span class="math display">\[A^{\pi}(\mathbf{s}_{t},\mathbf{a}_{t})\approx r(\mathbf{s}_{t},\mathbf{a}_{t})+V^{\pi}(\mathbf{s}_{t+1})-V^{\pi}(\mathbf{s}_{t})\]</span> <span class="math display">\[\nabla_{\theta}J(\theta)\approx\frac{1}{N}\nabla_{i=1}^{N}\sum_{t=1}^{T}\Sigma_{\theta}\log\pi_{\theta}({\bf a}_{i,t}|{\bf s}_{i,t})A^{\pi}({\bf s}_{i,t},{\bf a}_{i,t})\]</span> 这种近似的好处是, 相比涉及动作空间的 Q, V 更好计算, 近似后我们唯一需要算的就是 V 了, 而之前的 term: <code>J</code>, 可以视为根据初始状态 s1 分布的 V(s1)的期望; 此外, 由于只用一个样本估算, 也能减少方差<br />
对 V 来说最方便的估算就是使用蒙特卡洛方法, 尤其在模拟环境或者电子游戏之类的场景下, 可以大量采样 <span class="math inline">\(V^{\pi}(\mathbf{s}_{t})\approx\sum_{t^{\prime}=t}^{T}r(\mathbf{s}_{t^{\prime}},\mathbf{a}_{t^{\prime}})\)</span><br />
但也可以使用一个估算函数, 即 <code>function approximation</code>, 这是个典型的监督回归问题, 可以用神经网络解决, 数据集为 n 轮训练中的 <span class="math inline">\(s_{i,t}\)</span> 与其对应的从 t 开始的回报和 <span class="math inline">\(\left\{\left({\bf s}_{i,t},r({\bf s}_{i,t},{\bf a}_{i,t})+\hat{V}_{\phi}^{\pi}({\bf s}_{i,t+1})\right)\right\}\)</span> , loss 设置为平方误差和 <span class="math inline">\(\mathcal{L(\phi)}=\frac{1}{2}\sum_{i}\left|\left|\hat{V}_{\phi}^{\pi}(\mathbf{s}_{i})-y_{i}\right|\right|^{2}\)</span><br />
基于神经网络的方法的好处是, 网络能一定程度上捕获数据间的关系, 由一定的泛化能力, 也就是可能能够平滑化输出的结果, 即类似的状态应该有类似的期望回报, 从而减少方差; 缺点则是不能保证是无偏的</p>
<p>以上我们提到的方法就是标题里的 a-c, 如果我们再引入一个折扣因子，就可以表述为以下的流程:</p>
<p><img src="/assets/ml/image-25.png" /></p>
<p>这个版本的 a-c 其实非常好理解, 它就是用一个单独的神经网络预测 V，用于帮助另一个训练策略 π 的网络<br />
但问题在于这两个网络只有输入是相同的(s), 其特征是完全分开的, 这可能让它的性能受限, 一种改进是共享网络, 即用一个网络接受 s，同时生成 V 和 π<br />
目前的这个版本是 on-line 的, 这就意味着, 每次只能用一个数据来更新网络, 如果想实现 batch， 则需要并行化, 具体来说可以分为同步异步两种情况<br />
<img src="/assets/ml/image-26.png" /> 如上图, 同步的情况很好理解, 就是每次让一群线程获取行动后的数据, 每次都同步更新<br />
异步的情况下, 不同线程可能有不同的运行速度, 并各自用收集到的数据更新网络参数, 这样的效率更好, 但缺点是, 较慢的线程更新时可能遇上新版本的 actor, 也就是说会有这种用非当前 actor 的数据更新它的问题, 当然一般来说因为线程之间的速度不会有特别大的差别, 影响相对不大</p>
<h3 id="off-policy-a-c">off-policy a-c</h3>
<p>在 off-policy 情况下, 存储之前的状态转移数据为 <code>replay buffer</code>, 这些数据可能来自于很久之前的策略, 这会带来两个问题:</p>
<ol type="1">
<li>批评家对 V 的评估是基于旧策略的</li>
<li>策略更新时使用的策略梯度也是来自于旧策略的</li>
</ol>
<p>对此, 我们可以把 V 给扔掉, 转向 Q 函数, <span class="math inline">\({ Q}^{\pi}(\mathrm{s}_{t}, a_t)\stackrel{\_}{=}\sum_{t^{\prime}=t}^T E_{\pi_{\theta}}\ [r(\mathrm{s}_{t^{\prime}},a_{t^{\prime}})]\mathrm{s}_{t}, a_{t}]\)</span> 那么为什么要这么做? 考虑一下现在我们有什么数据, 即来自旧策略下的 V 函数, V 函数的意义是, 各个状态的期望回报, 也就是我们知道各个状态的价值, 而这些价值相对来说不会随着策略变动而改变<br />
新策略相比旧策略改动的只有给定 s 条件下 a 的分布, 也就是说给定旧策略的 s, 可以查到在新策略在相同给定状态 s 下产生的动作概率分布, 从而算出新策略对应的 Q 函数<br />
于是 off-line 方法可以写成:</p>
<p><img src="/assets/ml/image-27.png" /> 其中, <span class="math inline">\(a_{i}&#39;\)</span> 与 <span class="math inline">\(a_{i}^{\pi}\)</span> 是根据 <span class="math inline">\(s_{i}&#39;\)</span> 从新策略中产生的动作<br />
实际中一般用 Q 代替之前的 A，这是因为这种方法能提供大量样本, 从而弥补方差的问题<br />
那么 offline 是毫无损失的方法吗? 其实并不是, 因为对来自旧策略的旧状态 s 我们无能为力, 这必然会产生一些偏差, 但基本来说处于可以接受的范围内</p>
<h3 id="更好的-baseline">更好的 baseline</h3>
<p>上面提到, 基于神经网络的评论家能实现有偏下的低方差, 策略梯度则是无偏的高方差, 那么有办法得到无偏的低方差更新梯度吗?<br />
下图展示一种估计策略梯度的方法, 它基于一个原理, 对梯度乘以任何一个值依赖状态而不依赖动作的函数，都能保持无偏, 选取价值函数作为 baseline, 由于我们希望估计的就是 V 的期望，选择价值函数作为 baseline 应当能大量减少方差</p>
<p><img src="/assets/ml/image-28.png" /> 那么如果用 Q 函数作为基线呢? 如果这样的话, 理论上讲我们要求的 A 会在 0 的上下浮动, 这样不是能让方差达到理想情况吗? 但问题在于, 由于这样不无偏, 那就需要加上一个误差修正项, 我们将得到一个吓人的式子:</p>
<p><span class="math display">\[\nabla_{\theta}J(\theta)\approx\frac{1}{N}\sum_{i=1}^{N}\sum_{t=1}^{T}\nabla_{\theta}\log\pi_{\theta}({\bf a}_{i,t}|{\bf s}_{i,t})\left(\hat{Q}_{i,t}-Q_{\phi}^{\pi}({\bf s}_{i,t},{\bf a}_{i,t})\right) +\frac{1}{N}\sum_{i=1}^{N}\sum_{t=1}^{T}\nabla_{\theta}E_{\mathrm{a}\sim\pi_{\theta}({\bf a}_{t}|{\bf s}_{i,t})}\left[Q_{\phi}^{\pi}({\bf s}_{i,t},{\bf a}_{t})\right]\]</span><br />
这个式子看起来似乎根本用不了, 但其实也有技巧能用成本可控的方式估计误差修正项</p>
<p>还有一种比较常见的思路是取 n 步的回报，由于越往后数据越可能发散, 这样也能降低方差, 这里不多介绍</p>
<h2 id="基于价值的方法">基于价值的方法</h2>
<p>最直观的说, 策略就是从动作空间中选一个最佳的动作, 这个最佳一般用 Q 函数表示, 而 Q 依赖于 V, 从 Q 或者 V 的角度出发, 强化学习的问题就是选择一个最佳的 Q 与其对应的 A, 再有足够样本的情况下, 估算 Q 可以用 V 矩阵实现, 因此这称为基于价值的方法(value-based v-b)<br />
很明显, 现实中经常有很大或者无穷的动作空间, 因此基于表格的 V-B 不现实, 类似之前的 a-c, 可以训练一个估计 Q 函数的网络, 定义其 loss 为: <span class="math inline">\(\mathcal{L}(\phi)=\frac{1}{2}\left|\left|V_{\phi}(\mathbf{s})-\operatorname*{max}Q^{\pi}(\mathbf{s},\mathbf{a})\right|\right|^{2}\)</span><br />
但是如果我们想迭代 Q, 还有一个严峻的问题： 状态转移的概率很难得到, 因为 Q 由本轮回报与之后的根据未来状态分布的 V(s')的期望相加组成, 而未来状态的分布, 也就是根据策略 π 采取行动后的 s'我们是不知道的, 于是我们用其他的(a, s', a')样本来估计未来的 Q 期望, 也就是：<br />
<span class="math inline">\(Q^{\pi}({\bf s},{\bf a})\leftarrow r({\bf s},{\bf a})+\gamma E_{ {\bf s}^{\prime}\sim p({\bf s}^{\prime}|{\bf s},{\bf a})}[Q^{\pi}({\bf s}^{\prime},\pi({\bf s}^{\prime}))]\)</span><br />
为什么可以这么做? 或者说, 为什么策略的变化在这样的学习中似乎不重要? 这是因为在这种方法下策略就是选取最大化 Q 的动作, 也就是说策略由 Q 来定义, 而不是 Q 受策略影响, 不严谨地说, 可以认为 Q 的地位高于策略</p>
<p>稍微总结一下训练步骤, 对 V 来说:</p>
<p><span class="math display">\[\begin{array}{l}{ {1.\;\;\mathrm{set}\;{\bf{y}}_{i}\:\leftarrow\;\mathrm{max_{a_{i}}}(r({\bf{s}}_{i},{\bf{a}}_{i})+\gamma E[V_{\phi}({\bf{s}}_{i}^{\prime})])}}\\ { {2.\;\;\mathrm{set}\;\phi\:\leftarrow\;\mathrm{arg~min}_{\phi}\:\frac{1}{2}\:\sum_{i}\|V_{\phi}({\bf{s}}_{i})-{\bf{y}}_{i}\|^{2}}}\end{array}\]</span></p>
<p>于是对 Q 的计算:</p>
<p><span class="math display">\[\begin{array}{l}{ {1.\;\;\mathrm{set}\;\:{\mathrm{y}}_{i}\:\leftarrow\:r(\mathrm{s}_{i},{\bf a}_{i})+\gamma{ E}[V_{\phi}(\mathrm{s}_{i}^{\prime})]}}\\ { {2.\;\;\mathrm{set}\;\:\phi\:\leftarrow\:\mathrm{arg\,min}_{\phi}\:\frac{1}{2}\:\sum_{i}||{\cal Q}_{\phi}(\mathrm{s}_{i},{\bf a}_{i})-{\bf y}_{i}||^{2}}}\end{array}\]</span></p>
<p>也就是说, 由于我们定义策略为选择 Q 最大的 A, 那么相应的状态 s 的动作空间中最佳的 Q 就是状态 s 的期望价值<br />
由于没有麻烦的策略梯度, 以上的 v-b 有较小的方差, 但还有一个问题是, 对神经网络的方法它不保证收敛性</p>
<p>对训练 Q 的神经网络来说, 应用上述的用 Q 估算 V 的思想, 步骤为:</p>
<p><span class="math display">\[\begin{array}{l} {\mathrm{1.~collect~dataset~\{(s_{i},{\bf a}_{i},{\bf s}_{i}^{\prime},r_{i})\}~u{\mathrm{sing~some~policy}}}} \\ { {2.~\mathrm{set~yi}\leftarrow r({\bf s}_{i},{\bf a}_{i})+\gamma\mathrm{max_{a^{\prime}}}\,Q_{\phi}({\bf s}_{i}^{\prime},{\bf a}_{i}^{\prime})}} \\ { {3.~\mathrm{set~}\phi\leftarrow\mathrm{arg\,min}_{\phi}\ {\frac{1}{2}} { \Sigma_{i}|\vert{\cal Q}_{\phi}({\bf s}_{i},{\bf a}_{i})-{\bf y}_{i}}}\vert\vert^{2}}\end{array}\]</span></p>
<p>这种方法称为 <code>full fitted Q-iteration algorithm</code>, 其中 2.3.可以执行 k 次确保其更好地迭代</p>
<p>以上这个简单的方法就是最基础的 off-policy on-line 的 Q 学习, 这个方法有一个非常明显的问题, 由于它的策略不包括任何概率, 是完全决定性的, 而迭代过程是贪心的, 因此在迭代中如果有一些错误或者不好的数据集，很容易就会陷入一个表现较差且无法进一步迭代的循环里<br />
因此, 尤其是对最开始的一批数据, 我们希望能增加一些随机性, 最简单的方法就是对原来的策略加一层随机, 称为 <span class="math inline">\(\epsilon\)</span> 贪心:</p>
<p><span class="math display">\[\pi({\bf a}_{t}|{\bf s}_{t})=\left\{\begin{array}{l}{ {1-\epsilon\mathrm{~if~}{\bf a}_{t}=\arg\mathrm{max}_{ {\bf a}_{t}}Q_{\phi}({\bf s}_{t},{\bf a}_{t})}}\\ { {\epsilon/(|A|-1)\mathrm{~otherwise}}}\end{array}\right.\]</span></p>
<p>另一种常见方法称为 <code>Boltzmann exploration（波尔兹曼探索)</code>, 即 <span class="math inline">\(\pi({\bf a}_{t}|{\bf s}_{t})\propto\exp(Q_{\phi}({\bf s}_{t},{\bf a}_{t}))\)</span> 这种策略的好处是能根据 Q 的好坏加权策略, 如果是 Q 都比较高的行动, 概率不会相差太大, 而 Q 很低的行动则几乎不会被采取</p>
<h3 id="收敛性">收敛性</h3>
<p>接下来尝试证明 V-B 方法是否收敛</p>
<p><img src="/assets/ml/image-29.png" /></p>
<p>首先问题定义如图, B(back-up)操作用于根据给定 V 找到一个最好的动作, <code>V*</code> 则是一个最终的最优解, 也就是其回报与转移状态的价值期望之和在所有动作中最大<br />
也就是说, 如果用 B 操作不断迭代, 能到达 <code>V*</code> 状态, 就说明 v-b 是收敛的<br />
直观地, 我们有以下性质:<br />
<span class="math display">\[{\mathrm{for~any~}}V{\mathrm{~and~}}{\bar{V}},{\mathrm{~we~have~}}||B V-B{\bar{V}}||\infty\leq\gamma||V-{\bar{V}}||\infty\]</span><br />
于是:<br />
<span class="math display">\[\|B V-V^{*}\|_{\infty}\leq\gamma\|V-V^{*}\|_{\infty}\]</span></p>
<p>这样的证明省略了很多东西, 但对 cs 来说已经足够了, 事实上, 基于不断取最大值这个操作, 基于表格(矩阵/向量)的 v-b 收敛是很符合直觉的<br />
那么对基于神经网络回归的 v-b 呢? 这个问题稍微有点麻烦, 将 nn 能产出的价值全集定义为 <span class="math inline">\(\Omega\)</span>, 事实上由于基于线性拟合与目前这个损失函数, nn 做的其实是个最小二乘法, 实际产生的 V'是理想 V 在 Ω 上的一个投影<br />
对任意两个点 a, b 对一条直线的投影点 c, d, ab, cd 直接的距离 l1, l2 来说, 始终有 l1≥l2, 我们可以简单地构建一个直角三角形证明这点, 对 ab 不与直线平行的情况, ab 是斜边, cd 是直角边<br />
我们将投影操作定义为 π, 则有 <span class="math display">\[\|\Pi V-\Pi{\bar{V}}\|^{2}\leq\|V-{\bar{V}}\|^{2}\]</span><br />
而现在的问题是, π 和 B 都有我们很想要的收缩性质, 但如果同时存在这两种操作, 就不保证收缩了<br />
<img src="/assets/ml/image-30.png" /> 上图给出了一个很形象的例子, 由此我们证明基于 nn 的 v-b 不一定收敛<br />
对于 Q 学习, 实际上也差不多<br />
这有点像所谓的合成谬误, 我们有能让策略变好的贪心方法, 还有最小二乘法与梯度下降, 为什么加到一起后结果却不一定变好?<br />
事实上, 我们训练的这个回归网络并不是真正地使用了梯度下降, 见下图<br />
<img src="/assets/ml/image-31.png" /> 对同样和参数有关的 yi, 我们并没有计算梯度, 当然也可以计算(<code>Residual Gradient</code>), 但由于计算量太大, 这么做在工程上很不实用<br />
还有一个悲伤的消息是, 之前的 a-c 也不保证收敛</p>
<h3 id="局限">局限</h3>
<p>目前版本的 q 学习是一个随着时间收集数据的 on-line 方法, 除了效率问题外, 由于数据在时间上过于接近, 很可能产生过拟合问题, 类似策略梯度, 可以对其进行离线改进, 并且由于 q 学习的策略不那么重要, 可以说它很适合离线学习<br />
另一种方法称为 <code>replay buffers</code>, 简单地说就是去掉收集数据的步骤, 对一个巨大的数据池，每次随机取样用来不断进行预测和模型更新, 更新若干轮后再产生一些新数据放入 buffers 池</p>
<p><img src="/assets/ml/image-32.png" /></p>
<p>上图展示了目前为止的 q 学习步骤, 按经验来说一般 k 设置很小, 而 n 较大, 这会带来一个问题, 即更新模型参数的周期中, 早期采样数据 "落后(lag)" 得少, 而随着训练进行就会有很多落后很久的数据, 为了避免这个问题, 使用以下的更新方式<br />
<span class="math inline">\(\begin{array}{c c}{ {\mathrm{update}~\phi^{\prime};~\phi^{\prime}\leftarrow\tau\phi^{\prime}+(1-\tau)\phi}}&amp;{ {\qquad\qquad\tau=0.999~\mathrm{works~well}}}\end{array}\)</span><br />
为了减少上述伪梯度的影响, 对更新梯度, 我们用整体更新前的目标网络生成的 max 参数来更新临时网络参数<br />
以上的伪代码写作单线程迭代的形式, 但其实由于目标网络在一整批数据后才更新, 上述过程可以并行进行, 也就是用于规模庞大的深度学习中, 即(DQN Deep Q-Network) 此外, 对于整个 buffer 的维护, 由于 buffer 存在上限, 那么就必然需要一个“驱逐”机制, 一个简单的思路是采用环形 buffer, 使用以上机制的一个 dpo 流程可以表示为下图:</p>
<p><img src="/assets/ml/image-33.png" /></p>
<p>不同的 q 学习对 3 个 process 各有侧重:</p>
<ol type="1">
<li>online q learning: process1,2,3 have same speed</li>
<li>DQN: process 1,3 have same speed, process 2 is slow, buffer is large</li>
<li>fitted q-iteration: process 3 在 process 2 的内循环中, process 2 在 process1 的内循环中</li>
</ol>
<h3 id="改进">改进</h3>
<p>实际的 q 学习中, 有一个神奇的现象, q 的估计器整体性地高估 q 的值, 而不是在真实值的上下浮动<br />
对任意两个随机变量 x1, x2, 有如下不等式:<br />
<span class="math inline">\(E[\operatorname*{max}(X_{1},X_{2})]\geq\operatorname*{max}(E[X_{1}],E[X_{2}])\)</span><br />
也就是直观地说, 取 max 的范围越大, 这个 max 倾向越大的值, 而我们在取 Q 函数的估计时，会不断用过往的最高 q 值来更新模型, 而这个过往的 q 可能来自于某个误差, 假设这个误差来自于随机高斯分布, 有正有负, 由于我们总取最大值, 那么正误差的值就会被倾向于被用来更新<br />
如何解决这个问题？一个想法是, 将 q 的评估和更新分开, 称为 <code>double q-learning</code> , 其更新步骤为: <span class="math inline">\(Q_{\phi_{A}}(\mathrm{s},\mathbf{a})\leftarrow r+\gamma Q_{\phi_{B}}(\mathrm{s}^{\prime},\mathrm{arg~max}\,Q_{\phi_{A}}(\mathrm{s}^{\prime},\mathbf{a}^{\prime}))\)</span><br />
实际中, 我们正好有目标网络和训练网络两个网络, 可以用作这里的 A 和 B, 当然由于这两个网络其实会经常合并, 这样理论上还有问题, 但一般来说实际中效果尚可<br />
另一个方法称为 <code>multi-step returns</code>, 上面提到过 q 学习的迭代初期可能有非常糟糕的表现, 此时我们更看重采取某个动作后的回报值, 这个问题比较类似 a-c, 可以用多步的回报值解决(在 a-c 中我们用这个办法缓解策略更新的高方差), 即将更新步骤设置为: <span class="math inline">\(y_{j,t}=\sum_{l^{\prime}=t}^{t+N-1}\gamma^{t-t^{\prime}}r_{j,t^{\prime}}+\gamma^{N}\operatorname*{max}_{\Omega_{k},t+N}Q_{\phi^{\prime}}(\mathbf{s}_{j,t+N,\,\Omega_{j,t+N}})\)</span><br />
此外, 这种方法会让训练必须是 on-policy 的, 因为这 n 步的策略必须是被固定的, 当然, 也可以当做这个问题不存在, 毕竟理论上 n 不大的话策略不会改变太多, 又或者实现类似之前的重要性采样</p>
<h3 id="连续动作">连续动作</h3>
<p>连续动作空间的麻烦在于, 如何处理取 Qmax 的操作, 一个基于直觉的想法是梯度下降找机制, 但这样显然计算量很大<br />
为了提高效率, 一个简单的做法是对 Q 随机取 n 个样, 选一个最大值, 这称为 <code>stochastic optimization</code> ; 这种方法有一些改版, 例如通过交叉熵优化采样<br />
另一个不依赖随机的做法是, 用有数值解的函数来表示Q函数, 例如二次函数: <span class="math inline">\(Q_{\phi}({\bf s},{\bf a})=-\frac{1}{2}({\bf a}-\mu_{\phi}({\bf s}))^{T}P_{\phi}({\bf s})({\bf a}-\mu_{\phi}({\bf s}))+V_{\phi}({\bf s})\)</span><br />
从而: <span class="math inline">\(\quad\arg\operatorname*{max}_{\mathbf{a}}Q_{\phi}(\mathbf{s},\mathbf{a})=\mu_{\phi}(\mathbf{s})\qquad\quad\operatorname*{max}_{\mathbf{a}}Q_{\phi}(\mathbf{s},\mathbf{a})=V_{\phi}(\mathbf{s})\)</span><br />
这样以降低表达能力为代价, 得到了高效取max的方法<br />
以及, 训练一个独立的网络来取max, 有点类似于a-c<br />
m ## 最优控制 这一章的目的在于为基于模型的rl做铺垫, 这里先引入几个名词:</p>
<ol type="1">
<li>最优控制(optimal control): 即控制一个系统达到一个最优指标, 在rl中就是回报最大化</li>
<li>system dynamic: 即系统根据行动其状态会怎么变化</li>
<li>closed/open-loop 闭/开环控制: 简单地说, 闭环指的是做出行动后会得到行动后的系统状态, 用于下一次行动, 开环则相反, 做出行动后并不知道之后会发生什么; 根据定义来看, 很明显rl一般是闭环的, 如果不知道采取行动后的状态, 就很难进行学习</li>
<li>控制领域常用x,u表示rl中的s,a, 用c(cost)表示成本, 最小化成本而不是最大化回报</li>
</ol>
<p>本章中, 我们假设s-d完全已知, 无论状态转移是确定还是基于概率的, 目标都是最大化回报, 基于概率的情况下需要最大化的是期望回报<br />
最简单的做法是之前提到过的随机取k个样本取其中的最佳, 对其的改进是<code>Cross-Entropy Method(cem)</code>, cem让每次的k个取样服从一个分布例如高斯分布, 使用最大似然来拟合分布, 让分布去拟合那些有更高值的数据<br />
基于采样的方法在工程上很实用, 但一旦遇到比较多的维度即比较复杂的情况, 效果就不行了<br />
另一个常见方法是蒙特卡洛(<code>Monte Carlo tree search (MCTS)</code>), 也就是状态为节点, 动作与其对应回报作为边的树, 根据这个树先选择一个状态, 然后选择一个行动进行, 一路记录回报来更新树, 如此迭代<br />
具体策略上有很多选择, 这里暂且略过<br />
选定一个初始的(x,u)对, 最小化之后的cost, 这称为<code>shotting method(s-m)</code>, 下面引入一个经典场景: 线性的状态转移与二次的c函数<br />
<span class="math display">\[f(\mathbf{x}_{t},\mathbf{u}_{t})=\mathbf{F}_{t}\left[\begin{array}{l}{ {\mathbf{x}_{t}}}\\ { {\mathbf{u}_{t}}}\end{array}\right]+\mathbf{f}_{t}\]</span><br />
<span class="math display">\[c({\bf x}_{t},{\bf u}_{t})={\frac{1}{2}}\left[\begin{array}{c}{ { {\bf x}_{t}}}\\ { { {\bf u}_{t}}}\end{array}\right]^{T}{\bf C}_{t}\left[\begin{array}{c}{ { {\bf x}_{t}}}\\ { { {\bf u}_{t}}}\end{array}\right]+\left[\begin{array}{c}{ { {\bf x}_{t}}}\\ { { {\bf u}_{t}}}\end{array}\right]^{T}{\bf c}_{t}\]</span><br />
目标表示为 <span class="math display">\[\operatorname*{min}_{\mathbf{u}_{1},\ldots,\mathbf{u}_{T}} c({\bf x}_{1},{\bf u}_{1})+c(f({\bf x}_{1},{\bf u}_{1}),{\bf u}_{2}){ {+\;\cdot\;\cdot\;+\;c}}(f(f(\cdot\cdot\cdot)\;\cdot\;\cdot\;\cdot),\,{\bf u}_{T})\]</span></p>
<p><span class="math display">\[c_T = \left[\begin{array}{l}{ {\mathbf{c_{x_{T}}}}}\\ { {\mathbf{c_{u_{T}}}}}\end{array}\right]\]</span><br />
<span class="math display">\[C_T= \begin {pmatrix} C_{X_T,X_T} &amp; C_{X_T,U_T} \\ C_{U_T,X_T} &amp; C_{U_T,U_T} \end{pmatrix}\]</span> <span class="math display">\[Q({\bf x}_{T},{\bf u}_{T})=\mathrm{const}+\frac{1}{2}\left[\begin{array}{c}{ { {\bf x}_{T}}}\\ { { {\bf u}_{T}}}\end{array}\right]^{T}{\bf C}_{T}\left[\begin{array}{c}{ { {\bf x}_{T}}}\\ { { {\bf u}_{T}}}\end{array}\right]+\left[\begin{array}{c}{ { {\bf x}_{T}}}\\ { { {\bf u}_{T}}}\end{array}\right]^{T}{\bf c}_{T}\]</span></p>
<p>通过对u求导, 很容易能算出这个二次函数的极值, 也就是最优动作, 省略这些让人头疼的推导, 我们需要知道的是, 这个最佳的u依赖我们不知道的x<br />
将这个最佳的u代入, 则可以消除u， 得到对一个特定x的V值<br />
在以上这些推导后, 我们会得到一个用T时刻的(u,x)及其Q,V推导T-1时刻的(u,x)的Q,V的方式, 从而可以不断进行迭代<br />
在基于概率的情况, 例如状态转移服从一个均值为确定性转移的高斯分布的情况下, 此时的u最优解其实保持不变</p>
<h2 id="马尔可夫过程">马尔可夫过程</h2>
<p>马尔可夫过程中, 对一个特定状态则有一个特定的未来状态概率分布（可以理解为策略）, 很容易算出之后的期望回报<br />
即, 如果我们希望知道一个特定状态是好是坏, 可以定义状态价值 V 为其回报的期望, 即从这个状态开始, 折扣 γ 的指数与之后回报乘积的和, V 与回报 G 的区别是, V 绑定一个特定的状态 s, 即某个 s 开始的之后获取的回报的期望<br />
t 时刻后的回报为：<br />
<span class="math display">\[G_{t}=r_{t+1}+\gamma r_{t+2}+\gamma^{2}r_{t+3}+\gamma^{3}r_{t+4}+\ldots+\gamma^{T-t-1}r_{T}\]</span> 状态 s 的价值为：<br />
<span class="math display">\[\begin{array}{l}{ {V^{t}(s)=\mathbb{E}\left[G_{t}\mid s_{t}=s\right]} }\\ { {\ } }\\ { {=\mathbb{E}\left[r_{t+1}+\gamma r_{t+2}+\gamma^{2}r_{t+3}+.\cdot.+\gamma^{T-t-1}r_{T}\mid s_{t}=s\right]} }\end{array}\]</span><br />
即给定 <span class="math inline">\(s_t = s\)</span> 这个条件后的 Gt 期望<br />
实际训练中概率没法直接看出来, 符合常识的想法是靠采样估算, 也就是可以对采样的多组数据取平均值, 这称为 <code>蒙特卡洛(Monte Carlo,MC)采样</code><br />
另一种方法是贝尔曼方程, 这一方程常用于动态规划, 使用下个状态的价值来更新之前状态, 写作：<br />
<span class="math display">\[ V(s)=R(s) + \gamma\sum_{s^{\prime}\in S}p\left(s^{\prime}\mid s\right)V\left(s^{\prime}\right)  \]</span> <code>s′</code> 可以看成未来的某个状态, <code>p(s′|s)</code> 是指从当前状态转移到未来状态的概率。<code>V (s′)</code> 代表的是未来某一个状态的价值, <code>R(s)</code> 为 s 的奖励组成的向量<br />
贝尔曼方程的证明：<br />

$$\begin{array}{l}{ {V(s)=\mathbb{E}\left[G_{t}\mid s_{t}=s\right]} }\\ { {=\mathbb{E}\left[r_{t+1}+\gamma r_{t+2}+\gamma^{2}r_{t+3}+...\mid s_{t}=s\right]} }\\ { {=\mathbb{E}\left[r_{t+1}\right|s_{t}=s]+\gamma\mathbb{E}\left[r_{t+2}+\gamma r_{t+3}+\gamma^{2}r_{t+4}+\dots\mid s_{t}=s\right]} }\\ { {=R(s)+\gamma\mathbb{E}[G_{t+1}\left|s_{t}=s\right]} }\end{array}$$
 下面证明: <span class="math inline">\(\mathbb{E}[V(s_{t+1})|s_{t}]=\mathbb{E}[\mathbb{E}[G_{t+1}|s_{t+1}]|s_{t}]=\mathbb{E}[G_{t+1}|s_{t}]\)</span></p>
<p>条件期望的定义为：<br />
<span class="math display">\[\operatorname{\mathbb{E} }[X\mid Y=y]=\sum_{x}x p(X=x\mid Y=y)\]</span></p>
<p>具体证明：<br />
<img src="/assets/ml/52243716-045d-4e27-ad0c-0e84593fee81.png" /></p>
<p>代入之前的等式, 得：</p>
<p><span class="math display">\[\begin{array}{c}{ {V(s)=R(s)+\gamma\mathbb{E}[V(s_{t+1})|s_{t}=s]} }\\ { {=R(s)+\gamma{\sum\limits_{s^{\prime}\in S} } p\left(s^{\prime}\mid s\right)V\left(s^{\prime}\right)} }\end{array}\]</span></p>
<p>对单个 s 来说, 贝尔曼方程就是一个向量乘积加上偏移值, 所有的 s 可以写成一个矩阵乘法加上奖励向量的形式:</p>
<p><span class="math display">\[\begin{array}{c}{ {V=R+\gamma P V} }\\ { {E V=R+\gamma P V} }\\ { {(E-\gamma P)V=R} }\\ { {V=(I-\gamma P)^{-1}R} }\end{array}\]</span></p>
<p>这个解析解看着很美好, 但涉及到逆矩阵求解, 这是个复杂度 <span class="math inline">\(O(N^3)\)</span> 的问题(单论常用算法), 因此对较大的模型并不实用</p>
<p>得到 V 的两种方法：</p>
<ul>
<li>最简单直接的蒙特卡洛方法, 只需要对某个初始状态采样大量轨迹, 然后取平均值, 既可作为其价值</li>
<li>不断迭代贝尔曼方程, 直到结果趋向收敛, 这个收敛值即可以作为价值, 即“自举”（Bootstrapping）(利用当前的估计或状态值来更新和改进自身的学习过程)</li>
</ul>
<h3 id="决策">决策</h3>
<p>单纯的马尔可夫过程没有策略的空间, 也就是同时根据当前的状态和动作决定之后的状态分布, 策略可以基于概率或者直接产生特定输出, 如果策略已知, 依然可以通过计算 <span class="math inline">\(\sum_{a\in A}\pi(a\mid s)p\left(s^{\prime}\mid s,a\right)\)</span> 获取状态转移的概率<br />
也就是说, 通过采样行动, 我们可以得到使用特定策略的特定状态的价值 由于引进了决策, 定义一个 Q 函数, 也被称为动作价值函数(action-value function)。Q 函数定义的是在某一个状态采取某一个动作, 它有可能得到的回报的一个期望 <span class="math display">\[V_{\pi}(s)=\sum_{a\in A}\pi(a\mid s)Q_{\pi}(s,a)\]</span></p>
<p><span class="math display">\[\begin{array}{r l} {Q(s,a)} &amp;=\mathbb{E}\left[G_{t}\mid s_{t}=s,a_{t}=a\right] \\ &amp;=\mathbb{E}\left[r_{t+1}+\gamma r_{t+2}+\gamma^{2}r_{t+3}+\ldots\mid s_{t}=s,a_{t}=a\right]\\ &amp;{=R(s,a)+\gamma\mathbb{E}[G_{t+1}|s_{t}=s,a_{t}=a]}\\ &amp;{=R(s,a)+\gamma\mathbb{E}[V(s_{t+1})|s_{t}=s,a_{t}=a]}\\ &amp;{=R(s,a)+\gamma\displaystyle\sum_{s^{\prime}\in S}p(s^{\prime}\mid s,a)\,V\left(s^{\prime}\right)} \end{array}\]</span></p>
<p>可以把状态价值函数和 Q 函数拆解成两个部分: 即时奖励和后续状态的折扣价值, 这样分解后得到一个类似于之前马尔可夫奖励过程的贝尔曼方程——贝尔曼期望方程(<code>Bellman expectation equation</code>):</p>
<p><span class="math display">\[V_{\pi}(s)=\mathbb{E}_{\pi}\left[r_{t+1}+\gamma V_{\pi}\left(s_{t+1}\right)\mid s_{t}=s\right]\]</span> <span class="math display">\[Q_{\pi}(s,a)=\mathbb{E}_{\pi}\left[r_{t+1}+\gamma Q_{\pi}\left(s_{t+1},a_{t+1}\right)\right.\mid s_{t}=s,a_{t}=a]\]</span></p>
<h3 id="备份图">备份图</h3>
<p>以上我们得到了状态和动作的价值, 对某一个 s, 它可能对应多种 a, 确定一个 a 后得到下一个 state, 这其实是一种树状的关系, 因此可以用树形图表示 V 的演变, 称为备份(回溯)图<br />
<img src="https://datawhalechina.github.io/easy-rl/img/ch2/2.10.png" /></p>
<p>如图, 每一个空心圆圈代表一个状态, 每一个实心圆圈代表一个状态-动作对<br />
从叶节点开始向上回溯, 可以得到叶节点上一层的 Q, 然后继续递推, 得到 a 上一层 s 的 V, 这样逐层计算就能得出整张图的状态与动作价值</p>
<p><a target="_blank" rel="noopener" href="https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html">可视化学习过程</a></p>
<h3 id="迭代">迭代</h3>
<h4 id="策略迭代">策略迭代</h4>
<p>现在我们有 V 和 Q 这两个工具, 那么假设初始状态, 随便选定一套参数作为策略, 接下来随着训练过程, 不断更新轨迹中各个状态的价值, 由此再得到 Q 函数, 利用 Q 函数更新策略参数, 如此循环, 就可以让策略收敛到较优的性能<br />
优化过程, 简单地说就是先找到所有状态的 V, 然后根据采样结果以及 V 的分布算出对于各个动作 a 的 Q, 那么对于一个特定的 s, 应该采取的策略需要让价值最高的动作有最高的概率, 如果当做分类问题做, 就可以以交叉熵为 loss 函数做梯度下降<br />
这样的优化后, 理论上, 各个状态的 V 以及策略产生的 Q 都会更高, 而作为表现指标的初始状态的 V 也会更好<br />
收敛后, 取让 Q 函数值最大化的动作, Q 函数就会直接变成价值函数, 这称为 <code>贝尔曼最优方程（Bellman optimality equation）</code>: <span class="math inline">\(V_{\pi}(s)=\operatorname*{max}_{a\in A}Q_{\pi}(s,a)\)</span><br />
满足贝尔曼方程就说明, 这个策略对一个状态, 即使采取理论上的最佳行动, 也不会有更好的效果</p>
<h4 id="价值迭代">价值迭代</h4>
<p><code>最优性原理定理（principle of optimality theorem）</code>: 策略在状态 s 达到最优价值, 当且仅当对 s 可达的任何 s', 都已经达到了最优价值, 如果我们找到了初始状态 s 的最优价值, 那么自然就得到了最佳策略<br />
其过程为：</p>
<ol type="1">
<li>初始化: 对所有状态, 设置其价值为 0</li>
<li>循环直至收敛(k 为迭代次数)：
<ol type="1">
<li>对所有 s : <span class="math inline">\(Q_{k+1}(s,a)=R(s,a)+\gamma\sum_{s^{\prime}\in S}p\left(s^{\prime}\mid s,a\right)V_{k}\left(s^{\prime}\right)\)</span></li>
<li><span class="math inline">\(V_{k+1}(s)=\operatorname*{max}_{a}Q_{k+1}(s,a)\)</span></li>
</ol></li>
<li>最优策略 <span class="math inline">\(\pi(s)=\arg\operatorname*{max}_{a}\left[R(s,a)+\gamma\sum_{s^{\prime}\in S}p\left(s^{\prime}\mid s,a\right)V_{H+1}\left(s^{\prime}\right)\right]\)</span></li>
</ol>
<p>这可以单纯视为一个规划问题, 有点类似于计网中路由器网络寻找最短跳数, 由于每个状态的最优解都依赖于能到达它的其他状态是否最优, 而我们不知道具体迭代几次所有状态都能到达最优, 所以对每个状态, 不断地计算其能采取的行动的价值, 找到一个最优的 Q 作为其 V, 如果网络中存在没有达到最优的节点, 那么在某轮迭代中它就会被优化, 直到一轮中没有任何优化发生, 才算抵达了最优解<br />
设想一个类似“终点”的最优状态 s, s 有全局最佳的 V, 只要到达 s 就可以宣告游戏结束, 例如一个跑酷游戏的终点。那么对离 s 一步之遥的 <code>s'</code>, 迭代到它后, s'更新自己的价值为 <code>迈出最后一步(action)的回报+s的价值</code>(为了容易说明, 这里假设迈出这步到达 s 的概率是 1), 如果总共只有这两种状态, 由于 s 不会更好了, s'达到 s 也没有更好的路径, 继续迭代也不会更新, 问题解决了<br />
再复杂化一点, 假设有若干个状态, 其中 s 是 "终点", 到达 s 的行为会得到一个及其巨大的回报, 那么每次迭代, 可以通过行动到达 s 的 s'就能达到最优解, 这样依次递推, 直到离 s 最远的状态也找到了耗费最少的到达 s 的路径<br />
当然, 实际上, 很可能不存在这么一个方便的终点, 各个状态间会是互相传递更优解的情况, 但这种迭代的思路不会变化</p>
<h2 id="免模型方法">免模型方法</h2>
<h3 id="蒙特卡洛">蒙特卡洛</h3>
<p>实际情况中有很多不满足马尔科夫条件, 过程中回报难以获得, 或者 action 连续值无法取最佳之类的问题, 也就是说很难做出系统的理论, 因此需要不通过模型进行 rl 的方法。</p>
<ul>
<li><code>Q表格</code>: 以采样并记录的方法, 使用平均奖励估算每个状态下每个动作的价值<br />
</li>
<li><code>蒙特卡洛方法</code>: 简单地说, 就是大量采样估算状态价值, 简单地列式可得到 <span class="math inline">\(\mu_t=\mu_{t-1}+{\frac{1}{t}}\left(x_{t}-\mu_{t-1}\right)\)</span> 其中 <span class="math inline">\(\mu\)</span> 是到 t 时刻为止的平均值 <span class="math inline">\(x_{t}-\mu_{t-1}\)</span> 称为残差
<ul>
<li><code>增量式蒙特卡洛 （incremental MC）</code> : 用增量的方法更新数据 , <span class="math inline">\(\begin{array}{l}{ {N\left(s_{t}\right)\leftarrow N\left(s_{t}\right)+1}}\\ { {V\left(s_{t}\right)\leftarrow V\left(s_{t}\right)+\frac{1}{N(s_{t})}\left(G_{t}-V\left(s_{t}\right)\right)} }\end{array}\)</span> 其中 N 为状态 s 的访问次数</li>
</ul></li>
</ul>
<h3 id="时序差分">时序差分</h3>
<p>时序差分是免模型的在线算法</p>
<p><span class="math display">\[ V\left(s_{t}\right)\leftarrow V\left(s_{t}\right)+\alpha\left(r_{t+1}+\gamma V\left(s_{t+1}\right)-V\left(s_{t}\right)\right) \]</span></p>
<p>其中 <span class="math inline">\(r_{t+1}+\gamma V(s_{t+1})\)</span> 是称为时序差分目标(TD target)</p>
<p>时序差分误差（TD error）写作:</p>
<p><span class="math display">\[V\left(s_{t}\right)\leftarrow V\left(s_{t}\right)+\alpha\left(G_{i,t}-V\left(s_{t}\right)\right)\]</span></p>
<p>时序差分可以更改步数(走几步更新)</p>
<h2 id="理论问题">理论问题</h2>
<h1 id="模型">模型</h1>
<p>可以简单地讲选择 action 理解为分类问题, 损失函数使用交叉熵, 对不同的 s, 我们可能对执行的 action 有不同的期待, 因此可以设置一个损失函数的系数用于控制倾向性<br />
那么如何定义 reward 呢?最直接的即时 reward 肯定不是最优解, 因此设置一个 Gn 表示 an 行为之后的所有回报和, 但对之后的回报要乘上一个惩罚系数 γ, 其次数逐渐增加; 此外, 可以使用将 G 减去一个 baseline 的值来产生不同行为的倾向性<br />
rl, 准确地说是 on-policy 的 rl 与常见监督学习不同的是, 其训练资料在更新完一次模型后就无用了, 需要再次与环境互动收集资料<br />
off-policy 则有所不同, 它让训练模型和互动的模型分开, 从而让经验可复用, 例如 <code>Proximal Policy Optimization(PPO)</code></p>
<p>Critic actor: 我们希望有一个跟模型参数以及当前状态有关的函数 $ V^(s) $ 表示这种情况下的'价值', 可以理解为某个状态的预期回报<br />
为了得到这个价值函数, 可以怎么做呢:</p>
<ul>
<li>Monte-Carlo(MC): 较为直观的做法, 先训练大量的 s 及其 G 值, 然后用这些资料预测 V</li>
<li>Temporal-difference (TD): V 函数有以下性质:</li>
</ul>
<p><span class="math display">\[\begin{array}{l}{ {V^{\theta}(s_{t})=r_{t}+\gamma r_{t+1}+\gamma^{2}r_{t+2}\ldots} }\\ { {V^{\theta}(s_{t+1})=r_{t+1}+\gamma r_{t+2}+\ldots} }\\ { {V^{\theta}(s_{t})=\gamma V^{\theta}(s_{t+1})+r_{t} } }\end{array}\]</span><br />
那么 <span class="math display">\[V^{\theta}(s_{t})-\gamma V^{\theta}(s_{t+1})\leftrightarrow r_{t}\]</span><br />
通过这种关系就可以训练</p>
<p>现在我们有了 G 和 V, 两者区别是 G 有一个确定的 a, V 则假定 a 是随机的, 那么可以对每个 <span class="math inline">\(\{s_{t},a_{t}\}\)</span> 对, 可以定义一个 <span class="math inline">\(\mathrm{A}_{t}\,=\,G_{t}^{\prime}-V^{\theta}(s_{t})\)</span> , 用于表示这个行为的 "表现"<br />
也可以用 $r_{t}+V^{}(s_{t+1}) $ 代替上式的 $ G^{'}_t $ , 此式相当于采取行为 <span class="math inline">\(a_t\)</span> 后获得一个特定 <span class="math inline">\(r_t\)</span> 后的预期收益, 这称为 <code>Advantage Actor-Critic</code></p>
<p><code>Reward Shaping</code>: 对回馈比较模糊或者非常慢的场景来说, 想要好的表现就需要人为规定一些回报机制, 例如对游戏 ai 来说, 就需要惩罚什么都不做的行为(否则机器为了回报不为负就可能消极游戏); 其中一种机制称为 <code>Curiosity</code>, 也就是奖励机器发现新信息的情况</p>
<p>现实中不是所有情况都可以定义 reward, 此时只能通过人类提供示范来让机器学习, 这样当然会有很多问题, 比如数据集里没有错误示范等, 因此, 可能的思路是通过机器学习来找到 reward 再进行学习<br />
<code>Inverse Reinforcement Learning</code>: 假设老师行为是最优的, 在每次迭代中:</p>
<ol type="1">
<li>actor 与环境互动</li>
<li>定义一个奖励函数, 满足老师的奖励更优这个条件</li>
<li>actor 根据这个奖励函数进行学习</li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E4%BC%AF%E5%85%8B%E5%88%A9/" rel="tag"><i class="fa fa-tag"></i> 伯克利</a>
              <a href="/tags/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/" rel="tag"><i class="fa fa-tag"></i> 课程笔记</a>
              <a href="/tags/python/" rel="tag"><i class="fa fa-tag"></i> python</a>
              <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"><i class="fa fa-tag"></i> 人工智能</a>
              <a href="/tags/%E5%9B%BD%E7%AB%8B%E5%8F%B0%E6%B9%BE%E5%A4%A7%E5%AD%A6/" rel="tag"><i class="fa fa-tag"></i> 国立台湾大学</a>
              <a href="/tags/cs285/" rel="tag"><i class="fa fa-tag"></i> cs285</a>
              <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 强化学习</a>
          </div>

        
  <div class="social-like a2a_kit a2a_kit_size_32 a2a_default_style">
    <a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a>
      <a class="a2a_button_facebook"></a>
      <a class="a2a_button_twitter"></a>
      <a class="a2a_button_wechat"></a>
      <a class="a2a_button_qzone"></a>
  </div>

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/thinklive/48061/" rel="prev" title="机器学习笔记 all in one">
                  <i class="fa fa-angle-left"></i> 机器学习笔记 all in one
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/thinklive/24757/" rel="next" title="一些工具的使用备忘录">
                  一些工具的使用备忘录 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2023 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">thinklive</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">627k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">38:01</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 技术支持
  </div><script defer src="/lib/three.js"></script><script defer src="/lib/lines.js"></script><script defer src="/lib/waves.js"></script>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.24/fancybox/fancybox.umd.js" integrity="sha256-oyhjPiYRWGXaAt+ny/mTMWOnN1GBoZDUQnzzgC7FRI4=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>


  <script src=""></script>
  <script src="/%5Bobject%20Object%5D"></script>
  <script src="/%5Bobject%20Object%5D"></script>
  <script src="/%5Bobject%20Object%5D"></script>
  <script src="/%5Bobject%20Object%5D"></script>


<script>
var options = {
  bottom: '64px', // default: '32px'
  right: 'unset', // default: '32px'
  left: '32px', // default: 'unset'
  time: '0.5s', // default: '0.3s'
  mixColor: '#fff', // default: '#fff'
  backgroundColor: '#fff',  // default: '#fff'
  buttonColorDark: '#100f2c',  // default: '#100f2c'
  buttonColorLight: '#fff', // default: '#fff'
  saveInCookies: true, // default: true,
  label: '🌓', // default: ''
  autoMatchOsTheme: true // default: true
}
const darkmode = new Darkmode(options);
darkmode.showWidget();
</script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>


  <script class="next-config" data-name="wavedrom" type="application/json">{"enable":true,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/wavedrom/3.3.0/wavedrom.min.js","integrity":"sha256-IRMDzTC+wK5stMucZ/XSXkeS5VNtxZ+/Bm8Mcqfoxdo="}}</script>
  <script class="next-config" data-name="wavedrom_skin" type="application/json">{"enable":true,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/wavedrom/3.3.0/skins/default.js","integrity":"sha256-fduc/Zszk5ezWws2uInY/ALWVmIrmV6VTgXbsYSReFI="}}</script>
  <script src="/js/third-party/tags/wavedrom.js"></script>

  <script src="/js/third-party/fancybox.js"></script>

  <script src="/js/third-party/pace.js"></script>

  <script src="/js/third-party/addtoany.js"></script>

  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":false,"archive":false,"delay":true,"timeout":3000,"priority":true,"url":"https://thinklive1.github.io/thinklive/37185/"}</script>
  <script src="/js/third-party/quicklink.js"></script>
<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

<script>
var options = {
  bottom: '64px',
  right: '32px',
  left: 'unset',
  time: '0.5s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: true,
  label: '🌓',
  autoMatchOsTheme: true
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
</script>
<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"thinklive1/blog_comments","issue_term":"pathname","theme":"photon-dark"}</script>
<script src="/js/third-party/comments/utterances.js"></script>
<script>
    const snowflakes = ["❄", "❄", "❆", "❅", "✥","❄", "❄", "❆", "❅", "✥","✻"];
    // 创建雪花
    function createSnowflake() {
        const snowflake = document.createElement("span");
        snowflake.classList.add("snowflake");
        const randomIndex = Math.floor(Math.random() * snowflakes.length);
        snowflake.textContent = snowflakes[randomIndex];
        
        // 起始位置
        /* 80%概率 生成在页面两侧 30% 的位置
        const probability = Math.random();
        let startPosition = Math.random() * 100;

        if (probability < 0.8) {
            startPosition = Math.random() < 0.5 ? Math.random() * 30 : (Math.random() * 30) + 70;
        }
        snowflake.style.left = `${startPosition}vw`;
        */
        snowflake.style.left = `${Math.random() * 100}vw`;
        snowflake.style.top = `-30px`;
        // 雪花大小与透明度
        const size = Math.random() * 18 + 10;
        snowflake.style.fontSize = `${size}px`;
        const opacity = Math.random() * 0.6 + (size > 18 ? 0.4 : 0);
        snowflake.style.setProperty("--opacity", opacity);
        // 动画持续时间
        const fallDuration = Math.random() * 10 + 10;
        // 旋转持续时间
        const rotateDuration = Math.random() * 3 + 1;

        snowflake.style.animationDuration = `${fallDuration}s, ${fallDuration}s`; // 向 CSS 添加淡出动画的持续时间
        // 横向幅度
        const translateX = (Math.random() * 500 - 200);
        snowflake.style.setProperty("--translateX", `${translateX}px`);
        // 纵向幅度
        snowflake.style.setProperty("--translateY", `${window.innerHeight}px`);

        document.body.appendChild(snowflake);
        // 移除雪花
        setTimeout(() => {
            snowflake.remove();
        }, fallDuration * 1000);
    }
    
    function snowfallAnimation() {
        // 载入时若边栏是隐藏状态则不加载雪花
        const sidebarnav = document.querySelector('.sidebar');
        const sidebarnavdisplay = window.getComputedStyle(sidebarnav).getPropertyValue('display'); 
        if (sidebarnavdisplay !== 'none') {
            createSnowflake();
        }
        setTimeout(snowfallAnimation, 500); // 生成速度，毫秒
    }
    snowfallAnimation();
function toggleMode() {
    console.log("change color!");
    const root1 = document.documentElement;

    // 检查当前 color-scheme
    const isLightMode = getComputedStyle(root1).getPropertyValue('--content-bg-color').trim() === '#fff';

    if (isLightMode) {
        // 切换到暗模式
        const images = document.querySelectorAll('img'); // 选择所有<img>标签
        images.forEach(img => {
            img.style.filter = 'brightness(50%)'; // 设置亮度为50%
        });

        root1.style.setProperty('--content-bg-color', '#333');
        root1.style.setProperty('--link-color', '#aaa');
        root1.style.setProperty('--text-color', '#fff');
        root1.style.setProperty('--highlight-background', '#444');
        root1.style.setProperty('--highlight-foreground', '#bbb');
        root1.style.setProperty('--btn-default-bg', '#777');
        root1.style.setProperty('--menu-item-bg-color', '#777');
        root1.style.setProperty('--table-row-odd-bg-color', '#444');
        root1.style.setProperty('--note-warning-bg-color', '#555');
        root1.style.setProperty('--note-bg-color', '#555');
        root1.style.setProperty('--note-info-bg-color', '#555');
        root1.style.setProperty('--highlight-gutter-foreground', '#98d9ffff');
        root1.style.setProperty('', '#777');
        root1.style.transition = 'all 0.5s ease';

    }

    else {
        const images = document.querySelectorAll('img'); // 选择所有<img>标签
        images.forEach(img => {
            img.style.filter = 'brightness(100%)'; // 设置亮度为50%
        });
        root1.style.setProperty('--content-bg-color', '#fff');
        root1.style.setProperty('--text-color', '#555');
        root1.style.setProperty('--highlight-background', '#eaeef3');
        root1.style.setProperty('--highlight-foreground', '#00193a');
        root1.style.setProperty('--btn-default-bg', '#fff');
        root1.style.setProperty('--menu-item-bg-color', '#f5f5f5');
        root1.style.setProperty('--note-warning-bg-color', '#fdf8ea');
        root1.style.setProperty('--note-bg-color', '#f9f9f9');
        root1.style.setProperty('--note-info-bg-color', '#eef7fa');
        root1.style.setProperty('--table-row-odd-bg-color', '#f9f9f9');
        root1.style.setProperty('--highlight-gutter-foreground', '#172e4c');
        root1.style.transition = 'all 0.5s ease';
    }
}

function DarkTrigger() {
    console.log('dark!!')
    let isDarkMode = getComputedStyle(document.documentElement).getPropertyValue('--content-bg-color').trim() === '#000';
    console.log(isDarkMode)
    if (isDarkMode) {
        // 切换到暗模式
        const warningNotes = document.querySelectorAll('.post-body .note.warning');
        // 修改背景颜色
        warningNotes.forEach(note => {
        note.style.background = '#666';
        });

        const infoNotes = document.querySelectorAll('.post-body .note.info');
        // 修改背景颜色
        infoNotes.forEach(note => {
        note.style.background = '#666';
        });
    }
}


</script>

 <!--js: 线条特效-->
  <script type="text/javascript" color="255,255,255" opacity='1' zIndex="-1" count="50" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>

<button style="background: #868686;
  width: 3rem;
  height: 3rem;
  position: fixed;
  border-radius: 50%;
  border: none;
  right: unset;
  bottom: 2rem;
  left: 2rem;
  cursor: pointer;
  transition: all 0.5s ease;
  display: flex;
  justify-content: center;
  align-items: center;" class="darkmode-toggle" role="checkbox" onclick="toggleMode()">🌓</button>

  <video autoplay loop muted playsinline style="position:fixed;top:50%;opacity: 0.8;left:50%;min-width:100%;min-height:100%;transform:translateX(-50%)translateY(-50%);z-index:-2;">
  <source src="/images/red.mp4" type="video/mp4">
<!-- hexo injector body_end start --><script src="/assets/mmedia/mmedia-loader.js"></script><!-- hexo injector body_end end --></body>
</html>
